#!/usr/bin/env python3

import datetime
import inspect
import json
import os
import time
import uuid
import ydb
from typing import List, Dict, Any, Optional, Callable
from contextlib import contextmanager

class YDBWrapper:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å YDB —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    
    def __init__(self, config_path: str = None, enable_statistics: bool = None, script_name: str = None):
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: YDB_QA_CONFIG env > config file (JSON)
        ydb_qa_config_env = os.environ.get("YDB_QA_CONFIG")
        
        if ydb_qa_config_env:
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ ENV
            try:
                config_dict = json.loads(ydb_qa_config_env)
                enable_statistics = self._load_config_from_dict(config_dict, enable_statistics)
                
            except (json.JSONDecodeError, KeyError) as e:
                raise RuntimeError(f"Invalid YDB_QA_CONFIG format: {e}")
        else:
            # Fallback –∫ JSON —Ñ–∞–π–ª—É –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
            if config_path is None:
                dir_path = os.path.dirname(__file__)
                config_path = f"{dir_path}/../../config/ydb_qa_db.json"
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON –∫–æ–Ω—Ñ–∏–≥
            try:
                with open(config_path, 'r') as f:
                    config_dict = json.load(f)
                enable_statistics = self._load_config_from_dict(config_dict, enable_statistics)
            except FileNotFoundError:
                raise RuntimeError(f"Config file not found: {config_path}")
            except json.JSONDecodeError as e:
                raise RuntimeError(f"Invalid JSON config file: {e}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self._enable_statistics = enable_statistics
        self._cluster_version = None
        self._session_id = str(uuid.uuid4())
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º script_name –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if script_name is None:
            script_name = self._get_caller_script_name()
        self._script_name = script_name
        
        # GitHub Action info - –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑
        self._github_info = self._get_github_action_info()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏—é –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        try:
            with self.get_driver() as driver:
                self._get_cluster_version(driver)
        except Exception as e:
            self._log("warning", f"Failed to get cluster version: {e}")
            self._cluster_version = "unknown"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å stats DB —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self._stats_available = None
        if self._enable_statistics:
            self._stats_available = self._check_stats_availability()
            if self._stats_available:
                self._log("info", f"Statistics logging enabled - session_id: {self._session_id}")
            else:
                self._log("warning", "Statistics database is not available, statistics will not be logged")
        else:
            self._log("info", "Statistics logging disabled")
    
    def _load_config_from_dict(self, config_dict: dict, enable_statistics: bool = None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        dbs = config_dict["databases"]
        main = dbs["main"]
        stats = dbs.get("statistics", {})
        variables = config_dict.get("variables", {})
        flags = config_dict.get("flags", {})
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π
        config_mapping = {
            'database_endpoint': main["endpoint"],
            'database_path': main["path"],
            '_connection_timeout': main.get("connection_timeout", 60),
            '_main_db_tables': main.get("tables", {}),
            'stats_endpoint': stats.get("endpoint", main["endpoint"]),
            'stats_path': stats.get("path", main["path"]),
            '_stats_connection_timeout': stats.get("connection_timeout",60),
            'stats_table': stats.get("tables", {}).get("query_statistics", "analytics/query_statistics"),
            '_stats_db_tables': stats.get("tables", {}),
            'ydbd_path': variables.get("ydbd_path", "ydb/apps/ydbd/ydbd")
        }
        
        for key, value in config_mapping.items():
            setattr(self, key, value)
        
        # Enable statistics –∏–∑ flags (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        if enable_statistics is None:
            enable_statistics = flags.get("enable_statistics", True)
        
        return enable_statistics
    
    def _get_caller_script_name(self) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Å–∫—Ä–∏–ø—Ç–∞, –≤—ã–∑–≤–∞–≤—à–µ–≥–æ YDBWrapper"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–µ–∫ –≤—ã–∑–æ–≤–æ–≤
            stack = inspect.stack()
            
            # –ò—â–µ–º –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω–µ ydb_wrapper.py
            for frame_info in stack:
                frame_filename = frame_info.filename
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª (ydb_wrapper.py)
                if 'ydb_wrapper.py' not in frame_filename:
                    # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ –ø—É—Ç–∏ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                    script_name = os.path.splitext(os.path.basename(frame_filename))[0]
                    return script_name
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            return "unknown_script"
            
        except Exception:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            return "unknown_script"
    
    def _make_full_path(self, table_path: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ –≤ –ø–æ–ª–Ω—ã–π (—Å database_path)
        
        Args:
            table_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ
            
        Returns:
            –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ
        """
        # –ï—Å–ª–∏ –ø—É—Ç—å —É–∂–µ –ø–æ–ª–Ω—ã–π (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å database_path), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if table_path.startswith(self.database_path):
            return table_path
        
        # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–π —Å–ª–µ—à –µ—Å–ª–∏ –µ—Å—Ç—å
        table_path = table_path.lstrip('/')
        
        # –î–æ–±–∞–≤–ª—è–µ–º database_path
        return f"{self.database_path}/{table_path}"
    
    def __enter__(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        pass  # –ë–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ—Ç–æ–∫–∏
    
    def _log(self, level: str, message: str, details: str = ""):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        icons = {"start": "üöÄ", "progress": "‚è≥", "success": "‚úÖ", "error": "‚ùå", "info": "‚ÑπÔ∏è", "warning": "‚ö†Ô∏è", "stats": "üìä"}
        if details:
            print(f"üïê [{timestamp}] {icons.get(level, 'üìù')} {message} | {details}")
        else:
            print(f"üïê [{timestamp}] {icons.get(level, 'üìù')} {message}")
    
    def _setup_credentials(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö YDB"""
        if "CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS" not in os.environ:
            raise RuntimeError("Env variable CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS is missing")
        
        os.environ["YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS"] = os.environ[
            "CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS"
        ]
    
    def check_credentials(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö YDB"""
        if "CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS" not in os.environ:
            print("Error: Env variable CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS is missing, skipping")
            return False
        
        # Do not set up 'real' variable from gh workflows because it interfere with ydb tests
        # So, set up it locally
        os.environ["YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS"] = os.environ[
            "CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS"
        ]
        return True
    
    def _get_cluster_version(self, driver) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ YDB"""
        if self._cluster_version is not None:
            return self._cluster_version
            
        try:
            tc_settings = ydb.TableClientSettings().with_native_date_in_result_sets(enabled=True)
            table_client = ydb.TableClient(driver, tc_settings)
            scan_query = ydb.ScanQuery("SELECT Version() as version", {})
            it = table_client.scan_query(scan_query)
            result = next(it)
            
            if result.result_set.rows:
                self._cluster_version = result.result_set.rows[0]['version']
                return self._cluster_version
            else:
                raise RuntimeError("No version data returned from cluster")
                
        except Exception as e:
            raise RuntimeError(f"Failed to get cluster version: {e}")
    
    @contextmanager
    def get_driver(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞ YDB"""
        self._setup_credentials()
        
        with ydb.Driver(
            endpoint=self.database_endpoint,
            database=self.database_path,
            credentials=ydb.credentials_from_env_variables(),
        ) as driver:
            try:
                driver.wait(timeout=self._connection_timeout, fail_fast=True)
            except TimeoutError as e:
                self._log("error", f"Failed to connect to YDB: {e}")
                self._log("error", f"Endpoint: {self.database_endpoint}")
                self._log("error", f"Database: {self.database_path}")
                self._log("error", f"Timeout: {self._connection_timeout} seconds")
                self._log("error", "Possible causes: network issues, YDB service unavailable, or invalid credentials")
                self._log("error", f"Try increasing timeout with: export YDB_CONNECTION_TIMEOUT=30")
                raise RuntimeError(f"YDB connection timeout after {self._connection_timeout}s: {e}") from e
            except Exception as e:
                self._log("error", f"Failed to connect to YDB: {e}")
                self._log("error", f"Endpoint: {self.database_endpoint}")
                self._log("error", f"Database: {self.database_path}")
                self._log("error", f"Timeout: {self._connection_timeout} seconds")
                self._log("error", "Check your YDB credentials and network connectivity")
                raise RuntimeError(f"YDB connection failed: {e}") from e
            
            # –í–µ—Ä—Å–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ —É–∂–µ –ø–æ–ª—É—á–µ–Ω–∞ –≤ __init__
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –µ—ë —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ None (fallback)
            if self._cluster_version is None:
                try:
                    self._get_cluster_version(driver)
                except Exception as e:
                    self._log("warning", f"Failed to get cluster version during operation: {e}")
                    self._cluster_version = "unknown"
            
            yield driver
    
    def _check_stats_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –≤ __init__)"""
        try:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º credentials
            self._setup_credentials()
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            driver = ydb.Driver(
                endpoint=self.stats_endpoint,
                database=self.stats_path,
                credentials=ydb.credentials_from_env_variables()
            )
            try:
                driver.wait(timeout=self._stats_connection_timeout, fail_fast=True)
                tc_settings = ydb.TableClientSettings().with_native_date_in_result_sets(enabled=True)
                table_client = ydb.TableClient(driver, tc_settings)
                scan_query = ydb.ScanQuery("SELECT 1 as test", {})
                it = table_client.scan_query(scan_query)
                next(it)
                return True
            finally:
                driver.stop()
        except Exception:
            return False
    
    def _log_statistics(self, operation_type: str, query: str, duration: float, 
                       status: str, error: str = None, rows_affected: int = None,
                       cluster_version: str = None, table_path: str = None, query_name: str = None):
        """Log operation statistics (synchronously)
        
        Args:
            query_name: Optional name for the query (e.g., monitoring query filename)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if not self._enable_statistics or not self._stats_available:
            return
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º timestamp –≤ –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥–∞—Ö (–∫–∞–∫ YDB Timestamp)
        timestamp_us = int(time.time() * 1000000)
        
        stats_data = {
            'timestamp': timestamp_us,
            'session_id': self._session_id,
            'operation_type': operation_type,
            'query': query,
            'duration_ms': int(duration * 1000),
            'status': status,
            'error': error,
            'rows_affected': rows_affected,
            'script_name': self._script_name,
            'query_name': query_name,
            'cluster_version': cluster_version,
            'database_endpoint': self.database_endpoint,
            'database_path': self.database_path,
            'table_path': self._normalize_table_path(table_path),
            'github_workflow_name': self._github_info['workflow_name'],
            'github_run_id': self._github_info['run_id'],
            'github_run_url': self._github_info['run_url']
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self._log("stats", "Sending statistics", 
                  f"operation={operation_type}, status={status}, duration={duration:.2f}s, rows={rows_affected or 0}, cluster={cluster_version or 'unknown'}")
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        send_start = time.time()
        success = self._write_stats_sync(stats_data)
        send_duration = time.time() - send_start
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–ø—Ä–∞–≤–∫–∏
        if success:
            self._log("success", f"Statistics sent successfully in {send_duration:.2f}s")
    
    def _write_stats_sync(self, stats_data):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º credentials –¥–ª—è –±–∞–∑—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._setup_credentials()
            
            driver = ydb.Driver(
                endpoint=self.stats_endpoint,
                database=self.stats_path,
                credentials=ydb.credentials_from_env_variables()
            )
            try:
                # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                driver.wait(timeout=self._stats_connection_timeout, fail_fast=True)
                # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                self._ensure_stats_table_exists(driver)
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                stats_data_list = [stats_data]
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                table_client = ydb.TableClient(driver)
                column_types = (
                    ydb.BulkUpsertColumns()
                    .add_column("timestamp", ydb.OptionalType(ydb.PrimitiveType.Timestamp))
                    .add_column("session_id", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("operation_type", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("query", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("duration_ms", ydb.OptionalType(ydb.PrimitiveType.Uint64))
                    .add_column("status", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("error", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("rows_affected", ydb.OptionalType(ydb.PrimitiveType.Uint64))
                    .add_column("script_name", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("query_name", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("cluster_version", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("database_endpoint", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("database_path", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("table_path", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("github_workflow_name", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("github_run_id", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                    .add_column("github_run_url", ydb.OptionalType(ydb.PrimitiveType.Utf8))
                )
                
                full_path = f"{self.stats_path}/{self.stats_table}"
                table_client.bulk_upsert(full_path, stats_data_list, column_types)
                
                return True
                
            finally:
                driver.stop()
                
        except TimeoutError as e:
            self._log("warning", f"Failed to send statistics: connection timeout (timeout={self._stats_connection_timeout}s)")
            self._stats_available = False
            return False
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            self._log("warning", f"Failed to send statistics ({error_type}): {error_msg}")
            return False
    
    def _ensure_stats_table_exists(self, driver):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        def create_stats_table(session):
            create_sql = f"""
                CREATE TABLE IF NOT EXISTS `{self.stats_table}` (
                    `timestamp` Timestamp NOT NULL,
                    `session_id` Utf8 NOT NULL,
                    `operation_type` Utf8 NOT NULL,
                    `query` Utf8,
                    `duration_ms` Uint64 NOT NULL,
                    `status` Utf8 NOT NULL,
                    `error` Utf8,
                    `rows_affected` Uint64,
                    `script_name` Utf8 NOT NULL,
                    `query_name` Utf8,
                    `cluster_version` Utf8,
                    `database_endpoint` Utf8,
                    `database_path` Utf8,
                    `table_path` Utf8,
                    `github_workflow_name` Utf8,
                    `github_run_id` Utf8,
                    `github_run_url` Utf8,
                    PRIMARY KEY (`timestamp`, `session_id`, `operation_type`, `script_name`)
                )
                PARTITION BY HASH(`session_id`)
                WITH (STORE = COLUMN)
            """
            session.execute_scheme(create_sql)
        
        with ydb.SessionPool(driver) as pool:
            pool.retry_operation_sync(create_stats_table)
    
    def _execute_with_logging(self, operation_type: str, operation_func: Callable, 
                             query: str = None, table_path: str = None, query_name: str = None) -> Any:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        start_time = time.time()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–∏
        self._log("start", f"Executing {operation_type}")
        
        if query:
            # –î–ª—è bulk_upsert –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ batch'–∞
            if operation_type != "bulk_upsert":
                self._log("info", f"Query details:\n{query}")
        
        status = "success"
        error = None
        rows_affected = 0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º _cluster_version –∏–ª–∏ 'unknown' –µ—Å–ª–∏ –æ–Ω None
        cluster_version = self._cluster_version or "unknown"
        
        try:
            with self.get_driver() as driver:
                if operation_type == "scan_query":
                    tc_settings = ydb.TableClientSettings().with_native_date_in_result_sets(enabled=False)
                    table_client = ydb.TableClient(driver, tc_settings)
                    scan_query = ydb.ScanQuery(query, {})
                    it = table_client.scan_query(scan_query)
                    
                    results = []
                    batch_count = 0
                    
                    while True:
                        try:
                            result = next(it)
                            batch_count += 1
                            batch_size = len(result.result_set.rows) if result.result_set.rows else 0
                            results = results + result.result_set.rows
                            rows_affected += batch_size
                            
                            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π –∏–ª–∏ –∫–∞–∂–¥—ã–µ 10000 —Å—Ç—Ä–æ–∫ (–Ω–æ –Ω–µ –¥–ª—è –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
                            if (batch_count % 50 == 0 or (rows_affected > 0 and rows_affected % 10000 == 0)) and rows_affected > 0:
                                elapsed = time.time() - start_time
                                self._log("progress", f"Batch {batch_count}: {batch_size} rows (total: {rows_affected})", f"{elapsed:.2f}s")
                            
                        except StopIteration:
                            break
                    
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    if rows_affected == 0:
                        self._log("success", f"Scan query completed", f"No results found, Duration: {duration:.2f}s")
                    else:
                        self._log("success", f"Scan query completed", f"Total results: {rows_affected} rows, Duration: {duration:.2f}s")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    self._log_statistics(
                        operation_type=operation_type,
                        query=query,
                        duration=duration,
                        status=status,
                        rows_affected=rows_affected,
                        cluster_version=cluster_version,
                        table_path=table_path,
                        query_name=query_name
                    )
                    
                    return results
                
                elif operation_type == "scan_query_with_metadata":
                    # –î–ª—è scan_query_with_metadata –∏—Å–ø–æ–ª—å–∑—É–µ–º operation_func
                    result = operation_func(driver)
                    
                    # operation_func –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (results, column_types)
                    data, metadata = result
                    rows_affected = len(data) if isinstance(data, list) else 0
                    
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    if rows_affected == 0:
                        self._log("success", f"Scan query with metadata completed", f"No results found, Duration: {duration:.2f}s")
                    else:
                        self._log("success", f"Scan query with metadata completed", f"Total results: {rows_affected} rows, Duration: {duration:.2f}s")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–∏—Å–ø–æ–ª—å–∑—É–µ–º "scan_query" –¥–ª—è –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ scan –æ–ø–µ—Ä–∞—Ü–∏–π)
                    self._log_statistics(
                        operation_type="scan_query",
                        query=query,
                        duration=duration,
                        status=status,
                        rows_affected=rows_affected,
                        cluster_version=cluster_version,
                        table_path=table_path,
                        query_name=query_name
                    )
                    
                    return result
                
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
                    result = operation_func(driver)
                    
                    # –ï—Å–ª–∏ –æ–ø–µ—Ä–∞—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ —á–∏—Å–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ rows_affected
                    if isinstance(result, (int, float)) and operation_type in ["bulk_upsert", "create_table"]:
                        rows_affected = int(result)
                    # –ï—Å–ª–∏ –æ–ø–µ—Ä–∞—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ –∫–æ—Ä—Ç–µ–∂ (–¥–∞–Ω–Ω—ã–µ + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ), –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
                    elif isinstance(result, tuple) and len(result) == 2 and operation_type == "scan_query":
                        data, metadata = result
                        rows_affected = len(data) if isinstance(data, list) else 0
                    # –ï—Å–ª–∏ scan_query –≤–µ—Ä–Ω—É–ª –ø—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫, —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
                    elif isinstance(result, list) and operation_type == "scan_query":
                        rows_affected = len(result)
                    
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    self._log("success", f"{operation_type} completed", f"Duration: {duration:.2f}s")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    self._log_statistics(
                        operation_type=operation_type,
                        query=query or f"{operation_type} operation",
                        duration=duration,
                        status=status,
                        rows_affected=rows_affected,
                        cluster_version=cluster_version,
                        table_path=table_path,
                        query_name=query_name
                    )
                    
                    return result
                
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            status = "error"
            error = str(e)
            
            self._log("error", f"{operation_type} failed", f"Error: {error}, Duration: {duration:.2f}s")
            
            # Normalize operation_type for statistics (use "scan_query" for both scan operations)
            stats_operation_type = "scan_query" if operation_type == "scan_query_with_metadata" else operation_type
            
            # Log error statistics
            self._log_statistics(
                operation_type=stats_operation_type,
                query=query or f"{operation_type} operation",
                duration=duration,
                status=status,
                error=error,
                cluster_version=cluster_version,
                table_path=table_path if operation_type == "bulk_upsert" else None,
                query_name=query_name
            )
            
            raise
    
    
    def execute_scan_query(self, query: str, query_name: str = None) -> List[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ scan query —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        return self._execute_with_logging("scan_query", None, query, None, query_name)
    
    def execute_scan_query_with_metadata(self, query: str) -> tuple[List[Dict[str, Any]], List[tuple[str, Any]]]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ scan query —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        def operation(driver):
            tc_settings = ydb.TableClientSettings().with_native_date_in_result_sets(enabled=True)
            table_client = ydb.TableClient(driver, tc_settings)
            scan_query = ydb.ScanQuery(query, {})
            it = table_client.scan_query(scan_query)
            
            results = []
            column_types = None
            
            while True:
                try:
                    result = next(it)
                    if column_types is None:
                        column_types = [(col.name, col.type) for col in result.result_set.columns]
                    
                    results.extend(result.result_set.rows)
                
                except StopIteration:
                    break
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, column_types –º–æ–∂–µ—Ç –±—ã—Ç—å None
            if column_types is None:
                column_types = []
            
            return results, column_types
        
        return self._execute_with_logging("scan_query_with_metadata", operation, query, None)
    
    def create_table(self, table_path: str, create_sql: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            table_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'test_results/test_runs')
            create_sql: SQL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –¥–ª—è YDB (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø–æ–ª–Ω—ã–π)
        full_path = self._make_full_path(table_path)
        
        def operation(driver):
            def callee(session):
                session.execute_scheme(create_sql)
            
            with ydb.SessionPool(driver) as pool:
                pool.retry_operation_sync(callee)
            return 1  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 1 –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
        
        return self._execute_with_logging("create_table", operation, create_sql, table_path)
    
    def bulk_upsert(self, table_path: str, rows: List[Dict[str, Any]], 
                   column_types: ydb.BulkUpsertColumns):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ bulk upsert —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            table_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'test_results/test_runs')
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –¥–ª—è YDB
        full_path = self._make_full_path(table_path)
        rows_count = len(rows) if rows else 0
        
        def operation(driver):
            table_client = ydb.TableClient(driver)
            table_client.bulk_upsert(full_path, rows, column_types)
            return rows_count  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        
        return self._execute_with_logging("bulk_upsert", operation, f"BULK_UPSERT to {table_path}", table_path)
    
    def bulk_upsert_batches(self, table_path: str, all_rows: List[Dict[str, Any]], 
                           column_types: ydb.BulkUpsertColumns, batch_size: int = 1000):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ bulk upsert —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ batches –∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        
        Args:
            table_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'test_results/test_runs')
            all_rows: –í—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            column_types: –¢–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
            batch_size: –†–∞–∑–º–µ—Ä batch (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –¥–ª—è YDB
        full_path = self._make_full_path(table_path)
        
        start_time = time.time()
        total_rows = len(all_rows)
        
        if total_rows == 0:
            self._log("info", "bulk_upsert_batches: no rows to insert")
            return
        
        num_batches = (total_rows - 1) // batch_size + 1
        self._log("start", f"Executing bulk_upsert_batches", 
                  f"{total_rows} rows in {num_batches} batches of {batch_size}")
        
        status = "success"
        error = None
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º _cluster_version –∏–ª–∏ 'unknown' –µ—Å–ª–∏ –æ–Ω None
        cluster_version = self._cluster_version or "unknown"
        
        try:
            with self.get_driver() as driver:
                table_client = ydb.TableClient(driver)
                
                for batch_num, start_idx in enumerate(range(0, total_rows, batch_size), 1):
                    batch_rows = all_rows[start_idx:start_idx + batch_size]
                    table_client.bulk_upsert(full_path, batch_rows, column_types)
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 batches –∏–ª–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                    if batch_num % 10 == 0 or start_idx + batch_size >= total_rows:
                        elapsed = time.time() - start_time
                        processed = min(start_idx + batch_size, total_rows)
                        self._log("progress", f"Batch {batch_num}/{num_batches}", 
                                  f"{processed}/{total_rows} rows, {elapsed:.2f}s")
            
            duration = time.time() - start_time
            self._log("success", f"bulk_upsert_batches completed", 
                      f"Total: {total_rows} rows in {num_batches} batches, Duration: {duration:.2f}s")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –û–î–ù–£ –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –≤—Å–µ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
            self._log_statistics(
                operation_type="bulk_upsert",
                query=f"BULK_UPSERT to {table_path} ({total_rows} rows in {num_batches} batches)",
                duration=duration,
                status=status,
                rows_affected=total_rows,
                cluster_version=cluster_version,
                table_path=table_path
            )
            
        except Exception as e:
            duration = time.time() - start_time
            status = "error"
            error = str(e)
            
            self._log("error", f"bulk_upsert_batches failed", f"Error: {error}, Duration: {duration:.2f}s")
            
            self._log_statistics(
                operation_type="bulk_upsert",
                query=f"BULK_UPSERT to {table_path}",
                duration=duration,
                status=status,
                error=error,
                cluster_version=cluster_version,
                table_path=table_path
            )
            raise
    
    def execute_dml(self, query: str, parameters: Dict[str, Any] = None, query_name: str = None):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ DML –∑–∞–ø—Ä–æ—Å–∞ (INSERT/UPDATE/DELETE) —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        
        Args:
            query: SQL –∑–∞–ø—Ä–æ—Å —Å DECLARE –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            parameters: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {$param_name: value}
            query_name: –ò–º—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        def operation(driver):
            def callee(session):
                prepared_query = session.prepare(query)
                with session.transaction() as tx:
                    tx.execute(prepared_query, parameters or {}, commit_tx=True)
                    return 1  # –£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            
            with ydb.SessionPool(driver) as pool:
                return pool.retry_operation_sync(callee)
        
        return self._execute_with_logging("dml_query", operation, query, None, query_name)
    
    
    def _get_github_action_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GitHub Action –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω –≤ GitHub Actions"""
        github_info = {
            "workflow_name": None,
            "run_id": None,
            "run_url": None
        }
        
        try:
            # GitHub Actions —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —ç—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            workflow_name = os.environ.get("GITHUB_WORKFLOW")
            run_id = os.environ.get("GITHUB_RUN_ID")
            repository = os.environ.get("GITHUB_REPOSITORY")
            
            if workflow_name:
                github_info["workflow_name"] = workflow_name
            
            if run_id and repository:
                github_info["run_id"] = run_id
                github_info["run_url"] = f"https://github.com/{repository}/actions/runs/{run_id}"
                
        except Exception:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–æ–ª—É—á–µ–Ω–∏—è GitHub –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            pass
            
        return github_info
    
    def _normalize_table_path(self, table_path: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Ç–∏ –∫ —Ç–∞–±–ª–∏—Ü–µ - –∏—Å–∫–ª—é—á–∞–µ–º database_path –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏"""
        if not table_path:
            return None
            
        # –ï—Å–ª–∏ –ø—É—Ç—å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å database_path, —É–±–∏—Ä–∞–µ–º –µ–≥–æ
        if self.database_path and table_path.startswith(self.database_path):
            normalized = table_path[len(self.database_path):]
            # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–π —Å–ª–µ—à –µ—Å–ª–∏ –µ—Å—Ç—å
            if normalized.startswith('/'):
                normalized = normalized[1:]
            return normalized
            
        return table_path
    
    def get_table_path(self, table_name: str, database: str = "main") -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            table_name: –ò–º—è —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'test_results')
            database: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö ('main' –∏–ª–∏ 'statistics')
        
        Returns:
            –ü—É—Ç—å –∫ —Ç–∞–±–ª–∏—Ü–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        
        Raises:
            KeyError: –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        if database == "main":
            if table_name not in self._main_db_tables:
                raise KeyError(f"Table '{table_name}' not found in main_database.tables config")
            return self._main_db_tables[table_name]
        elif database == "statistics":
            if table_name not in self._stats_db_tables:
                raise KeyError(f"Table '{table_name}' not found in statistics_database.tables config")
            return self._stats_db_tables[table_name]
        else:
            raise ValueError(f"Unknown database: {database}. Use 'main' or 'statistics'")
    
    def get_available_tables(self, database: str = "main") -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
        
        Args:
            database: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö ('main' –∏–ª–∏ 'statistics')
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {table_name: table_path}
        """
        if database == "main":
            return self._main_db_tables.copy()
        elif database == "statistics":
            return self._stats_db_tables.copy()
        else:
            raise ValueError(f"Unknown database: {database}")
    
    def get_cluster_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ (–±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è)"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø–æ–ª—É—á–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é, –Ω–µ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            version = self._cluster_version
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats_status = "disabled"
            if self._enable_statistics:
                stats_status = "enabled_and_available" if self._stats_available else "enabled_but_unavailable"
            
            return {
                'session_id': self._session_id,
                'version': version,
                'endpoint': self.database_endpoint,
                'database': self.database_path,
                'statistics_enabled': self._enable_statistics,
                'statistics_status': stats_status,
                'statistics_endpoint': self.stats_endpoint,
                'statistics_database': self.stats_path,
                'statistics_table': self.stats_table,
                'github_workflow': self._github_info['workflow_name'],
                'github_run_id': self._github_info['run_id'],
                'github_run_url': self._github_info['run_url']
            }
                
        except Exception as e:
            return {
                'session_id': self._session_id,
                'version': None,
                'endpoint': self.database_endpoint,
                'database': self.database_path,
                'error': str(e),
                'statistics_enabled': self._enable_statistics,
                'statistics_status': "disabled",
                'statistics_endpoint': self.stats_endpoint,
                'statistics_database': self.stats_path,
                'statistics_table': self.stats_table
            }
