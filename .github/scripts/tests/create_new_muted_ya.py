#!/usr/bin/env python3
import argparse
import configparser
import datetime
import os
import re
import ydb
import logging
import sys
from pathlib import Path

# Add the parent directory to the path to import update_mute_issues
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from transform_ya_junit import YaMuteCheck
from update_mute_issues import (
    create_and_add_issue_to_project,
    generate_github_issue_title_and_body,
    get_muted_tests_from_issues,
    close_unmuted_issues,
)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

dir = os.path.dirname(__file__)
config = configparser.ConfigParser()
config_file_path = f"{dir}/../../config/ydb_qa_db.ini"
repo_path = f"{dir}/../../../"
muted_ya_path = '.github/config/muted_ya.txt'
config.read(config_file_path)

DATABASE_ENDPOINT = config["QA_DB"]["DATABASE_ENDPOINT"]
DATABASE_PATH = config["QA_DB"]["DATABASE_PATH"]

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω mute-–ª–æ–≥–∏–∫–∏
MUTE_DAYS = 4
UNMUTE_DAYS = 4
DELETE_DAYS = 7


def execute_query(branch='main', build_type='relwithdebinfo', days_window=1):
    # –ü–æ–ª—É—á–∞–µ–º today
    today = datetime.date.today()
    start_date = today - datetime.timedelta(days=days_window-1)
    end_date = today
    
    query_start_time = datetime.datetime.now()
    logging.info(f"Executing query for branch='{branch}', build_type='{build_type}', days_window={days_window}")
    logging.info(f"Date range: {start_date} to {end_date}")
    
    query_string = f'''
    SELECT 
        test_name, 
        suite_folder, 
        full_name, 
        build_type, 
        branch, 
        date_window,
        pass_count, 
        fail_count, 
        mute_count, 
        skip_count, 
        success_rate, 
        owner, 
        is_muted, 
        state, 
        days_in_state
    FROM `test_results/analytics/tests_monitor`
    WHERE date_window >= CurrentUtcDate() - 7*Interval("P1D")
        AND branch = '{branch}' 
        AND build_type = '{build_type}'
    '''
    
    logging.info(f"SQL Query:\n{query_string}")
    
    try:
        with ydb.Driver(
            endpoint=DATABASE_ENDPOINT,
            database=DATABASE_PATH,
            credentials=ydb.credentials_from_env_variables(),
        ) as driver:
            driver.wait(timeout=10, fail_fast=True)
            logging.info("Successfully connected to YDB")
            
            query = ydb.ScanQuery(query_string, {})
            table_client = ydb.TableClient(driver, ydb.TableClientSettings())
            it = table_client.scan_query(query)
            results = []
            
            logging.info("Starting to fetch results...")
            row_count = 0
            while True:
                try:
                    result = next(it)
                    batch_results = result.result_set.rows
                    results.extend(batch_results)
                    row_count += len(batch_results)
                    logging.debug(f"Fetched batch of {len(batch_results)} rows, total: {row_count}")
                except StopIteration:
                    break
            
            query_end_time = datetime.datetime.now()
            query_duration = query_end_time - query_start_time
            logging.info(f"Query completed successfully. Total rows returned: {len(results)}")
            logging.info(f"Query execution time: {query_duration.total_seconds():.2f} seconds")
            return results
        
    except Exception as e:
        query_end_time = datetime.datetime.now()
        query_duration = query_end_time - query_start_time
        logging.error(f"Error executing query: {e}")
        logging.error(f"Query parameters: branch='{branch}', build_type='{build_type}', days_window={days_window}")
        logging.error(f"Date range: {start_date} to {end_date}")
        logging.error(f"Query execution time before error: {query_duration.total_seconds():.2f} seconds")
        raise


def add_lines_to_file(file_path, lines_to_add):
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w') as f:
            f.writelines(lines_to_add)
        logging.info(f"Lines added to {file_path}")
    except Exception as e:
        logging.error(f"Error adding lines to {file_path}: {e}")

def aggregate_test_data(all_data, period_days):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
    """
    today = datetime.date.today()
    base_date = datetime.date(1970, 1, 1)
    start_date = today - datetime.timedelta(days=period_days-1)
    start_days = (start_date - base_date).days
    
    logging.info(f"Starting aggregation for {period_days} days period...")
    logging.info(f"Processing {len(all_data)} test records...")
    
    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ full_name
    aggregated = {}
    processed_count = 0
    total_count = len(all_data)
    
    for test in all_data:
        processed_count += 1
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π –∏–ª–∏ –∫–∞–∂–¥—ã–µ 10%
        if processed_count % 1000 == 0 or processed_count % max(1, total_count // 10) == 0:
            progress_percent = (processed_count / total_count) * 100
            logging.info(f"Aggregation progress: {processed_count}/{total_count} ({progress_percent:.1f}%)")
        
        if test.get('date_window', 0) >= start_days:
            full_name = test.get('full_name')
            if full_name not in aggregated:
                aggregated[full_name] = {
                    'test_name': test.get('test_name'),
                    'suite_folder': test.get('suite_folder'),
                    'full_name': full_name,
                    'build_type': test.get('build_type'),
                    'branch': test.get('branch'),
                    'pass_count': 0,
                    'fail_count': 0,
                    'mute_count': 0,
                    'skip_count': 0,
                    'owner': test.get('owner'),
                    'is_muted': test.get('is_muted'),
                    'state': test.get('state'),
                    'state_history': [test.get('state')],  # –ù–∞—á–∏–Ω–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ—Å—Ç–æ—è–Ω–∏–π
                    'state_dates': [test.get('date_window')],  # –ò—Å—Ç–æ—Ä–∏—è –¥–∞—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–π
                    'days_in_state': test.get('days_in_state'),
                    'date_window': test.get('date_window'),
                    'period_days': period_days,  # <--- –¥–æ–±–∞–≤–ª—è–µ–º —Å—é–¥–∞
                }
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                current_state = test.get('state')
                current_date = test.get('date_window')
                if current_state and (not aggregated[full_name]['state_history'] or aggregated[full_name]['state_history'][-1] != current_state):
                    aggregated[full_name]['state_history'].append(current_state)
                    aggregated[full_name]['state_dates'].append(current_date)
            
            # –°—É–º–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            aggregated[full_name]['pass_count'] += test.get('pass_count', 0)
            aggregated[full_name]['fail_count'] += test.get('fail_count', 0)
            aggregated[full_name]['mute_count'] += test.get('mute_count', 0)
            aggregated[full_name]['skip_count'] += test.get('skip_count', 0)
    
    # –í—ã—á–∏—Å–ª—è–µ–º success_rate, —Å–æ–∑–¥–∞–µ–º summary –∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º state –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–∞
    date_window_range = f"{start_date.strftime('%Y-%m-%d')}:{today.strftime('%Y-%m-%d')}"
    for test_data in aggregated.values():
        test_data['date_window'] = date_window_range
        if test_data.get('full_name') == 'ydb/core/kqp/ut/cost/KqpCost.VectorIndexLookup+useSink':
            print(1)
        total_runs = test_data['pass_count'] + test_data['fail_count'] + test_data['mute_count'] + test_data['skip_count']
        if total_runs > 0:
            test_data['success_rate'] = round((test_data['pass_count'] / total_runs) * 100, 1)
        else:
            test_data['success_rate'] = 0.0
        
        # –°–æ–∑–¥–∞–µ–º summary –¥–ª—è —Ç–∏–∫–µ—Ç–∞ (–±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è state, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —É–∂–µ –µ—Å—Ç—å –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏)
        test_data['summary'] = f"p-{test_data['pass_count']}, f-{test_data['fail_count']}, m-{test_data['mute_count']}, s-{test_data['skip_count']}, total-{total_runs}"
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º state –ø–æ –¥–Ω—è–º —Å –¥–∞—Ç–∞–º–∏
        if 'state_history' in test_data and len(test_data['state_history']) > 1:
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ –∏ –¥–∞—Ç–∞–º–∏
            state_with_dates = []
            for i, (state, date) in enumerate(zip(test_data['state_history'], test_data['state_dates'])):
                if date:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—É –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
                    date_obj = base_date + datetime.timedelta(days=date)
                    date_str = date_obj.strftime('%m-%d')
                    state_with_dates.append(f"{state}({date_str})")
                else:
                    state_with_dates.append(state)
            test_data['state'] = '->'.join(state_with_dates)
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
    
    logging.info(f"Aggregation completed: {len(aggregated)} unique tests aggregated from {total_count} records")
    return list(aggregated.values())


def calculate_success_rate(test):
    """–í—ã—á–∏—Å–ª—è–µ—Ç success rate –¥–ª—è —Ç–µ—Å—Ç–∞"""
    runs = test.get('pass_count', 0) + test.get('fail_count', 0)
    return round((test.get('pass_count', 0) / runs * 100) if runs > 0 else 0, 1)

def create_test_string(test, use_wildcards=False):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Ç–µ—Å—Ç–∞ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ wildcards"""
    testsuite = test.get('suite_folder')
    testcase = test.get('test_name')
    test_string = f"{testsuite} {testcase}\n"
    if use_wildcards:
        test_string = re.sub(r'\d+/(\d+)\]', r'*/*]', test_string)
    return test_string

def sort_key_without_prefix(line):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å—Ç—Ä–æ–∫, –∏–≥–Ω–æ—Ä–∏—Ä—É—é—â–∞—è –ø—Ä–µ—Ñ–∏–∫—Å—ã +++, ---, xxx"""
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã +++, ---, xxx –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    if line.startswith('+++ ') or line.startswith('--- ') or line.startswith('xxx '):
        return line[4:]  # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–µ 4 —Å–∏–º–≤–æ–ª–∞ (–ø—Ä–µ—Ñ–∏–∫—Å + –ø—Ä–æ–±–µ–ª)
    return line  # –î–ª—è —Å—Ç—Ä–æ–∫ –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å


def create_debug_string(test, success_rate=None, period_days=None, date_window=None):
    """–°–æ–∑–¥–∞–µ—Ç debug —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ç–µ—Å—Ç–∞"""
    if success_rate is None:
        success_rate = calculate_success_rate(test)
    
    testsuite = test.get('suite_folder')
    testcase = test.get('test_name')
    runs = test.get('pass_count', 0) + test.get('fail_count', 0) + test.get('mute_count', 0) 
    
    debug_string = f"{testsuite} {testcase} # owner {test.get('owner', 'N/A')} success_rate {success_rate}%"
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±–∞ –ø–µ—Ä–∏–æ–¥–∞: days –∏ –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å
    if period_days is None:
        period_days = test.get('period_days')
    if date_window is None:
        date_window = test.get('date_window')
    if period_days:
        debug_string += f" (last {period_days} days)"
    if date_window:
        debug_string += f" [{date_window}]"
    state = test.get('state', 'N/A')
    if test.get('is_test_chunk', 0):
        state = f"(chunk)"
    debug_string += f", p-{test.get('pass_count')}, f-{test.get('fail_count')},m-{test.get('mute_count')}, s-{test.get('skip_count')}, runs-{runs}, state {state}"
    return debug_string + "\n"

def is_flaky_test(test, aggregated_data):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Å—Ç flaky –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
    # –ò—â–µ–º –Ω–∞—à —Ç–µ—Å—Ç –≤ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_data = None
    for agg_test in aggregated_data:
        if agg_test['full_name'] == test.get('full_name'):
            test_data = agg_test
            break
    
    if not test_data:
        return False
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç–∞ –¥–ª—è debug
    test['pass_count'] = test_data['pass_count']
    test['fail_count'] = test_data['fail_count']
    test['period_days'] = test_data.get('period_days')
    
    total_runs = test_data['pass_count'] + test_data['fail_count']
    return (test_data['fail_count'] >= 2) or (test_data['fail_count'] >= 1 and total_runs <= 10)

def is_unmute_candidate(test, aggregated_data):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Å—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º –Ω–∞ —Ä–∞–∑–º—å—é—Ç –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
    # –ò—â–µ–º –Ω–∞—à —Ç–µ—Å—Ç –≤ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_data = None
    for agg_test in aggregated_data:
        if agg_test['full_name'] == test.get('full_name'):
            test_data = agg_test
            break
    
    if not test_data:
        return False
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç–∞ –¥–ª—è debug
    test['pass_count'] = test_data['pass_count']
    test['fail_count'] = test_data['fail_count']
    test['mute_count'] = test_data['mute_count']
    test['period_days'] = test_data.get('period_days')
    
    total_runs = test_data['pass_count'] + test_data['fail_count'] + test_data['mute_count']
    total_fails = test_data['fail_count'] + test_data['mute_count']
    
    return total_runs > 4 and total_fails == 0

def is_delete_candidate(test, aggregated_data):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Å—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –∏–∑ mute –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
    # –ò—â–µ–º –Ω–∞—à —Ç–µ—Å—Ç –≤ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_data = None
    for agg_test in aggregated_data:
        if agg_test['full_name'] == test.get('full_name'):
            test_data = agg_test
            break
    
    if not test_data:
        return False
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç–∞ –¥–ª—è debug
    test['pass_count'] = test_data['pass_count']
    test['fail_count'] = test_data['fail_count']
    test['mute_count'] = test_data['mute_count']
    test['skip_count'] = test_data['skip_count']
    test['period_days'] = test_data.get('period_days')
    
    total_runs = test_data['pass_count'] + test_data['fail_count'] + test_data['mute_count'] + test_data['skip_count']
    
    return total_runs == 0

def create_file_set(aggregated_for_mute, filter_func, mute_check=None, use_wildcards=False, resolution=None):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–ª—å—Ç—Ä–∞"""
    result_set = set()
    debug_list = []
    total = len(aggregated_for_mute)
    last_percent = -1
    for idx, test in enumerate(aggregated_for_mute, 1):
        testsuite = test.get('suite_folder')
        testcase = test.get('test_name')
        
        if not testsuite or not testcase:
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º mute_check –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        if mute_check and not mute_check(testsuite, testcase):
            continue
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        percent = int(idx / total * 100)
        if percent != last_percent and (percent % 5 == 0 or percent == 100):
            print(f"\r[create_file_set] Progress: {percent}% ({idx}/{total})", end="")
            last_percent = percent
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä
        if filter_func(test):
            test_string = create_test_string(test, use_wildcards)
            result_set.add(test_string)
            
            if resolution:
                debug_string = create_debug_string(
                    test,
                    period_days=test.get('period_days'),
                    date_window=test.get('date_window')
                )
                debug_list.append(debug_string)
    print()  # –ü–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    return sorted(result_set), sorted(debug_list)

def write_file_set(file_path, test_set, debug_list=None):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –≤ —Ñ–∞–π–ª"""
    add_lines_to_file(file_path, test_set)
    if debug_list:
        debug_path = file_path.replace('.txt', '_debug.txt')
        add_lines_to_file(debug_path, debug_list)
    logging.info(f"Created {os.path.basename(file_path)} with {len(test_set)} tests")

def apply_and_add_mutes(all_data, output_path, mute_check, aggregated_for_mute, aggregated_for_unmute, aggregated_for_delete):
    output_path = os.path.join(output_path, 'mute_update')
    logging.info(f"Creating mute files in directory: {output_path}")
    
    # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤)
    

    try:
        # 1. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ mute
        def is_mute_candidate(test):
            return is_flaky_test(test, aggregated_for_mute)
        
        to_mute, to_mute_debug = create_file_set(
            aggregated_for_mute, is_mute_candidate, use_wildcards=True, resolution='to_mute'
        )
        write_file_set(os.path.join(output_path, 'to_mute.txt'), to_mute, to_mute_debug)
        
        # 2. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ —Ä–∞–∑–º—å—é—Ç
        def is_unmute_candidate_wrapper(test):
            return is_unmute_candidate(test, aggregated_for_unmute)
        
        to_unmute, to_unmute_debug = create_file_set(
            aggregated_for_unmute, is_unmute_candidate_wrapper, mute_check, resolution='to_unmute'
        )
        
        # –î–ª—è wildcard-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –í–°–ï chunk'–∏ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥ —É—Å–ª–æ–≤–∏—è —Ä–∞–∑–º—å—é—Ç–∞
        wildcard_unmute_candidates = []
        wildcard_unmute_debug = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã –ø–æ wildcard-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        wildcard_groups = {}
        for test in aggregated_for_unmute:
            if mute_check and mute_check(test.get('suite_folder'), test.get('test_name')):
                wildcard_pattern = create_test_string(test, use_wildcards=True)
                if wildcard_pattern not in wildcard_groups:
                    wildcard_groups[wildcard_pattern] = []
                wildcard_groups[wildcard_pattern].append(test)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π wildcard-–ø–∞—Ç—Ç–µ—Ä–Ω
        for wildcard_pattern, chunks in wildcard_groups.items():
            # –ï—Å–ª–∏ –í–°–ï chunk'–∏ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥ —É—Å–ª–æ–≤–∏—è —Ä–∞–∑–º—å—é—Ç–∞
            all_chunks_unmutable = all(
                is_unmute_candidate(chunk, aggregated_for_unmute)
                for chunk in chunks
            )
            
            if all_chunks_unmutable:
                wildcard_unmute_candidates.append(wildcard_pattern)
                # –ë–µ—Ä–µ–º debug-—Å—Ç—Ä–æ–∫—É –ø–µ—Ä–≤–æ–≥–æ chunk'–∞
                if chunks:
                    debug_string = create_debug_string(
                        chunks[0],
                        period_days=chunks[0].get('period_days'),
                        date_window=chunks[0].get('date_window')
                    )
                    wildcard_unmute_debug.append(debug_string)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—ã—á–Ω—ã–µ –∏ wildcard –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        to_unmute = sorted(list(set(to_unmute) | set(wildcard_unmute_candidates)))
        to_unmute_debug = sorted(list(set(to_unmute_debug) | set(wildcard_unmute_debug)))
        
        write_file_set(os.path.join(output_path, 'to_unmute.txt'), to_unmute, to_unmute_debug)
        
        # 3. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –∏–∑ mute (to_delete)
        def is_delete_candidate_wrapper(test):
            return is_delete_candidate(test, aggregated_for_delete)
        
        to_delete, to_delete_debug = create_file_set(
            aggregated_for_delete, is_delete_candidate_wrapper, mute_check, resolution='to_delete'
        )
        write_file_set(os.path.join(output_path, 'to_delete.txt'), to_delete, to_delete_debug)
        
        # 4. muted_ya (–≤—Å–µ –∑–∞–º—å—é—á–µ–Ω–Ω—ã–µ —Å–µ–π—á–∞—Å)
        all_muted_ya, all_muted_ya_debug = create_file_set(
            aggregated_for_mute, lambda test: mute_check(test.get('suite_folder'), test.get('test_name')) if mute_check else True, use_wildcards=True, resolution='muted_ya'
        )
        write_file_set(os.path.join(output_path, 'muted_ya.txt'), all_muted_ya, all_muted_ya_debug)
        to_mute_set = set(to_mute)
        to_unmute_set = set(to_unmute)
        to_delete_set = set(to_delete)
        all_muted_ya_set = set(all_muted_ya)
        
         # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ debug-—Å—Ç—Ä–æ–∫
        all_muted_ya_debug_dict = dict(zip(all_muted_ya, all_muted_ya_debug))
        to_mute_debug_dict = dict(zip(to_mute, to_mute_debug))
        
        # 5. muted_ya+to_mute
        muted_ya_plus_to_mute = sorted(list(all_muted_ya_set | to_mute_set))
        muted_ya_plus_to_mute_debug = []
        for test in muted_ya_plus_to_mute:
            if test in all_muted_ya_debug_dict:
                muted_ya_plus_to_mute_debug.append(all_muted_ya_debug_dict[test])
            elif test in to_mute_debug_dict:
                muted_ya_plus_to_mute_debug.append(to_mute_debug_dict[test])
        write_file_set(os.path.join(output_path, 'muted_ya+to_mute.txt'), muted_ya_plus_to_mute, muted_ya_plus_to_mute_debug)
        
        # 6. muted_ya-to_unmute
        muted_ya_minus_to_unmute = [t for t in all_muted_ya if t not in to_unmute_set]
        muted_ya_minus_to_unmute_debug = [all_muted_ya_debug_dict[t] for t in muted_ya_minus_to_unmute if t in all_muted_ya_debug_dict]
        write_file_set(os.path.join(output_path, 'muted_ya-to_unmute.txt'), muted_ya_minus_to_unmute, muted_ya_minus_to_unmute_debug)
        
        # 7. muted_ya-to_delete
        muted_ya_minus_to_delete = [t for t in all_muted_ya if t not in to_delete_set]
        muted_ya_minus_to_delete_debug = [all_muted_ya_debug_dict[t] for t in muted_ya_minus_to_delete if t in all_muted_ya_debug_dict]
        write_file_set(os.path.join(output_path, 'muted_ya-to_delete.txt'), muted_ya_minus_to_delete, muted_ya_minus_to_delete_debug)
        
        # 8. muted_ya-to-delete-to-unmute
        muted_ya_minus_to_delete_to_unmute = [t for t in all_muted_ya if t not in to_delete_set and t not in to_unmute_set]
        muted_ya_minus_to_delete_to_unmute_debug = [all_muted_ya_debug_dict[t] for t in muted_ya_minus_to_delete_to_unmute if t in all_muted_ya_debug_dict]
        write_file_set(os.path.join(output_path, 'muted_ya-to-delete-to-unmute.txt'), muted_ya_minus_to_delete_to_unmute, muted_ya_minus_to_delete_to_unmute_debug)
        
        # 9. muted_ya-to-delete-to-unmute+to_mute
        muted_ya_minus_to_delete_to_unmute_set = set(muted_ya_minus_to_delete_to_unmute)
        muted_ya_minus_to_delete_to_unmute_plus_to_mute = sorted(list(muted_ya_minus_to_delete_to_unmute_set | to_mute_set))
        muted_ya_minus_to_delete_to_unmute_plus_to_mute_debug = []
        for test in muted_ya_minus_to_delete_to_unmute_plus_to_mute:
            if test in muted_ya_minus_to_delete_to_unmute_set and test in all_muted_ya_debug_dict:
                muted_ya_minus_to_delete_to_unmute_plus_to_mute_debug.append(all_muted_ya_debug_dict[test])
            elif test in to_mute_debug_dict:
                muted_ya_minus_to_delete_to_unmute_plus_to_mute_debug.append(to_mute_debug_dict[test])
        write_file_set(os.path.join(output_path, 'muted_ya-to-delete-to-unmute+to_mute.txt'), muted_ya_minus_to_delete_to_unmute_plus_to_mute, muted_ya_minus_to_delete_to_unmute_plus_to_mute_debug)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–æ—Ç –∂–µ —Ñ–∞–π–ª –∫–∞–∫ new_muted_ya.txt –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å workflow
        write_file_set(os.path.join(output_path, 'new_muted_ya.txt'), muted_ya_minus_to_delete_to_unmute_plus_to_mute, muted_ya_minus_to_delete_to_unmute_plus_to_mute_debug)
        
        # 10. muted_ya_changes - —Ñ–∞–π–ª —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
        muted_ya_changes = []
        muted_ya_changes_debug = []
        
        # –¢–µ—Å—Ç—ã –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è (+++)
        for test in to_mute:
            muted_ya_changes.append(f"+++ {test}")
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é debug-—Å—Ç—Ä–æ–∫—É
            if test in to_mute_debug_dict:
                muted_ya_changes_debug.append(f"+++ {to_mute_debug_dict[test]}")
        
        # –¢–µ—Å—Ç—ã –¥–ª—è —Ä–∞–∑–º—å—é—Ç–∞ (---)
        for test in to_unmute:
            muted_ya_changes.append(f"--- {test}")
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é debug-—Å—Ç—Ä–æ–∫—É –∏–∑ all_muted_ya_debug_dict
            if test in all_muted_ya_debug_dict:
                muted_ya_changes_debug.append(f"--- {all_muted_ya_debug_dict[test]}")
        
        # –¢–µ—Å—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (xxx)
        for test in to_delete:
            muted_ya_changes.append(f"xxx {test}")
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é debug-—Å—Ç—Ä–æ–∫—É –∏–∑ all_muted_ya_debug_dict
            if test in all_muted_ya_debug_dict:
                muted_ya_changes_debug.append(f"xxx {all_muted_ya_debug_dict[test]}")
        
        # –¢–µ—Å—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞)
        unchanged_tests = [t for t in all_muted_ya if t not in to_unmute_set and t not in to_delete_set]
        for test in unchanged_tests:
            muted_ya_changes.append(test)
            if test in all_muted_ya_debug_dict:
                muted_ya_changes_debug.append(all_muted_ya_debug_dict[test])
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –ø—Ä–µ—Ñ–∏–∫—Å—ã
        muted_ya_changes = sorted(muted_ya_changes, key=sort_key_without_prefix)
        muted_ya_changes_debug = sorted(muted_ya_changes_debug, key=sort_key_without_prefix)
        
        write_file_set(os.path.join(output_path, 'muted_ya_changes.txt'), muted_ya_changes, muted_ya_changes_debug)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logging.info(f"To mute: {len(to_mute)}")
        logging.info(f"To unmute: {len(to_unmute)}")
        logging.info(f"To delete: {len(to_delete)}")
        logging.info(f"Muted_ya: {len(all_muted_ya)}")
        logging.info(f"muted_ya+to_mute: {len(muted_ya_plus_to_mute)}")
        logging.info(f"muted_ya-to_unmute: {len(muted_ya_minus_to_unmute)}")
        logging.info(f"muted_ya-to_delete: {len(muted_ya_minus_to_delete)}")
        logging.info(f"muted_ya-to_delete-to_unmute: {len(muted_ya_minus_to_delete_to_unmute)}")
        logging.info(f"muted_ya-to_delete-to_unmute+to_mute: {len(muted_ya_minus_to_delete_to_unmute_plus_to_mute)}")
        
    except (KeyError, TypeError) as e:
        logging.error(f"Error processing test data: {e}. Check your query results for valid keys.")
        return []

    return len(to_mute)



def read_tests_from_file(file_path):
    result = []
    with open(file_path, "r") as fp:
        for line in fp:
            line = line.strip()
            try:
                testsuite, testcase = line.split(" ", maxsplit=1)
                result.append({'testsuite': testsuite, 'testcase': testcase, 'full_name': f"{testsuite}/{testcase}"})
            except ValueError:
                logging.warning(f"cant parse line: {line!r}")
                continue
    return result


def create_mute_issues(all_tests, file_path, close_issues=True):
    tests_from_file = read_tests_from_file(file_path)
    muted_tests_in_issues = get_muted_tests_from_issues()
    prepared_tests_by_suite = {}
    temp_tests_by_suite = {}
    
    # Create set of muted tests for faster lookup
    muted_tests_set = {test['full_name'] for test in tests_from_file}
    
    # Check and close issues if needed
    closed_issues = []
    partially_unmuted_issues = []
    if close_issues:
        closed_issues, partially_unmuted_issues = close_unmuted_issues(muted_tests_set)
    
    # First, collect all tests into temporary dictionary
    for test in all_tests:
        for test_from_file in tests_from_file:
            if test['full_name'] == test_from_file['full_name']:
                if test['full_name'] in muted_tests_in_issues:
                    logging.info(
                        f"test {test['full_name']} already have issue, {muted_tests_in_issues[test['full_name']][0]['url']}"
                    )
                else:
                    key = f"{test_from_file['testsuite']}:{test['owner']}"
                    if not temp_tests_by_suite.get(key):
                        temp_tests_by_suite[key] = []
                    temp_tests_by_suite[key].append(
                        {
                            'mute_string': f"{ test.get('suite_folder')} {test.get('test_name')}",
                            'test_name': test.get('test_name'),
                            'suite_folder': test.get('suite_folder'),
                            'full_name': test.get('full_name'),
                            'success_rate': test.get('success_rate'),
                            'days_in_state': test.get('days_in_state'),
                            'date_window': test.get('date_window', 'N/A'),
                            'owner': test.get('owner'),
                            'state': test.get('state'),
                            'summary': test.get('summary'),
                            'fail_count': test.get('fail_count'),
                            'pass_count': test.get('pass_count'),
                            'branch': test.get('branch'),
                        }
                    )
    
    # Split groups larger than 20 tests
    for key, tests in temp_tests_by_suite.items():
        if len(tests) <= 40:
            prepared_tests_by_suite[key] = tests
        else:
            # Split into groups of 40
            for i in range(0, len(tests), 40):
                chunk = tests[i:i+40]
                chunk_key = f"{key}_{i//40 + 1}"  # Add iterator to key starting from 1
                prepared_tests_by_suite[chunk_key] = chunk

    results = []
    for item in prepared_tests_by_suite:
        title, body = generate_github_issue_title_and_body(prepared_tests_by_suite[item])
        owner_value = prepared_tests_by_suite[item][0]['owner'].split('/', 1)[1] if '/' in prepared_tests_by_suite[item][0]['owner'] else prepared_tests_by_suite[item][0]['owner']
        result = create_and_add_issue_to_project(title, body, state='Muted', owner=owner_value)
        if not result:
            break
        else:
            results.append(
                {
                    'message': f"Created issue '{title}' for TEAM:@ydb-platform/{owner_value}, url {result['issue_url']}",
                    'owner': owner_value
                }
            )

    # Sort results by owner
    results.sort(key=lambda x: x['owner'])
    
    # Group results by owner and add spacing and headers
    formatted_results = []
    
    # Add closed issues section if any
    if closed_issues:
        formatted_results.append("üîí **CLOSED ISSUES**")
        formatted_results.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        formatted_results.append("")
        for issue in closed_issues:
            formatted_results.append(f"‚úÖ **Closed** {issue['url']}")
            formatted_results.append("   üìù **Unmuted tests:**")
            for test in issue['tests']:
                formatted_results.append(f"   ‚Ä¢ `{test}`")
            formatted_results.append("")
    
    # Add partially unmuted issues section if any
    if partially_unmuted_issues:
        if closed_issues:
            formatted_results.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
            formatted_results.append("")
        formatted_results.append("üîì **PARTIALLY UNMUTED ISSUES**")
        formatted_results.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        formatted_results.append("")
        for issue in partially_unmuted_issues:
            formatted_results.append(f"‚ö†Ô∏è **Partially unmuted** {issue['url']}")
            formatted_results.append("   üìù **Unmuted tests:**")
            for test in issue['unmuted_tests']:
                formatted_results.append(f"   ‚Ä¢ `{test}`")
            formatted_results.append("")
    
    # Add created issues section if any
    if results:
        if closed_issues or partially_unmuted_issues:
            formatted_results.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
            formatted_results.append("")
        formatted_results.append("üÜï **CREATED ISSUES**")
        formatted_results.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        formatted_results.append("")
    
        # Add created issues
        current_owner = None
        for result in results:
            if current_owner != result['owner']:
                if formatted_results and formatted_results[-1] != "":
                    formatted_results.append('')
                    formatted_results.append('')
                current_owner = result['owner']
                # Add owner header with team URL
                formatted_results.append(f"üë• **TEAM** @ydb-platform/{current_owner}")
                formatted_results.append(f"   https://github.com/orgs/ydb-platform/teams/{current_owner}")
                formatted_results.append("   ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ")
            
            # Extract issue URL and title
            issue_url = result['message'].split('url ')[-1]
            title = result['message'].split("'")[1]
            formatted_results.append(f"   üéØ {issue_url} - `{title}`")

    print("\n\n")
    print("\n".join(formatted_results))
    if 'GITHUB_OUTPUT' in os.environ:
        if 'GITHUB_WORKSPACE' not in os.environ:
            raise EnvironmentError("GITHUB_WORKSPACE environment variable is not set.")
        
        file_path = os.path.join(os.environ['GITHUB_WORKSPACE'], "created_issues.txt")
        print(f"Writing results to {file_path}")
        
        with open(file_path, 'w') as f:
            f.write("\n")
            f.write("\n".join(formatted_results))
            f.write("\n")
            
        with open(os.environ['GITHUB_OUTPUT'], 'a') as gh_out:
            gh_out.write(f"created_issues_file={file_path}")
            
        print(f"Result saved to env variable GITHUB_OUTPUT by key created_issues_file")


def mute_worker(args):

    # Simplified Connection
    if "CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS" not in os.environ:
        print("Error: Env variable CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS is missing, skipping")
        return 1
    else:
        # Do not set up 'real' variable from gh workflows because it interfere with ydb tests
        # So, set up it locally
        os.environ["YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS"] = os.environ[
            "CI_YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS"
        ]

    logging.info(f"Starting mute worker with mode: {args.mode}")
    logging.info(f"Branch: {args.branch}")
    
    mute_check = YaMuteCheck()
    mute_check.load(muted_ya_path)
    logging.info(f"Loaded muted_ya.txt with {len(mute_check.regexps)} test patterns")

    logging.info("Executing single query for 7 days window...")
    
    # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∑–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (7 –¥–Ω–µ–π)
    all_data = execute_query(args.branch, days_window=7)
    logging.info(f"Query returned {len(all_data)} test records")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    aggregated_for_mute = aggregate_test_data(all_data, MUTE_DAYS)  # MUTE_DAYS –¥–Ω–µ–π –¥–ª—è mute
    aggregated_for_unmute = aggregate_test_data(all_data, UNMUTE_DAYS)  # UNMUTE_DAYS –¥–Ω–µ–π –¥–ª—è unmute
    aggregated_for_delete = aggregate_test_data(all_data, DELETE_DAYS)  # DELETE_DAYS –¥–Ω–µ–π –¥–ª—è delete
    
    logging.info(f"Aggregated data: mute={len(aggregated_for_mute)}, unmute={len(aggregated_for_unmute)}, delete={len(aggregated_for_delete)}")
    
    if args.mode == 'update_muted_ya':
        output_path = args.output_folder
        os.makedirs(output_path, exist_ok=True)
        logging.info(f"Creating mute files in: {output_path}")
        apply_and_add_mutes(all_data, output_path, mute_check, aggregated_for_mute, aggregated_for_unmute, aggregated_for_delete)

    elif args.mode == 'create_issues':
        file_path = args.file_path
        logging.info(f"Creating issues from file: {file_path}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ 3 –¥–Ω—è
        create_mute_issues(aggregated_for_mute, file_path, close_issues=args.close_issues)
    
    logging.info("Mute worker completed successfully")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Add tests to mutes files based on flaky_today condition")

    subparsers = parser.add_subparsers(dest='mode', help="Mode to perform")

    update_muted_ya_parser = subparsers.add_parser('update_muted_ya', help='create new muted_ya')
    update_muted_ya_parser.add_argument('--output_folder', default=repo_path, required=False, help='Output folder.')
    update_muted_ya_parser.add_argument('--branch', default='main', help='Branch to get history')

    create_issues_parser = subparsers.add_parser(
        'create_issues',
        help='create issues by muted_ya like files',
    )
    create_issues_parser.add_argument(
        '--file_path', default=f'{repo_path}/mute_update/to_mute.txt', required=False, help='file path'
    )
    create_issues_parser.add_argument('--branch', default='main', help='Branch to get history')
    create_issues_parser.add_argument('--close_issues', action='store_true', default=True, help='Close issues when all tests are unmuted (default: True)')

    args = parser.parse_args()

    mute_worker(args)
