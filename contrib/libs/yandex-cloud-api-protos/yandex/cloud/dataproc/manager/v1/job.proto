syntax = "proto3";

package yandex.cloud.dataproc.manager.v1;

import "google/protobuf/timestamp.proto";

option go_package = "github.com/yandex-cloud/go-genproto/yandex/cloud/dataproc/manager/v1;dataproc_manager";
option java_package = "yandex.cloud.api.dataproc.manager.v1";

// Data Proc job.
message Job {
  // Unique ID of the Data Proc job.
  // This ID is assigned by MDB in the process of creating Data Proc job.
  string id = 1;

  // ID of the Data Proc cluster.
  string cluster_id = 2;

  // The time when the Data Proc job was created.
  google.protobuf.Timestamp created_at = 3;

  // The time when the Data Proc job was started.
  google.protobuf.Timestamp started_at = 4;

  // The time when the Data Proc job was finished.
  google.protobuf.Timestamp finished_at = 5;

  // Name of the Data Proc job.
  string name = 6;

  // The id of the user who created the job
  string created_by = 12;

  enum Status {
    STATUS_UNSPECIFIED = 0;
    PROVISIONING = 1; // Job created in metadb and is waiting agent to acquire.
    PENDING = 2; // Job acquired by agent and is waiting for execution.
    RUNNING = 3; // Job is running.
    ERROR = 4; // Job failed.
    DONE = 5; // Job finished.
    CANCELLED = 6; // Job cancelled.
    CANCELLING = 7; // Job is waiting for cancellation.
  }
  // Status.
  Status status = 7;

  // Job specification.
  oneof job_spec {
    MapreduceJob mapreduce_job = 8;
    SparkJob spark_job = 9;
    PysparkJob pyspark_job = 10;
    HiveJob hive_job = 11;
  }

  // Attributes of YARN application.
  ApplicationInfo application_info = 13;
}

message ApplicationAttempt {
  // ID of YARN application attempt
  string id = 1;

  // ID of YARN Application Master container
  string am_container_id = 2;
}

message ApplicationInfo {
  // ID of YARN application
  string id = 1;

  // YARN application attempts
  repeated ApplicationAttempt application_attempts = 2;
}

message MapreduceJob {
  // Optional arguments to the driver.
  repeated string args = 1;

  // URIs of file to run.
  repeated string jar_file_uris = 2;

  // URIs of files to be copied to the working directory of Data Proc drivers and distributed tasks.
  repeated string file_uris = 3;

  // URIs of archives to be extracted in the working directory of Data Proc drivers and tasks.
  repeated string archive_uris = 4;

  // A mapping of property names to values, used to configure Data Proc.
  map<string, string> properties = 5;

  oneof driver {
    // The HCFS URI of the jar file containing the main class.
    string main_jar_file_uri = 6;

    // The name of the driver's main class.
    string main_class = 7;
  }
}

message SparkJob {
  // Optional arguments to the driver.
  repeated string args = 1;

  // Jar file URIs to add to the CLASSPATHs of the Data Proc driver and tasks.
  repeated string jar_file_uris = 2;

  // URIs of files to be copied to the working directory of Data Proc drivers and distributed tasks.
  repeated string file_uris = 3;

  // URIs of archives to be extracted in the working directory of Data Proc drivers and tasks.
  repeated string archive_uris = 4;

  // A mapping of property names to values, used to configure Data Proc.
  map<string, string> properties = 5;

  // The HCFS URI of the jar file containing the main class.
  string main_jar_file_uri = 6;

  // The name of the driver's main class.
  string main_class = 7;

  // List of maven coordinates of jars to include on the driver and executor classpaths.
  repeated string packages = 8;

  // List of additional remote repositories to search for the maven coordinates given with --packages.
  repeated string repositories = 9;

  // List of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.
  repeated string exclude_packages = 10;
}

message PysparkJob {
  // Optional arguments to the driver.
  repeated string args = 1;

  // Jar file URIs to add to the CLASSPATHs of the Data Proc driver and tasks.
  repeated string jar_file_uris = 2;

  // URIs of files to be copied to the working directory of Data Proc drivers and distributed tasks.
  repeated string file_uris = 3;

  // URIs of archives to be extracted in the working directory of Data Proc drivers and tasks.
  repeated string archive_uris = 4;

  // A mapping of property names to values, used to configure Data Proc.
  map<string, string> properties = 5;

  // URI of the main Python file to use as the driver. Must be a .py file.
  string main_python_file_uri = 6;

  // URIs of Python files to pass to the PySpark framework.
  repeated string python_file_uris = 7;

  // List of maven coordinates of jars to include on the driver and executor classpaths.
  repeated string packages = 8;

  // List of additional remote repositories to search for the maven coordinates given with --packages.
  repeated string repositories = 9;

  // List of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.
  repeated string exclude_packages = 10;
}

message QueryList {
  repeated string queries = 1;
}

message HiveJob {
  // A mapping of property names to values, used to configure Hive.
  map<string, string> properties = 1;

  // Whether to continue executing queries if a query fails.
  bool continue_on_failure = 2;

  // Mapping of query variable names to values.
  map<string, string> script_variables = 3;

  // Jar file URIs to add to the CLASSPATHs of the Hive driver and tasks.
  repeated string jar_file_uris = 4;

  oneof query_type {
    // URI of the script that contains Hive queries.
    string query_file_uri = 5;

    QueryList query_list = 6;
  }
}

// Data Proc support job.
message SupportJob {
  // Required. Unique ID of the Data Proc job.
  // This ID is assigned by MDB in the process of creating Data Proc job.
  string id = 1;

  // Required. Unique ID of the Data Proc cluster.
  string cluster_id = 2;

  // The time when the Data Proc job was created.
  google.protobuf.Timestamp created_at = 3;

  // The time when the Data Proc job was started.
  google.protobuf.Timestamp started_at = 4;

  // The time when the Data Proc job was finished.
  google.protobuf.Timestamp finished_at = 5;

  enum Status {
    STATUS_UNSPECIFIED = 0;
    PROVISIONING = 1; // Job created in metadb and is waiting agent to acquire.
    PENDING = 2; // Job acquired by agent and is waiting for execution.
    RUNNING = 3; // Job is running.
    ERROR = 4; // Job failed.
    DONE = 5; // Job finished.
    CANCELLED = 6; // Job cancelled.
    CANCELLING = 7; // Job is waiting for cancellation.
  }
  // Status.
  Status status = 6;

  // Command.
  string cmd = 7;

  // Execution timeout in seconds.
  int64 timeout = 8;

  // The id of the user who created the job
  string created_by = 9;
}
