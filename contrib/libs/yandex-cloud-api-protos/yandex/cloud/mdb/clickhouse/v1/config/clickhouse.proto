syntax = "proto3";

package yandex.cloud.mdb.clickhouse.v1.config;

import "google/protobuf/wrappers.proto";
import "yandex/cloud/validation.proto";

option go_package = "github.com/yandex-cloud/go-genproto/yandex/cloud/mdb/clickhouse/v1/config;clickhouse";
option java_package = "yandex.cloud.api.mdb.clickhouse.v1.config";

// ClickHouse configuration options. Detailed description for each set of options
// is available in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/).
//
// Any options not listed here are not supported.
message ClickhouseConfig {
  enum LogLevel {
    LOG_LEVEL_UNSPECIFIED = 0;

    TRACE = 1;

    DEBUG = 2;

    INFORMATION = 3;

    WARNING = 4;

    ERROR = 5;
  }

  // Options specific to the MergeTree table engine.
  message MergeTree {
    // Number of blocks of hashes to keep in ZooKeeper.
    google.protobuf.Int64Value replicated_deduplication_window = 1;

    // Period of time to keep blocks of hashes for.
    google.protobuf.Int64Value replicated_deduplication_window_seconds = 2;

    // If table contains at least that many active parts in single partition, artificially slow down insert into table.
    google.protobuf.Int64Value parts_to_delay_insert = 3;

    // If more than this number active parts in single partition, throw 'Too many parts ...' exception.
    google.protobuf.Int64Value parts_to_throw_insert = 4;

    google.protobuf.Int64Value inactive_parts_to_delay_insert = 9;

    google.protobuf.Int64Value inactive_parts_to_throw_insert = 10;

    // How many tasks of merging and mutating parts are allowed simultaneously in ReplicatedMergeTree queue.
    google.protobuf.Int64Value max_replicated_merges_in_queue = 5;

    // If there is less than specified number of free entries in background pool (or replicated queue), start to lower
    // maximum size of merge to process.
    google.protobuf.Int64Value number_of_free_entries_in_pool_to_lower_max_size_of_merge = 6;

    // Maximum in total size of parts to merge, when there are minimum free threads in background pool (or entries
    // in replication queue).
    google.protobuf.Int64Value max_bytes_to_merge_at_min_space_in_pool = 7;

    google.protobuf.Int64Value max_bytes_to_merge_at_max_space_in_pool = 8;

    // Minimum number of bytes in a data part that can be stored in **Wide** format.
    //
    // More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#min_bytes_for_wide_part).
    google.protobuf.Int64Value min_bytes_for_wide_part = 11;

    // Minimum number of rows in a data part that can be stored in **Wide** format.
    //
    // More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#min_bytes_for_wide_part).
    google.protobuf.Int64Value min_rows_for_wide_part = 12;

    // Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables.
    //
    // More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#ttl_only_drop_parts).
    google.protobuf.BoolValue ttl_only_drop_parts = 13;

    google.protobuf.BoolValue allow_remote_fs_zero_copy_replication = 14;

	google.protobuf.Int64Value merge_with_ttl_timeout = 15;

    google.protobuf.Int64Value merge_with_recompression_ttl_timeout = 16;

    google.protobuf.Int64Value max_parts_in_total = 17;

    google.protobuf.Int64Value max_number_of_merges_with_ttl_in_pool = 18;

    google.protobuf.Int64Value cleanup_delay_period = 19;

    google.protobuf.Int64Value number_of_free_entries_in_pool_to_execute_mutation = 20;

	// The 'too many parts' check according to 'parts_to_delay_insert' and 'parts_to_throw_insert' will be active only if the average part size (in the relevant partition) is not larger than the specified threshold. If it is larger than the specified threshold, the INSERTs will be neither delayed or rejected. This allows to have hundreds of terabytes in a single table on a single server if the parts are successfully merged to larger parts. This does not affect the thresholds on inactive parts or total parts.
    // Default: 1 GiB
    // Min version: 22.10
    // See in-depth description in [ClickHouse GitHub](https://github.com/ClickHouse/ClickHouse/blob/f9558345e886876b9132d9c018e357f7fa9b22a3/src/Storages/MergeTree/MergeTreeSettings.h#L80)
    google.protobuf.Int64Value max_avg_part_size_for_too_many_parts = 21 [(value) = ">=0"];

    // Merge parts if every part in the range is older than the value of min_age_to_force_merge_seconds.
    // Default: 0 - disabled
    // Min_version: 22.10
    // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#min_age_to_force_merge_seconds)
    google.protobuf.Int64Value min_age_to_force_merge_seconds = 22 [(value) = ">=0"];

    // Whether min_age_to_force_merge_seconds should be applied only on the entire partition and not on subset.
    // Default: false
    // Min_version: 22.11
    // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#min_age_to_force_merge_seconds)
    google.protobuf.BoolValue min_age_to_force_merge_on_partition_only = 23;

    // Sleep time for merge selecting when no part is selected. A lower setting triggers selecting tasks in background_schedule_pool frequently, which results in a large number of requests to ClickHouse Keeper in large-scale clusters.
    // Default: 5000
    // Min_version: 21.10
    // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#merge_selecting_sleep_ms)
    google.protobuf.Int64Value merge_selecting_sleep_ms = 24 [(value) = ">0"];

  }

  message Kafka {
    enum SecurityProtocol {
      SECURITY_PROTOCOL_UNSPECIFIED = 0;
      SECURITY_PROTOCOL_PLAINTEXT = 1;
      SECURITY_PROTOCOL_SSL = 2;
      SECURITY_PROTOCOL_SASL_PLAINTEXT = 3;
      SECURITY_PROTOCOL_SASL_SSL = 4;
    }

    enum SaslMechanism {
      SASL_MECHANISM_UNSPECIFIED = 0;
      SASL_MECHANISM_GSSAPI = 1;
      SASL_MECHANISM_PLAIN = 2;
      SASL_MECHANISM_SCRAM_SHA_256 = 3;
      SASL_MECHANISM_SCRAM_SHA_512 = 4;
    }

    SecurityProtocol security_protocol = 1;
    SaslMechanism sasl_mechanism = 2;
    string sasl_username = 3;
    string sasl_password = 4;
	google.protobuf.BoolValue enable_ssl_certificate_verification = 5;
    google.protobuf.Int64Value max_poll_interval_ms = 6 [(value) = ">=0"];
    google.protobuf.Int64Value session_timeout_ms = 7 [(value) = ">=0"];
  }

  message KafkaTopic {
    string name = 1 [(required) = true];
    Kafka settings = 2 [(required) = true];
  }

  message Rabbitmq {

    // [RabbitMQ](https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/) username
    string username = 1;

    // [RabbitMQ](https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/) password
    string password = 2;

    // [RabbitMQ](https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/) virtual host
    string vhost = 3;
  }

  message Compression {
    enum Method {
      METHOD_UNSPECIFIED = 0;

      // [LZ4 compression algorithm](https://lz4.github.io/lz4/).
      LZ4 = 1;

      // [Zstandard compression algorithm](https://facebook.github.io/zstd/).
      ZSTD = 2;
    }

    // Compression method to use for the specified combination of [min_part_size] and [min_part_size_ratio].
    Method method = 1;

    // Minimum size of a part of a table.
    int64 min_part_size = 2 [(value) = ">=1"];

    // Minimum ratio of a part relative to the size of all the data in the table.
    double min_part_size_ratio = 3;

	google.protobuf.Int64Value level = 4 [(value) = ">=0"];
  }

  message ExternalDictionary {
    message HttpSource {
      // URL of the source dictionary available over HTTP.
      string url = 1 [(required) = true];

      // The data format. Valid values are all formats supported by ClickHouse SQL dialect.
      string format = 2 [(required) = true];
    }

    message MysqlSource {
      message Replica {
        // MySQL host of the replica.
        string host = 1 [(required) = true, (length) = "<=253"];

        // The priority of the replica that ClickHouse takes into account when connecting.
        // Replica with the highest priority should have this field set to the lowest number.
        int64 priority = 2 [(required) = true, (value) = ">0"];

        // Port to use when connecting to the replica.
        // If a port is not specified for a replica, ClickHouse uses the port specified for the source.
        int64 port = 3 [(value) = "0-65535"];

        // Name of the MySQL database user.
        string user = 4;

        // Password of the MySQL database user.
        string password = 5;
      }

      // Name of the MySQL database to connect to.
      string db = 1 [(required) = true];

      // Name of the database table to use as a ClickHouse dictionary.
      string table = 2 [(required) = true];

      // Default port to use when connecting to a replica of the dictionary source.
      int64 port = 3 [(value) = "0-65535"];

      // Name of the default user for replicas of the dictionary source.
      string user = 4;

      // Password of the default user for replicas of the dictionary source.
      string password = 5;

      // List of MySQL replicas of the database used as dictionary source.
      repeated Replica replicas = 6 [(size) = ">0"];

      // Selection criteria for the data in the specified MySQL table.
      string where = 7;

      // Query for checking the dictionary status, to pull only updated data.
      // For more details, see [ClickHouse documentation on dictionaries](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_lifetime/).
      string invalidate_query = 8;
    }

    message ClickhouseSource {
      // Name of the ClickHouse database.
      string db = 1 [(required) = true];

      // Name of the table in the specified database to be used as the dictionary source.
      string table = 2 [(required) = true];

      // ClickHouse host of the specified database.
      string host = 3 [(required) = true, (length) = "<=253"];

      // Port to use when connecting to the host.
      int64 port = 4 [(value) = "0-65535"];

      // Name of the ClickHouse database user.
      string user = 5 [(required) = true];

      // Password of the ClickHouse database user.
      string password = 6;

      // Selection criteria for the data in the specified ClickHouse table.
      string where = 7;
    }

    message MongodbSource {
      // Name of the MongoDB database.
      string db = 1 [(required) = true];

      // Name of the collection in the specified database to be used as the dictionary source.
      string collection = 2 [(required) = true];

      // MongoDB host of the specified database.
      string host = 3 [(required) = true, (length) = "<=253"];

      // Port to use when connecting to the host.
      int64 port = 4 [(value) = "0-65535"];

      // Name of the MongoDB database user.
      string user = 5 [(required) = true];

      // Password of the MongoDB database user.
      string password = 6;

	  string options = 7;
    }

    message PostgresqlSource {
      enum SslMode {
        SSL_MODE_UNSPECIFIED = 0;

        // Only try a non-SSL connection.
        DISABLE = 1;

        // First try a non-SSL connection; if that fails, try an SSL connection.
        ALLOW = 2;

        // First try an SSL connection; if that fails, try a non-SSL connection.
        PREFER = 3;

        // Only try an SSL connection, and verify that the server certificate is issued by a trusted certificate authority (CA).
        VERIFY_CA = 4;

        // Only try an SSL connection, verify that the server certificate is issued by a trusted CA and that the requested server host name matches that in the certificate.
        VERIFY_FULL = 5;
      }

      // Name of the PostrgreSQL database.
      string db = 1 [(required) = true];

      // Name of the table in the specified database to be used as the dictionary source.
      string table = 2 [(required) = true];

      // Name of the PostrgreSQL host
      repeated string hosts = 3 [(size) = ">0"];

      // Port to use when connecting to the host.
      int64 port = 4 [(value) = "0-65535"];

      // Name of the PostrgreSQL database user.
      string user = 5 [(required) = true];

      // Password of the PostrgreSQL database user.
      string password = 6;

      // Query for checking the dictionary status, to pull only updated data.
      // For more details, see [ClickHouse documentation on dictionaries](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_lifetime/).
      string invalidate_query = 7;

      // Mode of SSL TCP/IP connection to the PostgreSQL host.
      // For more details, see [PostgreSQL documentation](https://www.postgresql.org/docs/current/libpq-ssl.html).
      SslMode ssl_mode = 8;
    }

    message Structure {
      message Attribute {
        // Name of the column.
        string name = 1 [(required) = true];

        // Type of the column.
        string type = 2 [(required) = true];

        // Default value for an element without data (for example, an empty string).
        string null_value = 3;

        // Expression, describing the attribute, if applicable.
        string expression = 4;

        // Indication of hierarchy support.
        // Default value: `false`.
        bool hierarchical = 5;

        // Indication of injective mapping "id -> attribute".
        // Default value: `false`.
        bool injective = 6;
      }

      // Numeric key.
      message Id {
        // Name of the numeric key.
        string name = 1 [(required) = true];
      }

      // Complex key.
      message Key {
        // Attributes of a complex key.
        repeated Attribute attributes = 1 [(size) = ">0"];
      }

      // Single numeric key column for the dictionary.
      Id id = 1;

      // Composite key for the dictionary, containing of one or more key columns.
      // For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_structure/#composite-key).
      Key key = 3;

      // Field holding the beginning of the range for dictionaries with `RANGE_HASHED` layout.
      // For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_layout/#range-hashed).
      Attribute range_min = 4;

      // Field holding the end of the range for dictionaries with `RANGE_HASHED` layout.
      // For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_layout/#range-hashed).
      Attribute range_max = 5;

      // Description of the fields available for database queries.
      // For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_structure/#attributes).
      repeated Attribute attributes = 2 [(size) = ">0"];
    }

    // Layout determining how to store the dictionary in memory.
    message Layout {
      enum Type {
        TYPE_UNSPECIFIED = 0;

        // The entire dictionary is stored in memory in the form of flat arrays.
        // Available for all dictionary sources.
        FLAT = 1;

        // The entire dictionary is stored in memory in the form of a hash table.
        // Available for all dictionary sources.
        HASHED = 2;

        // Similar to HASHED, to be used with composite keys.
        // Available for all dictionary sources.
        COMPLEX_KEY_HASHED = 3;

        // The entire dictionary is stored in memory in the form of a hash table,
        // with an ordered array of ranges and their corresponding values.
        // Available for all dictionary sources.
        RANGE_HASHED = 4;

        // The dictionary is stored in a cache with a set number of cells.
        // Available for MySQL, ClickHouse and HTTP dictionary sources.
        CACHE = 5;

        // Similar to CACHE, to be used with composite keys.
        // Available for MySQL, ClickHouse and HTTP dictionary sources.
        COMPLEX_KEY_CACHE = 6;
      }

      // Layout type for an external dictionary.
      Type type = 1 [(required) = true];

      // Number of cells in the cache. Rounded up to a power of two.
      // Applicable only for CACHE and COMPLEX_KEY_CACHE layout types.
      int64 size_in_cells = 2;
    }

    message Range {
      // Minimum dictionary lifetime.
      int64 min = 1;

      // Maximum dictionary lifetime.
      int64 max = 2;
    }

    // Name of the external dictionary.
    string name = 1 [(required) = true];

    // Set of attributes for the external dictionary.
    // For in-depth description, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_structure/).
    Structure structure = 2 [(required) = true];

    // Layout for storing the dictionary in memory.
    // For in-depth description, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_layout/).
    Layout layout = 3 [(required) = true];

    // Setting for the period of time between dictionary updates.
    // For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_lifetime/).
    oneof lifetime {
      option (exactly_one) = true;

      // Fixed interval between dictionary updates.
      int64 fixed_lifetime = 4;

      // Range of intervals between dictionary updates for ClickHouse to choose from.
      Range lifetime_range = 5;
    }

    // Description of the source for the external dictionary.
    oneof source {
      option (exactly_one) = true;

      // HTTP source for the dictionary.
      HttpSource http_source = 6;

      // MySQL source for the dictionary.
      MysqlSource mysql_source = 7;

      // ClickHouse source for the dictionary.
      ClickhouseSource clickhouse_source = 8;

      // MongoDB source for the dictionary.
      MongodbSource mongodb_source = 9;

      // PostgreSQL source for the dictionary.
      PostgresqlSource postgresql_source = 10;
    }
  }

  // Rollup settings for the GraphiteMergeTree table engine.
  message GraphiteRollup {
    message Pattern {
      message Retention {
        // Minimum age of the data in seconds.
        int64 age = 1 [(value) = ">=0"];

        // Precision of determining the age of the data, in seconds.
        int64 precision = 2 [(value) = ">0"];
      }

      // Pattern for metric names.
      string regexp = 1;

      // Name of the aggregating function to apply to data of the age specified in [retention].
      string function = 2 [(required) = true];

      // Age of data to use for thinning.
      repeated Retention retention = 3 [(size) = ">0"];
    }

    // Name for the specified combination of settings for Graphite rollup.
    string name = 1 [(required) = true];

    // Pattern to use for the rollup.
    repeated Pattern patterns = 2 [(size) = ">0"];
  }

  // Logging level for the ClickHouse cluster. Possible values: TRACE, DEBUG, INFORMATION, WARNING, ERROR.
  LogLevel log_level = 1;

  // Settings for the MergeTree engine.
  // See description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/#merge_tree).
  MergeTree merge_tree = 2;

  // Compression settings for the ClickHouse cluster.
  // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/#compression).
  repeated Compression compression = 3;

  // Configuration of external dictionaries to be used by the ClickHouse cluster.
  // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts/).
  repeated ExternalDictionary dictionaries = 4;

  // Settings for thinning Graphite data.
  // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/#server_settings-graphite_rollup).
  repeated GraphiteRollup graphite_rollup = 5;

  Kafka kafka = 35;

  repeated KafkaTopic kafka_topics = 36;

  Rabbitmq rabbitmq = 37;

  // Maximum number of inbound connections.
  google.protobuf.Int64Value max_connections = 6 [(value) = ">=10"];

  // Maximum number of simultaneously processed requests.
  google.protobuf.Int64Value max_concurrent_queries = 7 [(value) = ">=10"];

  // Number of milliseconds that ClickHouse waits for incoming requests before closing the connection.
  google.protobuf.Int64Value keep_alive_timeout = 8;

  // Cache size (in bytes) for uncompressed data used by MergeTree tables.
  google.protobuf.Int64Value uncompressed_cache_size = 9;

  // Approximate size (in bytes) of the cache of "marks" used by MergeTree tables.
  google.protobuf.Int64Value mark_cache_size = 10 [(value) = ">0"];

  // Maximum size of the table that can be deleted using a DROP query.
  google.protobuf.Int64Value max_table_size_to_drop = 11;

  // Maximum size of the partition that can be deleted using a DROP query.
  google.protobuf.Int64Value max_partition_size_to_drop = 13;

  // The setting is deprecated and has no effect.
  google.protobuf.Int64Value builtin_dictionaries_reload_interval = 12 [deprecated = true];

  // The server's time zone to be used in DateTime fields conversions. Specified as an IANA identifier.
  string timezone = 14;

  // Enable or disable geobase.
  google.protobuf.BoolValue geobase_enabled = 66;

  // Address of the archive with the user geobase in Object Storage.
  string geobase_uri = 15;

  // The maximum size that query_log can grow to before old data will be removed. If set to 0, automatic removal of
  // query_log data based on size is disabled.
  google.protobuf.Int64Value query_log_retention_size = 16;

  // The maximum time that query_log records will be retained before removal. If set to 0, automatic removal of
  // query_log data based on time is disabled.
  google.protobuf.Int64Value query_log_retention_time = 17;

  // Whether query_thread_log system table is enabled.
  google.protobuf.BoolValue query_thread_log_enabled = 18;

  // The maximum size that query_thread_log can grow to before old data will be removed. If set to 0, automatic removal of
  // query_thread_log data based on size is disabled.
  google.protobuf.Int64Value query_thread_log_retention_size = 19;

  // The maximum time that query_thread_log records will be retained before removal. If set to 0, automatic removal of
  // query_thread_log data based on time is disabled.
  google.protobuf.Int64Value query_thread_log_retention_time = 20;

  // The maximum size that part_log can grow to before old data will be removed. If set to 0, automatic removal of
  // part_log data based on size is disabled.
  google.protobuf.Int64Value part_log_retention_size = 21;

  // The maximum time that part_log records will be retained before removal. If set to 0, automatic removal of
  // part_log data based on time is disabled.
  google.protobuf.Int64Value part_log_retention_time = 22;

  // Whether metric_log system table is enabled.
  google.protobuf.BoolValue metric_log_enabled = 23;

  // The maximum size that metric_log can grow to before old data will be removed. If set to 0, automatic removal of
  // metric_log data based on size is disabled.
  google.protobuf.Int64Value metric_log_retention_size = 24;

  // The maximum time that metric_log records will be retained before removal. If set to 0, automatic removal of
  // metric_log data based on time is disabled.
  google.protobuf.Int64Value metric_log_retention_time = 25;

  // Whether trace_log system table is enabled.
  google.protobuf.BoolValue trace_log_enabled = 26;

  // The maximum size that trace_log can grow to before old data will be removed. If set to 0, automatic removal of
  // trace_log data based on size is disabled.
  google.protobuf.Int64Value trace_log_retention_size = 27;

  // The maximum time that trace_log records will be retained before removal. If set to 0, automatic removal of
  // trace_log data based on time is disabled.
  google.protobuf.Int64Value trace_log_retention_time = 28;

  // Whether text_log system table is enabled.
  google.protobuf.BoolValue text_log_enabled = 29;

  // The maximum size that text_log can grow to before old data will be removed. If set to 0, automatic removal of
  // text_log data based on size is disabled.
  google.protobuf.Int64Value text_log_retention_size = 30;

  // The maximum time that text_log records will be retained before removal. If set to 0, automatic removal of
  // text_log data based on time is disabled.
  google.protobuf.Int64Value text_log_retention_time = 31;

  // Logging level for text_log system table. Possible values: TRACE, DEBUG, INFORMATION, WARNING, ERROR.
  LogLevel text_log_level = 32;

  // Enable or disable opentelemetry_span_log system table. Default value: false.
  google.protobuf.BoolValue opentelemetry_span_log_enabled = 42;

  // The maximum size that opentelemetry_span_log can grow to before old data will be removed. If set to 0 (default),
  // automatic removal of opentelemetry_span_log data based on size is disabled.
  google.protobuf.Int64Value opentelemetry_span_log_retention_size = 55 [(value) = ">=0"];

  // The maximum time that opentelemetry_span_log records will be retained before removal. If set to 0,
  // automatic removal of opentelemetry_span_log data based on time is disabled.
  google.protobuf.Int64Value opentelemetry_span_log_retention_time = 56 [(value) = ">=0"];

  // Enable or disable query_views_log system table. Default value: false.
  google.protobuf.BoolValue query_views_log_enabled = 49;

  // The maximum size that query_views_log can grow to before old data will be removed. If set to 0 (default),
  // automatic removal of query_views_log data based on size is disabled.
  google.protobuf.Int64Value query_views_log_retention_size = 50 [(value) = ">=0"];

  // The maximum time that query_views_log records will be retained before removal. If set to 0,
  // automatic removal of query_views_log data based on time is disabled.
  google.protobuf.Int64Value query_views_log_retention_time = 51 [(value) = ">=0"];

  // Enable or disable asynchronous_metric_log system table. Default value: false.
  google.protobuf.BoolValue asynchronous_metric_log_enabled = 52;

  // The maximum size that asynchronous_metric_log can grow to before old data will be removed. If set to 0 (default),
  // automatic removal of asynchronous_metric_log data based on size is disabled.
  google.protobuf.Int64Value asynchronous_metric_log_retention_size = 53 [(value) = ">=0"];

  // The maximum time that asynchronous_metric_log records will be retained before removal. If set to 0,
  // automatic removal of asynchronous_metric_log data based on time is disabled.
  google.protobuf.Int64Value asynchronous_metric_log_retention_time = 54 [(value) = ">=0"];

  // Enable or disable session_log system table. Default value: false.
  google.protobuf.BoolValue session_log_enabled = 57;

  // The maximum size that session_log can grow to before old data will be removed. If set to 0 (default),
  // automatic removal of session_log data based on size is disabled.
  google.protobuf.Int64Value session_log_retention_size = 58 [(value) = ">=0"];

  // The maximum time that session_log records will be retained before removal. If set to 0,
  // automatic removal of session_log data based on time is disabled.
  google.protobuf.Int64Value session_log_retention_time = 59 [(value) = ">=0"];

  // Enable or disable zookeeper_log system table. Default value: false.
  google.protobuf.BoolValue zookeeper_log_enabled = 60;

  // The maximum size that zookeeper_log can grow to before old data will be removed. If set to 0 (default),
  // automatic removal of zookeeper_log data based on size is disabled.
  google.protobuf.Int64Value zookeeper_log_retention_size = 61 [(value) = ">=0"];

  // The maximum time that zookeeper_log records will be retained before removal. If set to 0,
  // automatic removal of zookeeper_log data based on time is disabled.
  google.protobuf.Int64Value zookeeper_log_retention_time = 62 [(value) = ">=0"];

  // Enable or disable asynchronous_insert_log system table. Default value: false.
  // Minimal required ClickHouse version: 22.10.
  google.protobuf.BoolValue asynchronous_insert_log_enabled = 63;

  // The maximum size that asynchronous_insert_log can grow to before old data will be removed. If set to 0 (default),
  // automatic removal of asynchronous_insert_log data based on size is disabled.
  google.protobuf.Int64Value asynchronous_insert_log_retention_size = 64 [(value) = ">=0"];

  // The maximum time that asynchronous_insert_log records will be retained before removal. If set to 0,
  // automatic removal of asynchronous_insert_log data based on time is disabled.
  google.protobuf.Int64Value asynchronous_insert_log_retention_time = 65 [(value) = ">=0"];

  google.protobuf.Int64Value background_pool_size = 33 [(value) = ">0"];

  // Sets a ratio between the number of threads and the number of background merges and mutations that can be executed concurrently. For example, if the ratio equals to 2 and background_pool_size is set to 16 then ClickHouse can execute 32 background merges concurrently. This is possible, because background operations could be suspended and postponed. This is needed to give small merges more execution priority. You can only increase this ratio at runtime. To lower it you have to restart the server. The same as for background_pool_size setting background_merges_mutations_concurrency_ratio could be applied from the default profile for backward compatibility.
  // Default: 2
  // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings#background_merges_mutations_concurrency_ratio)
  google.protobuf.Int64Value background_merges_mutations_concurrency_ratio = 48 [(value) = ">0"];

  google.protobuf.Int64Value background_schedule_pool_size = 34 [(value) = ">0"];

  // Sets the number of threads performing background fetches for tables with **ReplicatedMergeTree** engines. Default value: 8.
  //
  // More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#background_fetches_pool_size).
  google.protobuf.Int64Value background_fetches_pool_size = 38 [(value) = ">0"];

  google.protobuf.Int64Value background_move_pool_size = 39 [(value) = ">0"];

  google.protobuf.Int64Value background_distributed_schedule_pool_size = 40 [(value) = ">0"];

  google.protobuf.Int64Value background_buffer_flush_schedule_pool_size = 41 [(value) = ">0"];

  google.protobuf.Int64Value background_message_broker_schedule_pool_size = 46 [(value) = ">0"];

  // The maximum number of threads that will be used for performing a variety of operations (mostly garbage collection) for *MergeTree-engine tables in a background.
  // Default: 8
  // See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings#background_common_pool_size)
  google.protobuf.Int64Value background_common_pool_size = 47 [(value) = ">0"];

  // The default database.
  //
  // To get a list of cluster databases, see [Yandex Managed ClickHouse documentation](/docs/managed-clickhouse/operations/databases#list-db).
  google.protobuf.StringValue default_database = 43;

  // Sets the memory size (in bytes) for a stack trace at every peak allocation step. Default value: **4194304**.
  //
  // More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#total-memory-profiler-step).
  google.protobuf.Int64Value total_memory_profiler_step = 44;

  google.protobuf.DoubleValue total_memory_tracker_sample_probability = 45;
}

message ClickhouseConfigSet {
  // Effective settings for a ClickHouse cluster (a combination of settings defined
  // in [user_config] and [default_config]).
  ClickhouseConfig effective_config = 1 [(required) = true];

  // User-defined settings for a ClickHouse cluster.
  ClickhouseConfig user_config = 2;

  // Default configuration for a ClickHouse cluster.
  ClickhouseConfig default_config = 3;
}
