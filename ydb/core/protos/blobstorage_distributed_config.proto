syntax = "proto3";

import "ydb/core/protos/config.proto";
import "ydb/core/protos/blobstorage_disk.proto";
import "ydb/core/protos/bridge.proto";
import "ydb/library/actors/protos/interconnect.proto";

package NKikimrBlobStorage;

message TNodeIdentifier {
    string Host = 1;
    uint32 Port = 2;
    uint32 NodeId = 3;
    NActorsInterconnect.TNodeLocation Location = 4;
    optional uint32 BridgePileId = 5;
}

message TStorageConfigMeta {
    uint64 Generation = 1;
    bytes Fingerprint = 2;
}

message TStorageConfig { // contents of storage metadata
    uint64 Generation = 1; // stored generation
    bytes Fingerprint = 2; // hash for config validation (must be the same for all nodes with the same Generation)
    NKikimrConfig.TBlobStorageConfig BlobStorageConfig = 3; // NodeWardenServiceSet for static group is inside
    NKikimrConfig.TDomainsConfig.TStateStorage StateStorageConfig = 9;
    NKikimrConfig.TDomainsConfig.TStateStorage StateStorageBoardConfig = 10;
    NKikimrConfig.TDomainsConfig.TStateStorage SchemeBoardConfig = 11;
    repeated TNodeIdentifier AllNodes = 5; // set of all known nodes
    string ClusterUUID = 6; // cluster GUID as provided in nameservice config
    string SelfAssemblyUUID = 7; // self-assembly UUID generated when config is first created
    optional bytes ConfigComposite = 12; // compressed stored storage config YAML (if stored)
    NKikimrConfig.TSelfManagementConfig SelfManagementConfig = 13;
    TStorageConfig PrevConfig = 100; // previous version of StorageConfig (if any)
    optional bytes CompressedStorageYaml = 14; // compressed storage YAML config (if dual-config is enabled)
    optional uint64 ExpectedStorageYamlVersion = 15;
    NKikimrBridge.TClusterState ClusterState = 16; // for bridged clusters; not set for ordinary ones
    NKikimrBridge.TClusterStateHistory ClusterStateHistory = 17; // history of unsynced state changes
}

message TPDiskMetadataRecord {
    TStorageConfig CommittedStorageConfig = 1; // currently active storage config
    TStorageConfig ProposedStorageConfig = 2; // proposed storage config
}

message TStorageFileContent {
    message TRecord {
        string Path = 1;
        fixed64 PDiskGuid = 2;
        uint64 Timestamp = 3;
        fixed64 Key = 4;
        TPDiskMetadataRecord Meta = 5;
        bool Unformatted = 6;
    }

    repeated TRecord Record = 1;
}

message TCacheUpdate {
    message TKeyValuePair {
        string Key = 1; // unique cache item key
        uint32 Generation = 2; // generation of the key: Value may only change with increasing generation
        optional bytes Value = 3; // updated value; when not set, this item is considered to be deleted
    }

    repeated TKeyValuePair KeyValuePairs = 1;
    repeated string RequestedKeys = 2;
}

// Attach sender node to the recipient one; if already bound, then just update configuration.
message TEvNodeConfigPush {
    message TBoundNode {
        TNodeIdentifier NodeId = 1;
        TStorageConfigMeta Meta = 2;
    }
    bool Initial = 1; // set to true if this push is initial connection establishment
    repeated TBoundNode BoundNodes = 2; // a list of bound node updates (including itself)
    repeated TNodeIdentifier DeletedBoundNodeIds = 3; // a list of detached nodes
    TCacheUpdate CacheUpdate = 4;
}

// Used to reverse-propagate configuration and to confirm/reject initial TEvNodePushBinding query.
message TEvNodeConfigReversePush {
    uint32 RootNodeId = 1; // current tree root as known by the sender, always nonzero
    bool Rejected = 2; // is the request rejected due to cyclic graph?
    TStorageConfig CommittedStorageConfig = 3; // last known committed storage configuration
    bool RecurseConfigUpdate = 4;
    TCacheUpdate CacheUpdate = 5;
}

// Remove node from bound list.
message TEvNodeConfigUnbind {
}

message TStorageSyncerInfo {
    uint32 NodeId = 1; // a node that contains running blobstorage syncer actor
    uint32 GroupId = 2; // bridged group id
    uint32 TargetBridgePileId = 3; // a target bridge pile sync goes for
}

// Propagate query to the tree bottom and collect replies.
message TEvNodeConfigScatter {
    message TCollectConfigs {
    }

    message TProposeStorageConfig {
        TStorageConfig Config = 1;
    }

    message TManageSyncers {
        repeated TStorageSyncerInfo RunSyncers = 1; // syncers expected to be running (suggests stopping the same ones on other nodes)
    }

    optional uint64 Cookie = 1;
    optional fixed64 TaskId = 4; // random (and not necessarily unique) identifier

    oneof Request {
        TCollectConfigs CollectConfigs = 2;
        TProposeStorageConfig ProposeStorageConfig = 3;
        TManageSyncers ManageSyncers = 5;
    }
}

// Collected replies from the bottom.
message TEvNodeConfigGather {
    message TCollectConfigs {
        message TNode {
            repeated TNodeIdentifier NodeIds = 1; // nodes with the same config
            TStorageConfig BaseConfig = 2; // config from config.yaml
        }
        message TDiskIdentifier {
            TNodeIdentifier NodeId = 1;
            string Path = 2;
            optional fixed64 Guid = 3;
        }
        message TPersistentConfig {
            repeated TDiskIdentifier Disks = 1; // disks with the same config
            TStorageConfig Config = 2;
        }
        repeated TNode Nodes = 1;
        repeated TPersistentConfig CommittedConfigs = 2;
        repeated TPersistentConfig ProposedConfigs = 3;
        repeated TDiskIdentifier NoMetadata = 4;
        repeated TDiskIdentifier Errors = 5;
    }

    message TProposeStorageConfig {
        enum EStatus {
            UNKNOWN = 0;
            ACCEPTED = 1;
            HAVE_NEWER_GENERATION = 2;
            RACE = 3;
            ERROR = 4;
            NO_STORAGE = 5; // nowhere to store configuration to
        }
        message TStatus {
            TNodeIdentifier NodeId = 1;
            EStatus Status = 2;
            string Reason = 3;

            message TDrive {
                string Path = 1;
                optional fixed64 Guid = 2;
            }
            reserved 4;
            repeated TDrive SuccessfulDrives = 5;
        }
        repeated TStatus Status = 1;
    }

    message TManageSyncers {
        message TNode {
            uint32 NodeId = 1;
            repeated TStorageSyncerInfo Syncers = 2;
        }
        repeated TNode Nodes = 1;
    }

    optional uint64 Cookie = 1;
    optional bool Aborted = 4;

    oneof Response {
        TCollectConfigs CollectConfigs = 2;
        TProposeStorageConfig ProposeStorageConfig = 3;
        TManageSyncers ManageSyncers = 5;
    }
}

message TStateStorageConfig {
    optional NKikimrConfig.TDomainsConfig.TStateStorage StateStorageConfig = 1;
    optional NKikimrConfig.TDomainsConfig.TStateStorage StateStorageBoardConfig = 2;
    optional NKikimrConfig.TDomainsConfig.TStateStorage SchemeBoardConfig = 3;
}

// Some kind of RPC -- this event can be sent to any NW, it will forward the request to root node.
message TEvNodeConfigInvokeOnRoot {
    message TUpdateConfig {
        TStorageConfig Config = 1;
    }

    message TQueryConfig
    {}

    message TReassignGroupDisk {
        NKikimrBlobStorage.TVDiskID VDiskId = 1; // which one to reassign
        optional NKikimrBlobStorage.TPDiskId PDiskId = 2; // where to put it (optional)
        bool ConvertToDonor = 3; // convert the current disk to donor?
        bool IgnoreGroupFailModelChecks = 4;
        bool IgnoreDegradedGroupsChecks = 5;
        bool IgnoreVSlotQuotaCheck = 6;
        bool IsSelfHealReasonDecommit = 7;
        bool FromSelfHeal = 8;
    }

    // Regenerate configuration so the slain VDisk is no more reported as DESTROY one in the list.
    message TStaticVDiskSlain {
        NKikimrBlobStorage.TVDiskID VDiskId = 1;
        NKikimrBlobStorage.TVSlotId VSlotId = 2;
    }

    message TDropDonor {
        NKikimrBlobStorage.TVDiskID VDiskId = 1;
        NKikimrBlobStorage.TVSlotId VSlotId = 2;
    }

    message TReassignStateStorageNode {
        uint32 From = 1;
        uint32 To = 2; // or zero to pick up automatically
        bool StateStorage = 3;
        bool StateStorageBoard = 4;
        bool SchemeBoard = 5;
    }

    message TGetStateStorageConfig {
        optional bool Recommended = 1;
    }

    message TSelfHealStateStorage {
        optional uint32 WaitForConfigStep = 1;
    }

    message TAdvanceGeneration
    {}

    message TFetchStorageConfig {
        reserved 1;
        bool MainConfig = 2;
        bool StorageConfig = 3;
        bool AddSectionsForMigrationToV1 = 5; // add sections so this config becomes suitable for migration to v1
        bool AddExplicitConfigs = 4; // add explicit settings for all managed entities
    }

    message TReplaceStorageConfig {
        optional string YAML = 1;
        optional string StorageYAML = 3;
        optional bool SwitchDedicatedStorageSection = 4;
        bool DedicatedStorageSectionConfigMode = 5;
        bool SkipConsoleValidation = 2;
        optional bytes UserToken = 6;
        optional string PeerName = 7;
    }

    message TBootstrapCluster {
        string SelfAssemblyUUID = 1;
    }

    message TSwitchBridgeClusterState {
        NKikimrBridge.TClusterState NewClusterState = 1;
        repeated uint32 SpecificBridgePileIds = 2; // obtain quorum only for specific pile set
    }

    message TNotifyBridgeSyncFinished {
        enum EStatus {
            Success = 0;
            TransientError = 1;
            PermanentError = 2;
        }
        uint64 Generation = 1; // generation matching ClusterState's one
        uint32 BridgePileId = 2; // which bridge pile is marked synchronized
        EStatus Status = 3; // status of this synchronization
        optional string ErrorReason = 4; // textual description of an error
        oneof Entity {
            uint32 GroupId = 5;
            bool BSC = 6;
        }
        repeated uint32 UnsyncedGroupIdsToAdd = 7;
    }

    message TMergeUnsyncedPileConfig {
        // the config of the pile that we are merging in right now
        TStorageConfig PileConfig = 1;
    }

    message TNegotiateUnsyncedConnection {
        uint32 NodeId = 1;
    }

    message TAdvanceClusterStateGeneration {
        uint64 Generation = 1;
    }

    oneof Request {
        TUpdateConfig UpdateConfig = 1;
        TQueryConfig QueryConfig = 2;
        TReassignGroupDisk ReassignGroupDisk = 3;
        TStaticVDiskSlain StaticVDiskSlain = 4;
        TDropDonor DropDonor = 5;
        TReassignStateStorageNode ReassignStateStorageNode = 6;
        TAdvanceGeneration AdvanceGeneration = 7;
        TFetchStorageConfig FetchStorageConfig = 8;
        TReplaceStorageConfig ReplaceStorageConfig = 9;
        TBootstrapCluster BootstrapCluster = 10;
        TSwitchBridgeClusterState SwitchBridgeClusterState = 11;
        TStateStorageConfig ReconfigStateStorage = 12;
        TGetStateStorageConfig GetStateStorageConfig = 13;
        TNotifyBridgeSyncFinished NotifyBridgeSyncFinished = 14;
        TSelfHealStateStorage SelfHealStateStorage = 15;
        TMergeUnsyncedPileConfig MergeUnsyncedPileConfig = 16;
        TNegotiateUnsyncedConnection NegotiateUnsyncedConnection = 17;
        TAdvanceClusterStateGeneration AdvanceClusterStateGeneration = 18;
    }
}

// Result of RPC invocation.
message TEvNodeConfigInvokeOnRootResult {
    enum EStatus {
        OK = 0; // request fulfilled successfully
        NO_QUORUM = 1; // root node did not have quorum of following nodes
        ERROR = 2; // failure during request execution
        RACE = 3; // race in requests with other entities
        CONTINUE_BSC = 4; // process this request with BSC
    }

    message TScepter {
        uint64 Id = 1;
        uint32 NodeId = 2;
    }

    message TQueryConfig {
        TStorageConfig Config = 1;
        TStorageConfig CurrentProposedStorageConfig = 2;
    }

    message TFetchStorageConfig {
        optional string YAML = 1;
        optional string StorageYAML = 2;
    }

    message TReplaceStorageConfig {
        optional bool AllowEnablingDistconf = 1;
    }

    message TMergeUnsyncedPileConfig {
        TStorageConfig MergedConfig = 1;
    }

    EStatus Status = 1;
    optional string ErrorReason = 2;
    TScepter Scepter = 3;

    reserved 4, 6, 7, 8, 9;

    oneof Response {
        TQueryConfig QueryConfig = 5;
        TFetchStorageConfig FetchStorageConfig = 10;
        TReplaceStorageConfig ReplaceStorageConfig = 11;
        TStateStorageConfig StateStorageConfig = 12;
        TMergeUnsyncedPileConfig MergeUnsyncedPileConfig = 13;
    }
}

message TEvNodeWardenDynamicConfigPush {
    TStorageConfig Config = 1;
    bool NoQuorum = 2;
    TCacheUpdate CacheUpdate = 3;
}

message TConnectivityPayload {
    TStorageConfig StorageConfig = 1; // storage config of a sender
    optional uint32 BridgePileId = 2; // bridge pile id of a sender
    optional uint32 RootNodeId = 3; // root node id of a sender (if set and zero, then this is the root)
    optional bool HasQuorumInPile = 4; // does the sender have quorum inside his pile? (when root)
    TEvNodeConfigInvokeOnRoot InvokeOnRoot = 5; // instruct the recipient to execute this command
}
