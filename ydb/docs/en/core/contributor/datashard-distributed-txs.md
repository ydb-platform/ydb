# DataShard: distributed transactions

{{ydb-short-name}} uses distributed transactions, which are based on ideas from Calvin (see [Calvin: A Scalable Distributed Transactions System](https://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf)). These transactions consist of a set of operations performed by a group of participants, such as DataShards. Unlike Calvin these operations are not required to be deterministic. To execute a distributed transaction, a proposer prepares the transaction at each participant, assigns a position (or timestamp) to the transaction in the global transaction execution order using one of the coordinator tablets, and collects the transaction results. Each participant receives and processes a subset of transactions it is involved in, following a specific order. Participants may process their part of the larger transaction at different speeds and not simultaneously. Distributed transactions share the same timestamp across all participating shards, and must include all changes from transactions with preceding timestamps. When viewed as a logical sequence, timestamps act as a single logical timeline, where any distributed transaction happens at the same point in time.

When the execution of a transaction depends on the state of other participants, the participants exchange data using so-called ReadSets. These are persistent messages exchanged between participants that are delivered at least once. The use of ReadSets causes transactions to go through additional phases:

1. **Reading phase**: The participant reads, persists and sends data that is needed by other participants. During this phase, KQP transactions (type `TX_KIND_DATA`, which have a non-empty `TDataTransaction.KqpTransaction` field and subtype `KQP_TX_TYPE_DATA`) validate optimistic locks. Older MiniKQL transactions (type `TX_KIND_DATA`, which have a non-empty `TDataTransaction.MiniKQL` field) perform reads and send arbitrary table data during this phase. Another example of using the reading phase is the distributed TTL transaction for deleting expired rows. The primary shard generates a bitmask of matching rows that have expired, ensuring that both the primary and index shards delete the same rows.
2. **Waiting phase**: The participant waits until it has received all the necessary data from the other participants.
3. **Execution phase**: The participant uses both local and remote data to determine whether to abort or complete the transaction. If the transaction is completed, the participant generates and applies the effects specified in the transaction body. The transaction body typically includes a program that uses the same data and leads all participants to come to the same conclusion.

Participants are allowed to execute transactions in any order for efficiency. However, it's important that this order cannot be observed by other transactions. Transaction ordering based on a coordinator's assigned timestamps ensures strict serializable isolation. In practice, single-shard transactions don't involve a coordinator, and shards use a locally consistent timestamp for such transactions. Due to variations in the arrival times of distributed transaction timestamps, this weakens the isolation level to serializable.

Version 24.1 of {{ydb-short-name}} has added support for "volatile" distributed transactions. These transactions allow participants, including coordinators, to store transaction data in volatile memory, which is lost when the shards are restarted, until the transaction is completed and the effects are persisted. This also allows participants to abort the transaction until the very last moment, which will be guaranteed to abort for all other participants. By using volatile memory, persistent storage is excluded from the critical path of the transaction execution, reducing latency.

When executing user's YQL transactions, {{ydb-short-name}} currently uses distributed transactions only for the final commit of non-read-only transactions. Individual queries are executed as single-shard operations before the commit, using optimistic locks and global multi-version concurrency control (MVCC) snapshots to ensure data consistency.

## Basic distributed transactions protocol

Operations that can be performed as distributed transactions in {{ydb-short-name}} include various types of participants. The basic protocol for distributed transactions is the same regardless of the type of transaction, with some notable differences in the schema changes, which have additional requirements to make sure these transactions are idempotent.

Distributed transactions are managed using proposer actors. Some examples of these are:

* [TKqpDataExecutor](https://github.com/ydb-platform/ydb/blob/main/ydb/core/kqp/executer_actor/kqp_data_executer.cpp) executes DML queries, including distributed commits.
* [SchemeShard](https://github.com/ydb-platform/ydb/tree/main/ydb/core/tx/schemeshard) executes distributed transactions for schema changes.
* [TDistEraser](https://github.com/ydb-platform/ydb/blob/main/ydb/core/tx/datashard/datashard_distributed_erase.cpp) executes a distributed transaction to consistently erase rows in tables with secondary indexes that match time-to-live (TTL) rules.

Distributed transactions in {{ydb-short-name}} are similar to two-phase commit protocols. The proposer actor goes through the following phases when executing a distributed transaction:

0. **Determining participants**: The proposer actor selects specific shards (`TabletId`) that are required for transaction execution. A table may consist of many shards (`DataShard` tablets with unique `TabletId` identifiers), but a particular transaction may only affect a smaller set of these shards based on the affected primary keys. This subset is fixed at the start of the transaction and cannot be changed later. Transactions that only affect a single shard are called "single-shard" transactions and are processed in what is known as the "immediate execution" mode.
1. **Prepare phase**: The proposer sends a special event, usually called `TEvProposeTransaction` (there is also `TEvWrite` variant in DataShards), which specifies a `TxId`, a transaction identifier, unique within a particular cluster, and includes the transaction body (operations and parameters). Participants validate whether the specified transaction can be executed, select a range of allowed timestamps, `MinStep` and `MaxStep`, and reply with a `PREPARED` status on success.
    * For single-shard transactions the proposer typically specifies an "immediate execution" mode (`Immediate`). The shard executes such transactions as soon as possible (at an unspecified timestamp consistent with other transactions) and replies with the result, rather than `PREPARED`, which causes the planning phase to be skipped. Some special single-shard operations, such as `TEvUploadRowsRequest` which implements `BulkUpsert`, don't even have a globally unique `TxId`.
    * The persistent transaction body is stored in the shard's local database, and the participant must ensure that it is executed when planned. In certain cases (for example, when performing a blind `UPSERT` into multiple shards), the participants must also ensure that the transaction is executed successfully, which may be in conflict with certain schema changes.
    * The volatile transaction body is stored in memory, and the participant responds with `PREPARED` as soon as possible. Future execution, whether successful or not, is not guaranteed in any way.
    * The proposer moves on to the next phase when they have received responses from all the participants.
    * It's not safe to re-send the propose event, except for schema operations, which thanks to special idempotent fields, guarantee that a particular transaction will be executed exactly once.
2. **Planning phase**: When the proposer has received `PREPARED` replies from all participants, it calculates the aggregated `MinStep` and `MaxStep` values and selects a  coordinator to assign the timestamp to the transaction. A `TEvTxProxy::TEvProposeTransaction` event is sent to the selected coordinator, which includes the `TxId` and the list of participants.
    * The transaction may only involve shards from the same database. Each shard attaches its [ProcessingParams](https://github.com/ydb-platform/ydb/blob/a68faed0a7b525a750d5f566e5c3fc60424cc91e/ydb/core/protos/subdomains.proto#L31) to the reply, which has the same list of coordinators when shards belong to the same database.
    * The coordinator is selected based on the received `ProcessingParams`, because historically it was possible to execute queries without specifying a database. The list of coordinators can only be found among the participants.
    * When the `TEvTxProxy::TEvProposeTransaction` event is re-sent (currently only for schema transactions), it is possible that the transaction may have multiple timestamps associated with it. This is not typically a problem as the transaction will execute at the earliest possible timestamp, and any later timestamps will be ignored (the transaction will have completed and been removed by the time they occur).
3. **Execution phase**: The proposer waits for responses from the selected coordinator and participants, collecting the overall transaction outcome.
    * In some cases, such as a temporary network disconnection or a shard restart, the proposer may try to re-establish the connection and wait for the result. This process may continue until the transaction has completed and the result is available.
    * When it is impossible to retrieve the result of a transaction from at least one the participants due to network issues, the transaction usually fails with an `UNDETERMINED` status, indicating that it is impossible to determine whether the transaction was successful or not.

## Prepare phase in the DataShard tablet

Distributed transactions in the DataShard tablet begin with the prepare phase, which is proposed used one of these events:

* [TEvDataShard::TEvProposeTransaction](https://github.com/ydb-platform/ydb/blob/c97ef92f814152462ae0374eafa093bca584d7b5/ydb/core/tx/datashard/datashard.h#L435) is an entry point for various transaction types
* [TDataEvents::TEvWrite](https://github.com/ydb-platform/ydb/blob/c97ef92f814152462ae0374eafa093bca584d7b5/ydb/core/tx/data_events/events.h#L38) is a special entry point for transactions that write data and commit

Events that don't have an `Immediate` execution mode specified begin the prepare phase for the distributed transaction. Transaction body is validated, checking whether it's even possible to execute (e.g., [CheckDataTxUnit](https://github.com/ydb-platform/ydb/blob/main/ydb/core/tx/datashard/check_data_tx_unit.cpp) for generic data transactions), and a range of timestamps is selected:

* `MinStep` is selected using the current mediator or wallclock time
* `MaxStep` is calculated using a planning deadline, which is currently 30 seconds for data transactions

Then the transaction is persisted to disk (for persistent transactions) or stored in memory (for volatile transactions), shard replies with the `PREPARED` status and starts waiting the plan which specifies `PlanStep` for the given `TxId`. Planning deadline is important when the proposer unexpectedly fails, since shard cannot determine whether the proposer successfully planned the transaction for some future timestamp, and shard must guarantee the transaction will execute when planned (unless it's volatile). Since transactions that are not planned yet block some concurrent operations (e.g. schema and partitioning changes), it uses a deadline that makes it impossible to plan the transaction after a certain timestamp. When mediator time surpasses `MaxStep` without a corresponding plan for the transaction, protocol guarantees it will not be possible to plan the transaction and it can be safely cleaned up.

Transactions are stored on disk and in memory using the [TTransQueue](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/tx/datashard/datashard_trans_queue.h#L25) class. Basic info about persistent transactions is stored in the [TxMain](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/tx/datashard/datashard_impl.h#L593) table, which is loaded in memory when DataShard starts. Potentially large transaction body is stored in the [TxDetails](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/tx/datashard/datashard_impl.h#L609) table and is not kept in memory while waiting. Transaction body is loaded in memory just before conflict analysis with other transactions in the pipeline.

Volatile transactions are only stored in memory, and are currently lost when DataShard is restarted (but may be migrated during graceful restarts in the future), which also aborts the transaction at all other participants. Abort of volatile transactions may also be initiated by any participant until transaction body is executed and effects are persisted. Shards use this for faster schema and partitioning changes, aborting all waiting transactions without waiting for the planning deadline.

Distributed transaction body must have enough information about other participants, so each shard knows when it has to generate and send outgoing ReadSets, and which shards have to expect and wait incoming ReadSets. KQP transactions currently use ReadSets for validating and committing optimistic locks, which is described using [TKqpLocks](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/protos/data_events.proto#L18) generated by `TKqpDataExecutor`. It specifies the following shard sets:

* `SendingShards` are shards which send ReadSets to all shards in the `ReceivingShards` set
* `ReceivingShards` are shards which expect ReadSets from all shards in the `SendingShards` set

Volatile transactions expect all shards to be in the `SendingShards` set (any shard may abort the transaction and needs to send its commit decision to other shards), and all shards that apply effects to be in the `ReceivingShards` set (whether or not effects are committed depends on decisions from other shards). Exchanged ReadSets data is the serialized [TReadSetData](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/protos/tx.proto#L312) message with a single `Decision` field specified.

An example of a distributed transaction that doesn't use ReadSets is a persistent distributed transaction with blind writes. In that case, after successful planning, transaction cannot be aborted, and shards must guarantee future success of this transaction during the prepare phase.

## Planning phase

When `PREPARED` replies have been gathered from all participants, the proposer computes the maximum `MinStep` and the minimum `MaxStep`, then select a coordinator (currently implemented using a `TxId` hash) and sends a `TEvTxProxy::TEvProposeTransaction` event which includes `TxId` and the list of participants, which includes a type of operation (read and/or write) at each participant for compatibility reasons, even though it's not used in practice. The selected coordinator chooses the closest matching `PlanStep` which is associated with the specified `TxId` and the list of participants. Persistent transactions are associated to plan steps allocated every 10ms (this setting is called a plan resolution), and this association is also persisted to disk. Volatile transactions are associated to the next plan step reserved for volatile planning, which can be allocated every 1ms, and this association is only stored in memory without touching the disk.

Planning each plan step (containing zero or more transactions) involves distributing involved participants (and transactions) to mediators, which is currently performed by hashing `TabletId` of the participant modulo the number of mediators. Each mediator receives a subset of each plan step in the increasing `PlanStep` order, and only includes its matching participants. This subset may be empty even when the whole plan step is not empty. Coordinator reconnects and resends all unacknowledged plan steps in the increasing `PlanStep` order when mediator is restarted or the network is temporarily disconnected.

Plan steps with persistent transactions are only sent to mediators after they are fully persisted to disk. The are only removed from coordinator's local database when acknowledged by participants, and are guaranteed to be delivered at least once. Plan steps with volatile transactions are only stored in memory, and may be lost when coordinator restarts. To accommodate volatile transactions plan step delivery guarantees have been changed starting with {{ydb-short-name}} version 24.1. When plan step is resent is may not include acknowledged transaction, and may not include previously sent volatile transactions even when they have not been acknowledged. This includes empty plan steps, only the last empty plan step is kept in memory and resent.

To reduce the number of errors during graceful restarts coordinators leave their state actor in memory even after the tablet stops working, with the address of this state actor persisted after the instance is fully initialized. When possible new instances contact this state actor and transfer the last known in-memory state. This state actor is also used to transfer an unused volatile planning reserve, and allows new instances to start faster without waiting until that reserve expires.

Mediators receive a stream of `TEvTxCoordinator::TEvCoordinatorStep` events from each coordinator and merge them by the matching `PlanStep` field. Merged plan steps with steps less or equal to the minimum of the last step received from each coordinator are considered complete and are sent to participants using `TEvTxProcessing::TEvPlanStep` events. Each participant receives an event with `PlanStep` specifying the timestamp and a list of `TxId` which must be executed at this timestamp. Transactions within each plan step are ordered by their `TxId`. The `(Step, TxId)` pairs are then used as the global MVCC version in the database.

Participants acknowledge that they received (and persisted in the case of persistent transaction) each plan step by sending `TEvTxProcessing::TEvPlanStepAccepted` event to the sender (which is a mediator tablet) and `TEvTxProcessing::TEvPlanStepAck` event to the coordinator tablet actor (specified in the `AckTo` field of each transaction). Plan steps and/or transactions are considered delivered when these events are processed, and will not be resent.

Based on `TEvTxProcessing::TEvPlanStepAccepted` events mediators also track which `PlanStep` (inclusive) has been delivered to all participants. This maximum `PlanStep` is known as the current mediator time and is distributed to nodes with running DataShards over subscriptions to the `TimeCast` service. The current mediator time shows that all possible `TEvTxProcessing::TEvPlanStep` events have been received and acknowledged by shards up to and including the specified `PlanStep`, and so shards must be aware of all transactions up to this timestamp. The current mediator time is mostly useful as it allows shards to be aware of the time progressing even when they don't participate in distributed transactions. Shards are currently partitioned to several timecast buckets at each mediator for efficiency, and the current time in a bucket advances when all participants of transactions in that bucket acknowledge their `TEvTxProcessing::TEvPlanStep` events. The current mediator time is available to shards when they subscribe by [sending TEvRegisterTablet event to the time cast service](https://github.com/ydb-platform/ydb/blob/73296225ea843dc36665fa82444093ced7660ba4/ydb/core/tx/datashard/datashard.cpp#L427) during shard startup and get the address of an atomic variable [from the TEvRegisterTabletResult event](https://github.com/ydb-platform/ydb/blob/73296225ea843dc36665fa82444093ced7660ba4/ydb/core/tx/datashard/datashard.cpp#L3392). This atomic variable allows the system to avoid broadcasting many frequent events to idle shards.

DataShards handle `TEvTxProcessing::TEvPlanStep` events in the [TTxPlanStep](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/datashard__plan_step.cpp#L19) transaction. Transactions found by the corresponding `TxId` get their assigned `Step` and are then added to the `Pipeline` in their assigned order using `PlanQueue` which limits the number of concurrently running transactions.

## Execution phase in the DataShard tablet

The [PlanQueue](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/plan_queue_unit.cpp#L44) unit allows distributed transactions to start subject to concurrency limits and in the increasing `(Step, TxId)` order. The transaction body is loaded from disk when needed (for evicted persistent transactions), KQP transactions [finalize execution plan](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/datashard_active_transaction.cpp#L667) and arrive at the [BuildAndWaitDependencies](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/build_and_wait_dependencies_unit.cpp#L72) unit. This unit analyzes transaction keys and ranges declared for reading and writing, and may add dependencies on earlier conflicting transactions. For example, when transaction `A` writes to key `K` and a later transaction `B` reads from key `K` then transaction `B` depends on transaction `A` and transaction `B` cannot start until transaction `A` completes. Transactions leave `BuildAndWaitDependencies` when they no longer have direct dependencies on other transactions.

Next persistent KQP transactions execute the read phase (this includes validating optimistic locks) and generate outgoing OutReadSets in the [BuildKqpDataTxOutRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/build_kqp_data_tx_out_rs_unit.cpp#L44) unit. Then the [StoreAndSendOutRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/store_and_send_out_rs_unit.cpp#L42) persists optimistic locks access log and outgoing ReadSets. Optimistic locks that had attached uncommitted changes are [marked with the `Frozen` flag](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/store_and_send_out_rs_unit.cpp#L67), which prevents their abort until transaction completes. Otherwise lock validity is guaranteed by assigning writes with higher MVCC version and the correct execution order of conflicting transactions. Operations that had access log or outgoing ReadSets persisted is [added to the `Incomplete` set](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/store_and_send_out_rs_unit.cpp#L79), which ensures that new writes cannot change the validity of performed reads and generally have to use a higher MVCC version, but new reads don't necessarily have to block on the result of the incomplete transaction and are free to use a lower MVCC version as long as it's consistent with other transactions.

Persistent KQP transactions prepare data structures for incoming InReadSets in the [PrepareKqpDataTxInRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/prepare_kqp_data_tx_in_rs_unit.cpp#L31) unit and begin waiting for all necessary ReadSets from other participants in the [LoadAndWaitInRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/load_and_wait_in_rs_unit.cpp#L36) unit. In some cases (e.g. blind writes to multiple shards without lock validation) distributed transactions may not need to exchange ReadSets and ReadSet-related units above don't perform any actions.

Finally, KQP transaction operation reaches the [ExecuteKqpDataTx](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L59) unit, which validates local optimistic locks (using previously persisted `AccessLog` when present), validates ReadSets received from other participants, and given everything checks out executes transaction body and returns the result. When lock validation (either local or remote) fails the transaction body is not executed and operation fails with the `ABORTED` status.

### Volatile transactions

Volatile distributed transactions are only stored in memory, and are currently lost when the shard is restarted. They must nevertheless guarantee the distributed transaction will either commit at all participants, or will be aborted at all participants in a timely manner. The big upside is no need for separate read, wait and execution phases, they are atomically executed in the single execution phase with 1RTT storage latency on the hot path from distributed commit start to the successful reply. Because they are atomically executed they usually don't stall the pipeline and may increase transaction throughput. Since any shard can abort a volatile distributed transaction without waiting for planning deadline it also limits shard unavailability during partitioning and schema changes.

Volatile transactions are based on [persistent uncommitted changes](localdb-uncommitted-txs.md) in the local database. During execution phase DataShard optimistically assumes all remote locks will validate, applies effects in the form of uncommitted changes (using the globally unique `TxId` of the distributed transaction) and adds the transaction record to the [VolatileTxManager](https://github.com/ydb-platform/ydb/blob/1f1b84d1d160a2b1cfe4298b271c0078ec1602b1/ydb/core/tx/datashard/volatile_tx.cpp#L439) in the waiting state for decisions by other participants. Successful reply is only sent when all effects are persistent, and a commit decision has been received from all other participants. Reads use the `TxMap` to observe all pending changes and check their status using the `TxObserver`. When the read result is dependent on whether or not the preceding volatile transaction commits, operations subscribe to the transaction status and restart after the transaction is decided (either reading now committed changes, or skipping them after an abort).

Having uncommitted and not yet aborted changes limits shard's ability to perform blind writes. Since uncommitted changes must commit in the same order they have been applied to any given key, and shards don't keep these keys in memory after transactions have been executed, DataShard needs to read each key and find conflicts before it can do any write. These conflicting changes may either be uncommitted changes attached to an optimistic lock (and these locks need to be broken with changes rolled back), or uncommitted changes belonging to waiting volatile transactions. We don't want to block writes unnecessarily, so even non-volatile operations may switch to a volatile commit. Such operations allocate `GlobalTxId` when necessary (this is a per-cluster unique `TxId` needed when not provided by the request, e.g. in `BulkUpsert`) and write changes to conflicting keys as uncommitted. The transaction record is then added to `VolatileTxManager` without specifying other participants, becomes initially committed and specifies a list of dependencies. These transactions don't block reads and are eventually committed in the local database after all dependencies have also been committed or aborted.

To reduce stalls in the pipeline transactions use the conflict cache for keys that are declared for writes in distributed transactions. These keys are read while transactions wait in the queue, and conflicting transaction sets are cached by the [RegisterDistributedWrites](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/build_and_wait_dependencies_unit.cpp#L95) function call. All writes to cached keys update these conflict sets and keep them up to date. This allows distributed transactions with writes to execute faster by processing lists of conflicting transactions using a hash-table lookup even when table data is evicted from cache.

DataShard may have change collectors (e.g. async indexes and/or CDC). Collecting these changes for volatile transactions is similar to [uncommitted changes in transactions](datashard-locks-and-change-visibility.md). It generates a stream of uncommitted change records using `TxId` as its `LockId`, which are then either atomically added to shard change records on commit, or deleted on abort. Depending on change collector settings it may also need to read the current row state. When this row state depends on other waiting volatile transactions this is handled similar to any other read by adding a dependency and restarting when those dependencies are resolved. This may stall the transaction pipeline, but it's not conceptually different from persistent distributed transactions with ReadSets stalled by their read-write conflicts.

Volatile transactions also [generate](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L193) and [write to disk](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L367) OutReadSets for all other participants specified in the `ReceivingShards` sets in the same local database transaction that writes uncommitted effects. When uncommitted changes are persistent these ReadSets are [sent](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L496) to other participants, which notifies them this shard is ready to commit the distributed transaction and won't change its decision unless the transaction is aborted by another participant.

Shards forget about volatile transactions (that did not persist their transaction record) when they are restarted and will not send outgoing ReadSets for transactions they no longer know nothing about. Shards that successfully execute the transaction body send a [special event](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L363) called ReadSet Expectation that informs about them are waiting for an incoming ReadSet. This is done even before effects are persisted, so shards have a chance to find out about aborted transactions as early as possible. Restarted shards will see a request for transaction they know nothing about and reply with a [special ReadSet without data](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/datashard__readset.cpp#L48). This way all participants find out about aborted transactions and abort them as well.

### Volatile transaction guarantees

To reiterate volatile transactions are stored in memory of both coordinators and participants, and may fail to various reasons. It's important to show why volatile transactions either commits or aborts at all participants, and it's impossible for transaction to only commit at a subset of participants.

Some examples of abort sources for the transaction:

1. Any shard may unexpectedly restart, including the case where a new instance has started while the old instance is still running and unaware of the new instance. Since transaction is only stored in memory, new instance may not be aware of a transaction that failed to store its effects to disk. Transaction may have also successfully executed and committed, leaving no traces except its committed effects.
2. Coordinator may unexpectedly restart without an in-memory state transfer, and the transaction may have been sent to some participants but not all of them.
3. Any shard may have decided to abort the transaction with an error.

An important guarantee is when any shard has successfully persisted uncommitted effects and the transaction record, and also received `DECISION_COMMIT` from all other participants, then this transaction will eventually commit at all participants and cannot be aborted. It's also important that when any shard aborts the transaction with an error (including forgetting about the transaction due to a restart) it will never be able to commit and will eventually abort.

These guarantees are derived from the following:

1. ReadSets with `DECISION_COMMIT` are sent to other participants only after all uncommitted changes, outgoing ReadSets and transaction record have been persisted to disk. So `DECISION_COMMIT` messages are persistent and will be delivered to other participants until acknowledged. These messages will be delivered even when transaction is aborted by another shard.
2. ReadSets are only acknowledged either when shard successfully writes something to disk, or when readonly lease is active when an unexpected ReadSet is received. In particular, an old tablet instance cannot acknowledge a ReadSet for an unknown transaction that has been prepared and executed by a newer generation. Local database guarantees a new tablet generation will not be activated until readonly lease at older generations has expired (as long as monotonic clock frequency does not differ more than twice). A successful write also confirms that the tablet had the storage lock at the start of the commit and no newer generation could be running at that time.
    * Volatile transactions use an optimization where received ReadSets are never written to disk. Acknowledgement is only sent after a the commit or abort (removal) of the transaction record is fully persistent on disk.
    * When all participants have generated and persisted their outgoing `DECISION_COMMIT` ReadSets they will keep receiving the full set of commit decisions until they fully commit transaction effects and send their acknowledgements.
3. Any message related to an aborted transaction is only sent after its effects are written to disk. This is needed to handle races between multiple generations of a particular generation running at the same time. After a newer generation successfully commits a volatile transaction and acknowledges ReadSets an older generation (which will fail when trying to access the disk or validate a readonly lease) may receive a `NO_DATA` reply to their ReadSet Expectation (which was acknowledged by a newer generation). However an older generation will not be able to commit the removal of the transaction record and will not be able to erroneously reply with an error. When the removal of the transaction record (and effects) is fully committed, current generation can be sure newer generations did not and will not start with this transaction in the waiting state and it will not be committed.
4. A successful reply after gathering all `DECISION_COMMIT` ReadSets does not have to wait for the final commit in the local database, but has to wait until uncommitted effects and transaction record has been persisted. This allows successful replies to have a 1RTT storage latency on the hot path, which is the write of uncommitted changes and the transaction record. This successful reply is stable:
    * In the case shard is restarted, effects and transaction record are persistent, so in the worst case it will be restored in the `Waiting` state.
    * From the MVCC viewpoint the transaction has already been completed, any new read will have an MVCC version that includes these changes. Any new read will start waiting for the transaction to resolve, which is now guaranteed to succeed.
    * The restored `Waiting` transaction could not have its incoming ReadSets acknowledged, so they will be resent and since we received `DECISION_COMMIT` earlier it will be received again, and transaction will quickly move to the `Committed` state.
5. As long as the transaction is enqueued, and especially as long as its `PlanStep` is not yet known, incoming ReadSets are attached in memory, and will not be acknowledged. Acknowledgements are delayed until the transaction is fully committed, or until it is aborted early (in which case no effects and no transaction record have been committed, and newer generations cannot recover and commit the transaction).

Volatile distributed transactions also guarantee change visibility to be stable across multiple shards participating in the same distributed transaction. For example, persistent distributed transactions allow the following anomaly:

1. Distributed transaction `Tx1` performs a blind write to keys `x` and `y` in two different shards.
2. Single-shard transaction `Tx2` reads key `x` and observes a value written by `Tx1`.
3. Single-shard transaction `Tx3` (which starts after `Tx2` has completed) reads key `y` and does not observe a value written by `Tx1`.

This anomaly may be observed when the shard with key `x` quickly receives its plan step with the transaction `Tx1` and performs the write. A read in `Tx2` which arrives a bit later will have its local MVCC version assigned that includes changes to key `x` and observe the change. The shard with key `y` may run on a node that's a bit slower and may not even be aware of the plan step with the transaction `Tx1` when a read in `Tx3` arrives. Since its time cast bucket doesn't have `Tx1` acknowledged its mediator time will stay in the past, and `Tx3` will have its local MVCC version assigned that doesn't include changes to key `y`. From the user's perspective `Tx2` happened before `Tx3`, but in the global serializable order turned out to be `Tx3`, `Tx1`, `Tx2`.

Volatile transactions cannot observe this particular anomaly since a read that observed changes to key `x` must have received `DECISION_COMMIT` from the other shard with key `y`. This means the transaction record was persisted at both shards, and crucially the shard with key `y` had it marked as completed. A read from key `y` will have to choose a local MVCC version that includes changes by `Tx1` and will have to wait until `Tx1` is resolved.

In other words, once a client observed the result of a volatile transaction, all subsequent reads will also observe the result of that volatile transaction and changes stability is not violated.

### Indirect planning of volatile transactions

Coordinator restart may cause plan steps to only arrive at a subset of participants of a volatile distributed transaction. In that case some shards may execute the transaction and start waiting for ReadSets while other shards will keep waiting for the plan step to arrive. Since planning timeout is 30 seconds this could have causes some transaction to have excessive delays until they are eventually aborted.

When any participant receives the first plan step of a volatile transaction it will include `PlanStep` within ReadSets it sends to other participants. Other participants then indirectly learn the `PlanStep` that was assigned to the transaction, and remember it as `PredictedPlanStep`. Even when their own plan step is lost DataShard will add the transaction to the PlanQueue when its mediator time (directly or indirectly) reaches `PredictedPlanStep` as if it received a plan step with this predicted `PlanStep` and `TxId`. In the case where `PredictedPlanStep` is in the past the transaction is quickly aborted as if it reached the planning deadline.

Since the transaction may be planned to different plan steps, some of which may be lost, different participants may have a different `PlanStep` assigned to the same transaction and the same transaction will try to execute at different timestamps. DataShards validate that `DECISION_COMMIT` messages have their `PlanStep` matching their assigned `PlanStep`, and only consider them when steps match. Mismatching plan steps cause the transaction to abort.
