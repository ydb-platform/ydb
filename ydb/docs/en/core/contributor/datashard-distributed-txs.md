# DataShard: distributed transactions

{{ydb-short-name}} uses distributed transactions based on ideas from [Calvin](https://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf). These transactions consist of operations (not necessarily deterministic) that are preformed at a predetermined set of participants (e.g. DataShards). Executing a transaction involves preparing it on all participants and assigning a position (timestamp) in the global transaction execution order using one of coordinator tablets in the database. Each participant receives and executes an ordered stream of transactions that it is involved in. Participants may receive and execute their parts of a larger transaction with varying speed and not necessarily simultaneously. However, any particular distributed transaction has the same timestamp at all participants, and must include all effects with preceding timestamps, so when viewed as a logical clock (based on global timestamps) these distributed transaction are performed at the same point in logical time.

When transaction execution depends on the state of other participants, participants exchange data using so called ReadSets, which are persistent messages between participants, delivered at least once. ReadSets cause the transaction to have some additional phases:

1. Reading phase. Participant reads, persists and sends data needed for other participants. Currently commits of KQP transactions (transaction type `TX_KIND_DATA` with a non-empty field `TDataTransaction.KqpTransaction` and type `KQP_TX_TYPE_DATA`) validate optimistic locks in this phase. Older MiniKQL transactions (transaction type `TX_KIND_DATA` with a non-empty field `TDataTransaction.MiniKQL`) used this phase to read and send arbitrary table data. Distributed TTL transaction for erasing expired rows is another example of using this phase for reads, the primary shard generates a bit mask of matching expired rows, making sure primary and index shards delete the same set of rows.
2. Waiting phase. Participant waits until it receives all the necessary data from other participants.
3. Execution phase. Participant uses local and remote data to decide whether to abort or execute the transaction, generating and applying effects using the transaction body. Transaction body usually has a program that uses the same data and causes all participants to arrive at the same decision.

Participants are allowed to execute transactions in a different order for efficiency, however it's important that this reordering cannot be observed by reads and writes from concurrent transactions. Transaction ordering based on coordinator assigned timestamps ensures strict serializable isolation. In practice, single-shard transactions do not involve coordinator tablets and shards assign a locally consistent timestamp to such transactions. Due to variations in transaction stream arrival times this weakens transaction isolation to serializable.

Version 24.1 of {{ydb-short-name}} added support for "volatile" distributed transaction. These transactions allow participants (including coordinator) to store transaction information in volatile memory (which is lost when shards are restarted) until transaction is executed and effects are persisted. This also allows participants to abort transactions (which is guaranteed to abort at all other participants) until the very last moment. Using volatile memory excludes persistent storage from the hot path of transaction execution and reduces latency.

When executing user's YQL transactions {{ydb-short-name}} currently only uses distributed transaction for the final commit, which applies effects in read-write transactions. Individual queries before the commit are executed as single-shard operations, ensuring consistency using optimistic locks and global MVCC snapshots.

## Basic distributed transactions protocol

Operations that may be executed as distributed transactions in {{ydb-short-name}} and types of supported participants are extended over time. Basic protocol for distributed transactions is however the same regardless of transaction type, with notable differences in schema changes, which have additional requirements to make sure these transactions are idempotent.

Distributed transactions are orchestrated using proposer actors, examples of which are:

* [TKqpDataExecutor](https://github.com/ydb-platform/ydb/blob/main/ydb/core/kqp/executer_actor/kqp_data_executer.cpp) executes DML queries, including distributed commits
* [SchemeShard](https://github.com/ydb-platform/ydb/tree/main/ydb/core/tx/schemeshard) executes distributed transactions for schema changes
* [TDistEraser](https://github.com/ydb-platform/ydb/blob/main/ydb/core/tx/datashard/datashard_distributed_erase.cpp) executes distributed transactions for consistently erasing rows in tables with secondary indexes that match TTL rules

Distributed transactions in {{ydb-short-name}} are a bit similar to two-phase commits. Overall the proposer actor has the following phases of distributed transaction execution:

0. Determining participants. The proposer actor selects specific shards (`TabletId`) that are required for transaction execution. For example, a table may consist of many shards (`DataShard` tablets with unique `TabletId` identifiers), but a particular transaction may be limited to a smaller set of shards based on affected primary keys. This shard set is fixed at the start of the transaction and cannot be extended later. Transactions that affect a single shard are called single-shard transactions and may be executed as immediate.
1. Prepare phase. Proposer sends a special event (usually called `TEvProposeTransaction`, in the case of DataShard there is a `TEvWrite` variant), which specifies a `TxId` (a transaction identifier, unique within a particular cluster), and includes operations and parameters (transaction body). Participants validate whether it is possible to execute the specified transaction, select a range of allowed timestamps (`MinStep` and `MaxStep`), and reply with a `PREPARED` status on success.
    * For single-shard transactions the proposer usually specifies an "immediate execution" mode (`Immediate`). Shard executes such transactions as soon as possible (at an unspecified timestamp consistent with other transactions), and replies with the result instead of `PREPARED`, which causes planning phase to be skipped. Some special single-shard operations (e.g. `TEvUploadRowsRequest` which implements `BulkUpsert`) don't even have a globally unique `TxId`.
    * Persistent transaction body is persisted in the shard's local database and participant must guarantee that it will be executed when planned. In some cases (e.g. blind `UPSERT` into multiple shards) participants must also guarantee it will be executed successfully, which may conflict with certain schema changes.
    * Volatile transaction body is stored in memory and participant replies with `PREPARED` as soon as possible. Future execution (whether successful or not) is not guaranteed in any way.
    * Proposer moves to the next phase when it gathers replies from all participants.
    * It's not safe to retry sending the propose event, except for schema operations which thanks to special idempotency fields guarantee a particular transaction will execute exactly once.
2. Planning phase. When the proposer receives `PREPARED` replies from all participants, it computes aggregated `MinStep` and `MaxStep` values and selects a  coordinator used for assigning timestamp to the transaction. A `TEvTxProxy::TEvProposeTransaction` event is sent to the selected coordinator which includes `TxId` and a list of participants.
    * Transaction may only involve shards from the same database. Every shard includes its [ProcessingParams](https://github.com/ydb-platform/ydb/blob/a68faed0a7b525a750d5f566e5c3fc60424cc91e/ydb/core/protos/subdomains.proto#L31) in the reply, which has the same list of coordinators for shards in the same database.
    * Coordinator is selected based on received `ProcessingParams` because it was historically possible to execute queries without specifying a database, and the list of coordinators could only be found from participants.
    * When `TEvTxProxy::TEvProposeTransaction` event is retried (currently only for schema transactions) it is possible for the transaction to have multiple timestamps assigned. This is usually not a problem, the transaction executes at the minimum timestamp, and all other timestamps are ignored (transaction is executed and removed by that time).
3. Execution and reply gathering phase. The proposer waits for replies from the selected coordinator and participants, gathering the overall transaction result.
    * In some cases (temporary disconnect, shard restart) the proposer may attempt to restore the connection and keep waiting for the result unless the transaction has executed and the result has been lost.
    * When it is impossible to acquire the transaction result from at least one participant due to network errors, transaction usually fails with the `UNDETERMINED` status, which signifies it's impossible to know whether the transaction executed successfully or not.

## Prepare phase in the DataShard tablet

Distributed transactions in the DataShard tablet begin with the prepare phase, which is proposed used one of these events:

* [TEvDataShard::TEvProposeTransaction](https://github.com/ydb-platform/ydb/blob/c97ef92f814152462ae0374eafa093bca584d7b5/ydb/core/tx/datashard/datashard.h#L435) is an entry point for various transaction types
* [TDataEvents::TEvWrite](https://github.com/ydb-platform/ydb/blob/c97ef92f814152462ae0374eafa093bca584d7b5/ydb/core/tx/data_events/events.h#L38) is a special entry point for transactions that write data and commit

Events that don't have an `Immediate` execution mode specified begin the prepare phase for the distributed transaction. Transaction body is validated, checking whether it's even possible to execute (e.g., [CheckDataTxUnit](https://github.com/ydb-platform/ydb/blob/main/ydb/core/tx/datashard/check_data_tx_unit.cpp) for generic data transactions), and a range of timestamps is selected:

* `MinStep` is selected using the current mediator or wallclock time
* `MaxStep` is calculated using a planning deadline, which is currently 30 seconds for data transactions

Then the transaction is persisted to disk (for persistent transactions) or stored in memory (for volatile transactions), shard replies with the `PREPARED` status and starts waiting the plan which specifies `PlanStep` for the given `TxId`. Planning deadline is important when the proposer unexpectedly fails, since shard cannot determine whether the proposer successfully planned the transaction for some future timestamp, and shard must guarantee the transaction will execute when planned (unless it's volatile). Since transactions that are not planned yet block some concurrent operations (e.g. schema and partitioning changes), it uses a deadline that makes it impossible to plan the transaction after a certain timestamp. When mediator time surpasses `MaxStep` without a corresponding plan for the transaction, protocol guarantees it will not be possible to plan the transaction and it can be safely cleaned up.

Transactions are stored on disk and in memory using the [TTransQueue](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/tx/datashard/datashard_trans_queue.h#L25) class. Basic info about persistent transactions is stored in the [TxMain](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/tx/datashard/datashard_impl.h#L593) table, which is loaded in memory when DataShard starts. Potentially large transaction body is stored in the [TxDetails](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/tx/datashard/datashard_impl.h#L609) table and is not kept in memory while waiting. Transaction body is loaded in memory just before conflict analysis with other transactions in the pipeline.

Volatile transactions are only stored in memory, and are currently lost when DataShard is restarted (but may be migrated during graceful restarts in the future), which also aborts the transaction at all other participants. Abort of volatile transactions may also be initiated by any participant until transaction body is executed and effects are persisted. Shards use this for faster schema and partitioning changes, aborting all waiting transactions without waiting for the planning deadline.

Distributed transaction body must have enough information about other participants, so each shard knows when it has to generate and send outgoing ReadSets, and which shards have to expect and wait incoming ReadSets. KQP transactions currently use ReadSets for validating and committing optimistic locks, which is described using [TKqpLocks](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/protos/data_events.proto#L18) generated by `TKqpDataExecutor`. It specifies the following shard sets:

* `SendingShards` are shards which send ReadSets to all shards in the `ReceivingShards` set
* `ReceivingShards` are shards which expect ReadSets from all shards in the `SendingShards` set

Volatile transactions expect all shards to be in the `SendingShards` set (any shard may abort the transaction and needs to send its commit decision to other shards), and all shards that apply effects to be in the `ReceivingShards` set (whether or not effects are committed depends on decisions from other shards). Exchanged ReadSets data is the serialized [TReadSetData](https://github.com/ydb-platform/ydb/blob/a833a4359ba77706f9b1fe4104741ef0acfbc83b/ydb/core/protos/tx.proto#L312) message with a single `Decision` field specified.

An example of a distributed transaction that doesn't use ReadSets is a persistent distributed transaction with blind writes. In that case, after successful planning, transaction cannot be aborted, and shards must guarantee future success of this transaction during the prepare phase.

## Planning phase

When `PREPARED` replies have been gathered from all participants, the proposer computes the maximum `MinStep` and the minimum `MaxStep`, then select a coordinator (currently implemented using a `TxId` hash) and sends a `TEvTxProxy::TEvProposeTransaction` event which includes `TxId` and the list of participants, which includes a type of operation (read and/or write) at each participant for compatibility reasons, even though it's not used in practice. The selected coordinator chooses the closest matching `PlanStep` which is associated with the specified `TxId` and the list of participants. Persistent transactions are associated to plan steps allocated every 10ms (this setting is called a plan resolution), and this association is also persisted to disk. Volatile transactions are associated to the next plan step reserved for volatile planning, which can be allocated every 1ms, and this association is only stored in memory without touching the disk.

Planning each plan step (containing zero or more transactions) involves distributing involved participants (and transactions) to mediators, which is currently performed by hashing `TabletId` of the participant modulo the number of mediators. Each mediator receives a subset of each plan step in the increasing `PlanStep` order, and only includes its matching participants. This subset may be empty even when the whole plan step is not empty. Coordinator reconnects and resends all unacknowledged plan steps in the increasing `PlanStep` order when mediator is restarted or the network is temporarily disconnected.

Plan steps with persistent transactions are only sent to mediators after they are fully persisted to disk. The are only removed from coordinator's local database when acknowledged by participants, and are guaranteed to be delivered at least once. Plan steps with volatile transactions are only stored in memory, and may be lost when coordinator restarts. To accommodate volatile transactions plan step delivery guarantees have been changed starting with {{ydb-short-name}} version 24.1. When plan step is resent is may not include acknowledged transaction, and may not include previously sent volatile transactions even when they have not been acknowledged. This includes empty plan steps, only the last empty plan step is kept in memory and resent.

To reduce the number of errors during graceful restarts coordinators leave their state actor in memory even after the tablet stops working, with the address of this state actor persisted after the instance is fully initialized. When possible new instances contact this state actor and transfer the last known in-memory state. This state actor is also used to transfer an unused volatile planning reserve, and allows new instances to start faster without waiting until that reserve expires.

Mediators receive a stream of `TEvTxCoordinator::TEvCoordinatorStep` events from each coordinator and merge them by the matching `PlanStep` field. Merged plan steps with steps less or equal to the minimum of the last step received from each coordinator are considered complete and are sent to participants using `TEvTxProcessing::TEvPlanStep` events. Each participant receives an event with `PlanStep` specifying the timestamp and a list of `TxId` which must be executed at this timestamp. Transactions within each plan step are ordered by their `TxId`. The `(Step, TxId)` pairs are then used as the global MVCC version in the database.

Participants acknowledge that they received (and persisted in the case of persistent transaction) each plan step by sending `TEvTxProcessing::TEvPlanStepAccepted` event to the sender (which is a mediator tablet) and `TEvTxProcessing::TEvPlanStepAck` event to the coordinator tablet actor (specified in the `AckTo` field of each transaction). Plan steps and/or transactions are considered delivered when these events are processed, and will not be resent.

Based on `TEvTxProcessing::TEvPlanStepAccepted` events mediators also track which `PlanStep` (inclusive) has been delivered to all participants. This maximum `PlanStep` is known as the current mediator time and is distributed to nodes with running DataShards over subscriptions to the `TimeCast` service. The current mediator time shows that all possible `TEvTxProcessing::TEvPlanStep` events have been received and acknowledged by shards up to and including the specified `PlanStep`, and so shards must be aware of all transactions up to this timestamp. The current mediator time is mostly useful as it allows shards to be aware of the time progressing even when they don't participate in distributed transactions. Shards are currently partitioned to several timecast buckets at each mediator for efficiency, and the current time in a bucket advances when all participants of transactions in that bucket acknowledge their `TEvTxProcessing::TEvPlanStep` events. The current mediator time is available to shards when they subscribe by [sending TEvRegisterTablet event to the time cast service](https://github.com/ydb-platform/ydb/blob/73296225ea843dc36665fa82444093ced7660ba4/ydb/core/tx/datashard/datashard.cpp#L427) during shard startup and get the address of an atomic variable [from the TEvRegisterTabletResult event](https://github.com/ydb-platform/ydb/blob/73296225ea843dc36665fa82444093ced7660ba4/ydb/core/tx/datashard/datashard.cpp#L3392). This atomic variable allows the system to avoid broadcasting many frequent events to idle shards.

DataShards handle `TEvTxProcessing::TEvPlanStep` events in the [TTxPlanStep](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/datashard__plan_step.cpp#L19) transaction. Transactions found by the corresponding `TxId` get their assigned `Step` and are then added to the `Pipeline` in their assigned order using `PlanQueue` which limits the number of concurrently running transactions.

## Execution phase in the DataShard tablet

The [PlanQueue](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/plan_queue_unit.cpp#L44) unit allows distributed transactions to start subject to concurrency limits and in the increasing `(Step, TxId)` order. The transaction body is loaded from disk when needed (for evicted persistent transactions), KQP transactions [finalize execution plan](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/datashard_active_transaction.cpp#L667) and arrive at the [BuildAndWaitDependencies](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/build_and_wait_dependencies_unit.cpp#L72) unit. This unit analyzes transaction keys and ranges declared for reading and writing, and may add dependencies on earlier conflicting transactions. For example, when transaction `A` writes to key `K` and a later transaction `B` reads from key `K` then transaction `B` depends on transaction `A` and transaction `B` cannot start until transaction `A` completes. Transactions leave `BuildAndWaitDependencies` when they no longer have direct dependencies on other transactions.

Next persistent KQP transactions execute the read phase (this includes validating optimistic locks) and generate outgoing OutReadSets in the [BuildKqpDataTxOutRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/build_kqp_data_tx_out_rs_unit.cpp#L44) unit. Then the [StoreAndSendOutRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/store_and_send_out_rs_unit.cpp#L42) persists optimistic locks access log and outgoing ReadSets. Optimistic locks that had attached uncommitted changes are [marked with the `Frozen` flag](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/store_and_send_out_rs_unit.cpp#L67), which prevents their abort until transaction completes. Otherwise lock validity is guaranteed by assigning writes with higher MVCC version and the correct execution order of conflicting transactions. Operations that had access log or outgoing ReadSets persisted is [added to the `Incomplete` set](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/store_and_send_out_rs_unit.cpp#L79), which ensures that new writes cannot change the validity of performed reads and generally have to use a higher MVCC version, but new reads don't necessarily have to block on the result of the incomplete transaction and are free to use a lower MVCC version as long as it's consistent with other transactions.

Persistent KQP transactions prepare data structures for incoming InReadSets in the [PrepareKqpDataTxInRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/prepare_kqp_data_tx_in_rs_unit.cpp#L31) unit and begin waiting for all necessary ReadSets from other participants in the [LoadAndWaitInRS](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/load_and_wait_in_rs_unit.cpp#L36) unit. In some cases (e.g. blind writes to multiple shards without lock validation) distributed transactions may not need to exchange ReadSets and ReadSet-related units above don't perform any actions.

Finally, KQP transaction operation reaches the [ExecuteKqpDataTx](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L59) unit, which validates local optimistic locks (using previously persisted `AccessLog` when present), validates ReadSets received from other participants, and given everything checks out executes transaction body and returns the result. When lock validation (either local or remote) fails the transaction body is not executed and operation fails with the `ABORTED` status.

### Volatile transactions

Volatile distributed transactions are only stored in memory, and are currently lost when the shard is restarted. They must nevertheless guarantee the distributed transaction will either commit at all participants, or will be aborted at all participants in a timely manner. The big upside is no need for separate read, wait and execution phases, they are atomically executed in the single execution phase with 1RTT storage latency on the hot path from distributed commit start to the successful reply. Because they are atomically executed they usually don't stall the pipeline and may increase transaction throughput. Since any shard can abort a volatile distributed transaction without waiting for planning deadline it also limits shard unavailability during partitioning and schema changes.

Volatile transactions are based on [persistent uncommitted changes](localdb-uncommitted-txs.md) in the local database. During execution phase DataShard optimistically assumes all remote locks will validate, applies effects in the form of uncommitted changes (using the globally unique `TxId` of the distributed transaction) and adds the transaction record to the [VolatileTxManager](https://github.com/ydb-platform/ydb/blob/1f1b84d1d160a2b1cfe4298b271c0078ec1602b1/ydb/core/tx/datashard/volatile_tx.cpp#L439) in the waiting state for decisions by other participants. Successful reply is only sent when all effects are persistent, and a commit decision has been received from all other participants. Reads use the `TxMap` to observe all pending changes and check their status using the `TxObserver`. When the read result is dependent on whether or not the preceding volatile transaction commits, operations subscribe to the transaction status and restart after the transaction is decided (either reading now committed changes, or skipping them after an abort).

Having uncommitted and not yet aborted changes limits shard's ability to perform blind writes. Since uncommitted changes must commit in the same order they have been applied to any given key, and shards don't keep these keys in memory after transactions have been executed, DataShard needs to read each key and find conflicts before it can do any write. These conflicting changes may either be uncommitted changes attached to an optimistic lock (and these locks need to be broken with changes rolled back), or uncommitted changes belonging to waiting volatile transactions. We don't want to block writes unnecessarily, so even non-volatile operations may switch to a volatile commit. Such operations allocate `GlobalTxId` when necessary (this is a per-cluster unique `TxId` needed when not provided by the request, e.g. in `BulkUpsert`) and write changes to conflicting keys as uncommitted. The transaction record is then added to `VolatileTxManager` without specifying other participants, becomes initially committed and specifies a list of dependencies. These transactions don't block reads and are eventually committed in the local database after all dependencies have also been committed or aborted.

To reduce stalls in the pipeline transactions use the conflict cache for keys that are declared for writes in distributed transactions. These keys are read while transactions wait in the queue, and conflicting transaction sets are cached by the [RegisterDistributedWrites](https://github.com/ydb-platform/ydb/blob/3fa95b9777601584da35d5925d7908f283f671a9/ydb/core/tx/datashard/build_and_wait_dependencies_unit.cpp#L95) function call. All writes to cached keys update these conflict sets and keep them up to date. This allows distributed transactions with writes to execute faster by processing lists of conflicting transactions using a hash-table lookup even when table data is evicted from cache.

DataShard may have change collectors (e.g. async indexes and/or CDC). Collecting these changes for volatile transactions is similar to [uncommitted changes in transactions](datashard-locks-and-change-visibility.md). It generates a stream of uncommitted change records using `TxId` as its `LockId`, which are then either atomically added to shard change records on commit, or deleted on abort. Depending on change collector settings it may also need to read the current row state. When this row state depends on other waiting volatile transactions this is handled similar to any other read by adding a dependency and restarting when those dependencies are resolved. This may stall the transaction pipeline, but it's not conceptually different from persistent distributed transactions with ReadSets stalled by their read-write conflicts.

Volatile transactions also [generate](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L193) and [write to disk](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L367) OutReadSets for all other participants specified in the `ReceivingShards` sets in the same local database transaction that writes uncommitted effects. When uncommitted changes are persistent these ReadSets are [sent](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L496) to other participants, which notifies them this shard is ready to commit the distributed transaction and won't change its decision unless the transaction is aborted by another participant.

Shards forget about volatile transactions (that did not persist their transaction record) when they are restarted and will not send outgoing ReadSets for transactions they no longer know nothing about. Shards that successfully execute the transaction body send a [special event](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/execute_kqp_data_tx_unit.cpp#L363) called ReadSet Expectation that informs about them are waiting for an incoming ReadSet. This is done even before effects are persisted, so shards have a chance to find out about aborted transactions as early as possible. Restarted shards will see a request for transaction they know nothing about and reply with a [special ReadSet without data](https://github.com/ydb-platform/ydb/blob/6e0ff4ff86f8dd5ac589b83de59c8bb377fe38c5/ydb/core/tx/datashard/datashard__readset.cpp#L48). This way all participants find out about aborted transactions and abort them as well.

### Volatile transaction guarantees

To reiterate volatile transactions are stored in memory of both coordinators and participants, and may fail to various reasons. It's important to show why volatile transactions either commits or aborts at all participants, and it's impossible for transaction to only commit at a subset of participants.

Some examples of abort sources for the transaction:

1. Any shard may unexpectedly restart, including the case where a new instance has started while the old instance is still running and unaware of the new instance. Since transaction is only stored in memory, new instance may not be aware of a transaction that failed to store its effects to disk. Transaction may have also successfully executed and committed, leaving no traces except its committed effects.
2. Coordinator may unexpectedly restart without an in-memory state transfer, and the transaction may have been sent to some participants but not all of them.
3. Any shard may have decided to abort the transaction with an error.

An important guarantee is when any shard has successfully persisted uncommitted effects and the transaction record, and also received `DECISION_COMMIT` from all other participants, then this transaction will eventually commit at all participants and cannot be aborted. It's also important that when any shard aborts the transaction with an error (including forgetting about the transaction due to a restart) it will never be able to commit and will eventually abort.

These guarantees are derived from the following:

1. ReadSets with `DECISION_COMMIT` are sent to other participants only after all uncommitted changes, outgoing ReadSets and transaction record have been persisted to disk. So `DECISION_COMMIT` messages are persistent and will be delivered to other participants until acknowledged. These messages will be delivered even when transaction is aborted by another shard.
2. ReadSets are only acknowledged either when shard successfully writes something to disk, or when readonly lease is active when an unexpected ReadSet is received. In particular, an old tablet instance cannot acknowledge a ReadSet for an unknown transaction that has been prepared and executed by a newer generation. Local database guarantees a new tablet generation will not be activated until readonly lease at older generations has expired (as long as monotonic clock frequency does not differ more than twice). A successful write also confirms that the tablet had the storage lock at the start of the commit and no newer generation could be running at that time.
    * Volatile transactions use an optimization where received ReadSets are never written to disk. Acknowledgement is only sent after a the commit or abort (removal) of the transaction record is fully persistent on disk.
    * When all participants have generated and persisted their outgoing `DECISION_COMMIT` ReadSets they will keep receiving the full set of commit decisions until they fully commit transaction effects and send their acknowledgements.
3. Any message related to an aborted transaction is only sent after its effects are written to disk. This is needed to handle races between multiple generations of a particular generation running at the same time. After a newer generation successfully commits a volatile transaction and acknowledges ReadSets an older generation (which will fail when trying to access the disk or validate a readonly lease) may receive a `NO_DATA` reply to their ReadSet Expectation (which was acknowledged by a newer generation). However an older generation will not be able to commit the removal of the transaction record and will not be able to erroneously reply with an error. When the removal of the transaction record (and effects) is fully committed, current generation can be sure newer generations did not and will not start with this transaction in the waiting state and it will not be committed.
4. A successful reply after gathering all `DECISION_COMMIT` ReadSets does not have to wait for the final commit in the local database, but has to wait until uncommitted effects and transaction record has been persisted. This allows successful replies to have a 1RTT storage latency on the hot path, which is the write of uncommitted changes and the transaction record. This successful reply is stable:
    * In the case shard is restarted, effects and transaction record are persistent, so in the worst case it will be restored in the `Waiting` state.
    * From the MVCC viewpoint the transaction has already been completed, any new read will have an MVCC version that includes these changes. Any new read will start waiting for the transaction to resolve, which is now guaranteed to succeed.
    * The restored `Waiting` transaction could not have its incoming ReadSets acknowledged, so they will be resent and since we received `DECISION_COMMIT` earlier it will be received again, and transaction will quickly move to the `Committed` state.
5. As long as the transaction is enqueued, and especially as long as its `PlanStep` is not yet known, incoming ReadSets are attached in memory, and will not be acknowledged. Acknowledgements are delayed until the transaction is fully committed, or until it is aborted early (in which case no effects and no transaction record have been committed, and newer generations cannot recover and commit the transaction).

Volatile distributed transactions also guarantee change visibility to be stable across multiple shards participating in the same distributed transaction. For example, persistent distributed transactions allow the following anomaly:

1. Distributed transaction `Tx1` performs a blind write to keys `x` and `y` in two different shards.
2. Single-shard transaction `Tx2` reads key `x` and observes a value written by `Tx1`.
3. Single-shard transaction `Tx3` (which starts after `Tx2` has completed) reads key `y` and does not observe a value written by `Tx1`.

This anomaly may be observed when the shard with key `x` quickly receives its plan step with the transaction `Tx1` and performs the write. A read in `Tx2` which arrives a bit later will have its local MVCC version assigned that includes changes to key `x` and observe the change. The shard with key `y` may run on a node that's a bit slower and may not even be aware of the plan step with the transaction `Tx1` when a read in `Tx3` arrives. Since its time cast bucket doesn't have `Tx1` acknowledged its mediator time will stay in the past, and `Tx3` will have its local MVCC version assigned that doesn't include changes to key `y`. From the user's perspective `Tx2` happened before `Tx3`, but in the global serializable order turned out to be `Tx3`, `Tx1`, `Tx2`.

Volatile transactions cannot observe this particular anomaly since a read that observed changes to key `x` must have received `DECISION_COMMIT` from the other shard with key `y`. This means the transaction record was persisted at both shards, and crucially the shard with key `y` had it marked as completed. A read from key `y` will have to choose a local MVCC version that includes changes by `Tx1` and will have to wait until `Tx1` is resolved.

In other words, once a client observed the result of a volatile transaction, all subsequent reads will also observe the result of that volatile transaction and changes stability is not violated.

### Indirect planning of volatile transactions

Coordinator restart may cause plan steps to only arrive at a subset of participants of a volatile distributed transaction. In that case some shards may execute the transaction and start waiting for ReadSets while other shards will keep waiting for the plan step to arrive. Since planning timeout is 30 seconds this could have causes some transaction to have excessive delays until they are eventually aborted.

When any participant receives the first plan step of a volatile transaction it will include `PlanStep` within ReadSets it sends to other participants. Other participants then indirectly learn the `PlanStep` that was assigned to the transaction, and remember it as `PredictedPlanStep`. Even when their own plan step is lost DataShard will add the transaction to the PlanQueue when its mediator time (directly or indirectly) reaches `PredictedPlanStep` as if it received a plan step with this predicted `PlanStep` and `TxId`. In the case where `PredictedPlanStep` is in the past the transaction is quickly aborted as if it reached the planning deadline.

Since the transaction may be planned to different plan steps, some of which may be lost, different participants may have a different `PlanStep` assigned to the same transaction and the same transaction will try to execute at different timestamps. DataShards validate that `DECISION_COMMIT` messages have their `PlanStep` matching their assigned `PlanStep`, and only consider them when steps match. Mismatching plan steps cause the transaction to abort.
