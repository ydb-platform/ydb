# Трансформация и подготовка данных (ETL/ELT)

Подготовка данных для анализа — ключевой этап в построении хранилища данных. {{ydb-short-name}} поддерживает все стандартные подходы к трансформации данных, позволяя выбрать инструмент, наиболее подходящий для конкретной задачи: от чистого SQL до сложных пайплайнов на Apache Spark.

## ELT

Трансформации данных с помощью SQL зачастую являются наиболее производительными, поскольку вся обработка происходит непосредственно в движке {{ydb-short-name}}, без перемещения данных во внешние системы и обратно. Логика описывается на SQL и выполняется распределённым MPP-движком, оптимизированным для аналитических операций.

### Производительность в бенчмарке TPC-H

Производительность операций ELT напрямую зависит от скорости выполнения сложных аналитических запросов. Стандартным отраслевым тестом для оценки таких запросов является [TPC-H](https://www.tpc.org/tpch/).

Сравнение с другой распределённой аналитической СУБД на наборе запросов TPC-H показывает, что {{ydb-short-name}} демонстрирует более стабильную производительность, особенно при выполнении запросов, содержащих:

- Соединения (`JOIN`) большого числа таблиц, пяти и более;
- Вложенные подзапросы, используемые для фильтрации;
- Агрегации (`GROUP BY`) с последующей сложной фильтрацией по результатам.

![](_includes/ydb_vs_another.png){width=600}

Такая стабильность свидетельствует о высокой эффективности стоимостного оптимизатора запросов {{ ydb-short-name }} при построении планов выполнения для комплексных SQL-шаблонов, типичных для реальных ELT-процессов. Для платформы хранилища данных (DWH) это означает предсказуемое время обновления данных и снижение рисков неконтролируемой деградации производительности в продакшн-среде.

### Основные сценарии

- Построение витрин данных: используйте привычный синтаксис [`INSERT INTO ... SELECT FROM ...`](../../../yql/reference/syntax/insert_into.md) для создания агрегированных таблиц (витрин) из «сырых» данных;
- Объединение OLTP и OLAP данных: {{ydb-short-name}} позволяет в одном запросе объединять данные как из транзакционных (строковых), так и из аналитических (колоночных) таблиц. Это даёт возможность обогащать «холодные» аналитические данные актуальной информацией из OLTP-системы без необходимости их дублирования;
- Массовое обновление: для «слепой» записи больших объёмов данных без проверки существования можно использовать оператор [`UPSERT INTO`](../../../yql/reference/syntax/upsert_into.md).

### Управление SQL-пайплайнами с помощью dbt {#dbt}

Для управления сложными SQL-пайплайнами используйте [плагин к dbt](../../../integrations/migration/dbt.md). Плагин позволяет дата-инженерам описывать модели данных в виде `SELECT`-запросов, а dbt автоматически строит граф зависимостей между моделями и выполняет их в правильном порядке. Этот подход помогает реализовать принципы software engineering (тестирование, документирование, версионирование) при работе с SQL-кодом.

## ETL

### Сложные трансформации с помощью внешних фреймворков {#external-etl}

Для задач, требующих сложной логики на языках программирования (Python, Scala, Java), интеграции с ML-пайплайнами или обработки больших объемов данных, удобно использовать внешние фреймворки для распределенной обработки.

Apache Spark — один из самых популярных инструментов для таких задач, для которого разработан [специализированный коннектор](../../../integrations/ingestion/spark.md) к {{ydb-short-name}}. Если в вашей компании используются другие схожие решения (например, Apache Flink), их также можно задействовать для построения ETL-процессов за поддержки [JDBC-драйвера](../../../reference/languages-and-apis/jdbc-driver/index.md).

Ключевое преимущество {{ydb-short-name}} при работе с такими системами — архитектура, позволяющая считывать данные параллельно. В {{ydb-short-name}} нет выделенного мастер-узла для экспорта, поэтому внешние инструменты могут читать информацию напрямую со всех узлов хранения. Это обеспечивает высокую скорость и линейную масштабируемость чтения.

## Оркестрация пайплайнов

Для запуска пайплайнов по расписанию и управления зависимостями используются оркестраторы.

- Apache Airflow: для оркестрации данных в {{ydb-short-name}} поддерживается [провайдер к Apache Airflow](../../../integrations/orchestration/airflow.md). С его помощью можно создавать DAG'и, которые запускают `dbt run`, выполняют YQL-скрипты или инициируют Spark-джобы.
- Встроенные механизмы: для некоторых задач не требуется внешний оркестратор. {{ydb-short-name}} может выполнять часть операций автоматически:

    - Удаление устаревших данных по TTL: автоматическая очистка партиций по истечении заданного времени.
    - Автоматический сompaction: процессы слияния и оптимизации данных для LSM-дерева происходят в фоновом режиме, что исключает необходимость регулярного запуска команд вроде `VACUUM`.

- Другие оркестраторы: если в вашей компании принят другой инструмент (например, Dagster, Prefect) или собственный планировщик, вы можете использовать его для запуска тех же команд. Большинство оркестраторов позволяют выполнять shell-скрипты, что дает возможность вызывать YDB CLI, [dbt](#dbt) run и другие утилиты.

## Интеграция с другими ETL-инструментами через JDBC

{{ydb-short-name}} предоставляет [JDBC-драйвер](../../../reference/languages-and-apis/jdbc-driver/index.md), что позволяет использовать широкий спектр существующих ETL-инструментов, таких как [Apache NiFi](https://nifi.apache.org/) и другие JDBC-совместимые системы.
