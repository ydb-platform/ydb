# Глоссарий {{ ydb-short-name }}

Эта статья представляет собой обзор терминов и определений, используемых в {{ ydb-short-name }} и её документации. Он [начинается с ключевых терминов](#key-terminology), с которыми полезно ознакомиться на раннем этапе работы с {{ ydb-short-name }}, а остальная часть статьи содержит [более продвинутые термины](#advanced-terminology), которые могут быть полезны в дальнейшем.

## Ключевая терминология {#key-terminology}

Этот раздел описывает термины, которые полезны любому человеку, работающему с {{ ydb-short-name }}, независимо от его роли и сценария использования.

### Кластер {#cluster}

**Кластер** или **cluster** {{ ydb-short-name }} представляет собой набор взаимосвязанных [узлов](#node) {{ ydb-short-name }}, которые обмениваются данными для выполнения пользовательских запросов и надёжного хранения данных. Эти узлы формируют одну из поддерживаемых [топологий кластера](#topology), что напрямую влияет на его характеристики надёжности и производительности.

Кластеры {{ ydb-short-name }} являются мультитенантными (многопользовательскими) и могут содержать несколько изолированных [баз данных](#database).

### База данных {#database}

Как и в большинстве систем управления базами данных, **база данных** в {{ ydb-short-name }} представляет собой логический контейнер для других сущностей, таких как [таблицы](#table). Однако в {{ ydb-short-name }} пространство имён внутри баз данных иерархическое, как в [виртуальных файловых системах](https://en.wikipedia.org/wiki/Virtual_file_system), и таким образом [директории](#folder) позволяют более структурированно организовывать сущности.

Ещё одной важной характеристикой баз данных {{ ydb-short-name }} является то, что им обычно выделяются отдельные вычислительные ресурсы. Как следствие, создание дополнительной базы данных обычно выполняется снаружи системы [инженерами DevOps](../devops/index.md) или автоматизацией, а не через SQL-запрос.

### Узел {#node}

{{ ydb-short-name }} **узел** или **node**  — это серверный процесс, выполняющий исполняемый файл под названием `ydbd`. На одном физическом сервере или виртуальной машине могут работать несколько узлов {{ ydb-short-name }}, что является обычной практикой. Таким образом, в контексте {{ ydb-short-name }} узлы **не** являются синонимами хостов.

Так как {{ ydb-short-name }} использует подход раздельных слоёв хранения и вычислений (storage and compute separation), `ydbd` имеет несколько режимов работы, определяющих тип узла. Доступные типы узлов описаны ниже.

#### Узел базы данных {#database-node}

**Узлы базы данных** (также известные как **тенантные узлы**, **database node** или **tenant node**) обрабатывают пользовательские запросы, адресованные конкретной логической [базе данных](#database). Их состояние находится только в оперативной памяти и может быть восстановлено из [распределённого хранилища](#distributed-storage). Совокупность узлов баз данных заданного [кластера {{ ydb-short-name }}](cluster/index.md) можно считать вычислительным слоем этого кластера. Таким образом, добавление узлов баз данных и выделение им дополнительных ресурсов (CPU и RAM) — это основные способы увеличения вычислительных ресурсов базы данных.

Основная роль узлов базы данных — запуск различных [таблеток](#tablet) и [акторов](#actor), а также приём входящих запросов по сети.

#### Узел хранения {#storage-node}

**Узлы хранения** или **storage node** являются узлами с состоянием, отвечающими за долгосрочное хранение фрагментов данных. Совокупность узлов хранения заданного [кластера {{ ydb-short-name }}](#cluster) называются [распределённым хранилищем](#distributed-storage) и могут рассматриваться как слой хранения этого кластера. Таким образом, добавление дополнительных узлов хранения и их дисков — это основные способы увеличения ёмкости хранилища и пропускной способности ввода/вывода кластера.

#### Гибридный узел {#hybrid-mode}

**Гибридный узел** — это процесс, который одновременно выполняет обе роли [узла базы данных](#database-node) и [узла хранения](#storage-node). Гибридные узлы часто используются в целях разработки. Например, вы можете запустить контейнер с полнофункциональным {{ ydb-short-name }}, содержащим только один процесс `ydbd` в гибридном режиме. В производственных средах они используются редко.

#### Статический узел {#static-node}

**Статические узлы** или **static nodes** конфигурируются вручную во время первоначальной инициализации или перенастройки кластера. Как правило, они выполняют роль [узлов хранения](#storage-node), но технически возможно настроить их и в качестве [узлов баз данных](#database-node).

#### Динамический узел {#dynamic}

**Динамические узлы** или **dynamic nodes** добавляются и удаляются из кластера на ходу. Они могут выполнять только роль [узлов базы данных](#database-node).

### Распределённое хранилище {#distributed-storage}

**Распределённое хранилище**, **Distributed storage**, **Blob storage** или **BlobStorage** — это распределённый отказоустойчивый слой хранения данных в {{ ydb-short-name }}. Оно имеет специализированный API, предназначенный для хранения неизменяемых фрагментов данных [таблетки](#tablet).

Множество терминов, связанных с [реализацией распределённого хранилища](#distributed-storage-implementation), рассматриваются ниже.

### Группа хранения {#storage-group}

**Группа хранения**, **группа распределённого хранилища**, **storage group** или **группа Blob storage** — это место для надёжного хранения данных, аналогичное [RAID](https://en.wikipedia.org/wiki/RAID), но использующее диски нескольких серверов. В зависимости от выбранной [топологии кластера](#topology) группы хранения используют различные алгоритмы для обеспечения высокой доступности, аналогично [стандартным уровням RAID](https://en.wikipedia.org/wiki/Standard_RAID_levels).

[Распределённое хранилище](#distributed-storage) обычно управляет большим количеством относительно небольших групп хранения. Каждую группу можно назначить конкретной [базе данных](#database) для увеличения ёмкости дискового пространства и пропускной способности ввода/вывода, доступных для этой базы данных.

#### Статическая группа {#static-group}

**Статическая группа** или **static group** — это специальная [группа хранения](#storage-group), созданная во время изначального развёртывания кластера. Её основная роль заключается в хранении данных системных [таблеток](#tablet), которые можно рассматривать как метаданные уровня всего кластера.

Статическая группа может требовать особого внимания во время крупного обслуживания кластера, такого как вывод [зоны доступности](#regions-az) из эксплуатации.

#### Динамическая группа {#dynamic-group}

Обычные группы хранения, которые не являются [статическими](#static-group), называются **динамическими группами** или **dynamic group**. Их называют динамическими, потому что они могут быть созданы и удалены на лету во время работы [кластера](#cluster).

### Актор {#actor}

[Модель акторов](https://ru.wikipedia.org/wiki/%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%B0%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2) — это один из основных подходов к параллелизму выполнения, используемый в {{ ydb-short-name }}. В этой модели **акторы** или **actor** — это легковесные процессы в пользовательском пространстве, которые могут иметь и изменять своё частное состояние, но могут влиять друг на друга только косвенно через обмен сообщениями. {{ ydb-short-name }} имеет собственную реализацию этой модели, которая описана [ниже](#actor-implementation).

В {{ ydb-short-name }} акторы с надёжно сохраняемым состоянием называются [таблетками](#tablet).

### Таблетка {#tablet}

**Таблетка** — это один из основных строительных блоков и абстракций {{ ydb-short-name }}. Она представляет собой сущность, ответственную за относительно небольшой сегмент пользовательских или системных данных. Обычно таблетка управляет объёмом данных до нескольких гигабайт, однако некоторые типы таблеток могут обрабатывать и больший объём.

Например, [построчная пользовательская таблица](#row-oriented-table) управляется одной или несколькими таблетками типа [DataShard](#datashard), причём каждая таблетка отвечает за непрерывный диапазон [первичных ключей](#primary-key) и соответствующих им данных.

Конечным пользователям, отправляющим запросы в кластер {{ ydb-short-name }} для выполнения, не нужно знать подробностей о таблетках, их видах или принципах работы, но это может быть полезно, например, для оптимизации производительности.

Технически таблетки являются [акторами](#actor) с состоянием, надёжно сохранённым в [распределённом хранилище](#distributed-storage). Это состояние позволяет таблетке продолжать работу на другом [узле базы данных](#database-node), если предыдущий вышел из строя или перегружен.

[Подробности реализации таблеток](#tablet-implementation) и связанные термины, а также [основные типы таблеток](#tablet-types), рассматриваются ниже.

### Распределённые транзакции {#distributed-transaction}

{{ ydb-short-name }} реализует **транзакции** (**transactions**) на двух основных уровнях:

* [Локальная база данных](#local-database) и остальная [инфраструктура таблеток](#tablet-implementation) позволяют [таблеткам](#tablet) манипулировать своим состоянием, используя **локальные транзакции** с [уровнем изоляции serializable](https://ru.wikipedia.org/wiki/%D0%A3%D1%80%D0%BE%D0%B2%D0%B5%D0%BD%D1%8C_%D0%B8%D0%B7%D0%BE%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D1%82%D1%80%D0%B0%D0%BD%D0%B7%D0%B0%D0%BA%D1%86%D0%B8%D0%B9#Serializable_(%D1%83%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D1%87%D0%B8%D0%B2%D0%B0%D0%B5%D0%BC%D0%BE%D1%81%D1%82%D1%8C)). Технически они не являются локальными для одного узла, так как это состояние сохраняется удалённо в [распределённом хранилище](#distributed-storage).
* В контексте {{ ydb-short-name }} термин **распределённые транзакции** обычно относится к транзакциям, охватывающим несколько таблеток. Например, транзакции между таблицами или даже строками одной таблицы часто являются распределёнными.

Эти механизмы позволяют {{ ydb-short-name }} обеспечивать [строгую согласованность](https://en.wikipedia.org/wiki/Consistency_model#Strict_consistency).

{% if oss == "true" %}

Реализация распределённых транзакций рассмотрена в отдельной статье [{#T}](../contributor/datashard-distributed-txs.md), а ниже приведён список нескольких [связанных терминов](#distributed-transactions-implementation).

{% endif %}

### Многоверсионное управление параллелизмом {#mvcc}

[**Многоверсионное управление параллелизмом**](https://ru.wikipedia.org/wiki/MVCC), **multi-version concurrency control** или **MVCC** — это метод, используемый {{ ydb-short-name }} для одновременного доступа нескольких параллельных транзакций к базе данных без взаимных помех. Он описан более подробно в отдельной статье [{#T}](mvcc.md).

### Топология {#topology}

{{ ydb-short-name }} поддерживает несколько **топологий** [кластера](#cluster) (или **topology**), описанных более подробно в отдельной статье [{#T}](topology.md). Ниже объяснены несколько связанных терминов.

#### Зоны доступности и регионы {#regions-az}

**Зона доступности** — это дата-центр или его изолированный сегмент с минимальным физическим расстоянием между узлами и минимальным риском отказа одновременно с другими зонами доступности. Таким образом, зоны доступности не должны использовать общую инфраструктуру, такую как электропитание, охлаждение или внешние сетевые соединения.

**Регион** — это большая географическая область, содержащая несколько зон доступности. Расстояние между зонами доступности в одном регионе должно составлять около 500 км или менее. {{ ydb-short-name }} выполняет записи данных в каждую зону доступности в регионе синхронно, обеспечивая разумную задержку и бесперебойную работу в случае отказа одной из зон доступности.

#### Стойка {#rack}

**Стойка**, **серверная стойка**, **rack** или **server rack** — это оборудование, используемое для организации размещения нескольких серверов. Серверы в одной стойке с большей вероятностью могут стать недоступными одновременно из-за проблем на уровне стойки, связанных с электропитанием, охлаждением и т. д. {{ ydb-short-name }} может учитывать информацию о том, какой сервер находится в какой стойке, при размещении каждого фрагмента данных в средах, основанных на физических серверах.

### Таблица {#table}

**Таблица** или **table** — это структурированный фрагмент информации, организованный в строки и столбцы. Каждая строка представляет собой одну запись или элемент, а каждый столбец — это конкретный атрибут или поле с определённым типом данных.

Существуют два основных подхода к представлению табличных данных в оперативной памяти или на дисках: [построчный (строка за строкой)](#row-oriented-table) и [постолбцовый (столбец за столбцом)](#column-oriented-table). Выбранный подход сильно влияет на характеристики производительности операций с этими данными: первый больше подходит для транзакционных нагрузок (OLTP), а второй — для аналитических (OLAP). {{ ydb-short-name }} поддерживает оба подхода.

#### Построчная таблица {#row-oriented-table}

**Построчные таблицы** или **row-oriented tables** хранят данные для всех или большинства столбцов каждой строки физически рядом друг с другом. Они описаны более подробно в [{#T}](datamodel/table.md#row-oriented-tables).

#### Постолбцовая таблица {#column-oriented-table}

**Постолбцовые таблицы**, **колоночные таблицы**, **column-oriented table** или **columnar table** хранят данные для каждого столбца отдельно. Они оптимизированы для построения агрегатов по небольшому числу столбцов, но менее подходят для доступа к конкретным строкам, так как строки нужно восстанавливать из их ячеек на лету. Они описаны более подробно в [{#T}](datamodel/table.md#column-oriented-tables).

#### Первичный ключ {#primary-key}

**Первичный ключ** или **primary key** — это упорядоченный список столбцов, значения которых однозначно идентифицируют строку. Он используется для создания [первичного индекса](#primary-index) таблицы. Он задаётся пользователем {{ ydb-short-name }} при [создании таблицы](../yql/reference/syntax/create_table.md) и существенно влияет на производительность операций с этой таблицей.

Руководство по выбору первичных ключей представлено в [{#T}](../dev/primary-key/index.md).

#### Первичный индекс {#primary-index}

**Первичный индекс**, **индекс первичного ключа**, **primary index** или **primary key index** — это основная структура данных, используемая для нахождения строк в таблице. Он создаётся на основе выбранного [первичного ключа](#primary-key) и определяет физический порядок строк в таблице; таким образом, у каждой таблицы может быть только один первичный индекс. Первичный индекс является уникальным.

#### Вторичный индекс {#secondary-index}

**Вторичный индекс** или **secondary index** — это дополнительная структура данных, используемая для нахождения строк в таблице, обычно когда это нельзя эффективно сделать с помощью [первичного индекса](#primary-index). В отличие от первичного индекса, вторичные индексы управляются независимо от основных данных таблицы. Таким образом, у таблицы может быть несколько вторичных индексов для различных сценариев. Возможности {{ ydb-short-name }} в отношении вторичных индексов описаны в отдельной статье [{#T}](secondary_indexes.md). Вторичный индекс может быть как уникальным, так и неуникальным.

#### Семейство столбцов {#column-family}

**Семейство столбцов**, **группа столбцов**, **column family** или **column group** — это функция, позволяющая хранить подмножества столбцов [построчной таблицы](#row-oriented-table) отдельно в отдельном семействе или группе. Основной сценарий использования — хранение части столбцов на других типах дисков (перенос менее важных столбцов на HDD) или с другими настройками компрессии. Если рабочая нагрузка требует многих семейств столбцов, рассмотрите возможность использования [колоночных таблиц](#column-oriented-table).

#### Время жизни {#ttl}

**Время жизни**, **time to live** или **TTL** — это механизм для автоматического удаления старых строк из таблицы асинхронно в фоновом режиме. Он описан в отдельной статье [{#T}](ttl.md).

### Топик {#topic}


**Очередь сообщений** используется для надёжной асинхронной связи между различными системами посредством передачи сообщений. {{ ydb-short-name }} предоставляет инфраструктуру, обеспечивающую семантику "exactly once" (ровно один раз) в таких коммуникациях. С её использованием можно добиться гарантии отсутствия потерянных сообщений и случайных дубликатов.

**Топик**, **тема**, **topic** — это именованная сущность в очереди сообщений, предназначенная для взаимодействия [писателей](#producer) и [читателей](#consumer).

Несколько терминов, связанных с топиками, приведены ниже. Как работают топики {{ ydb-short-name }} объясняется более подробно в отдельной статье [{#T}](topic.md).

#### Партиция {#partition}

Для целей горизонтального масштабирования топики разделяются на отдельные элементы, называемые **партициями** или **partitions**. Таким образом, партиции являются единицей параллелизма внутри топики. Сообщения внутри каждой партиции упорядочены.

Однако подмножества данных, управляемые одним [data shard](#data-shard) или [column shard](#column-shard), также могут называться партициями.

#### Смещение {#offset}

**Смещение** или **offset** — это порядковый номер, идентифицирующий сообщение внутри [партиции](#partition).

#### Писатель {#producer}

**Писатель**, **производитель** или **producer** — это сущность, записывающая новые сообщения в топик.

#### Читатель {#consumer}

**Читатель**, **потребитель** или **consumer** — это сущность, читающая сообщения из топика.

### Захват изменений данных {#cdc}

**Захват изменений данных**, **change data capture** или **CDC** — это механизм, позволяющий подписываться на поток изменений в конкретной [таблице](#table). Технически он реализован поверх [топиков](#topic). Он описан более подробно в отдельной статье [{#T}](cdc.md).

### YQL {#yql}

**YQL ({{ ydb-short-name }} Query Language)** — это высокоуровневый язык для работы с системой. Он является диалектом [ANSI SQL](https://en.wikipedia.org/wiki/SQL). Существует много материалов, посвящённых YQL, включая [туториал](../dev/yql-tutorial/index.md), [справочник](../yql/reference/syntax/index.md) и [рецепты](../recipes/yql/index.md).

### Федеративные запросы {#federated-queries}

**Федеративные запросы** или **federated queries** — это функциональность, позволяющая выполнять запросы к данным, хранящимся в системах, внешних по отношению к кластеру {{ ydb-short-name }}.

Ниже объяснены несколько терминов, связанных с федеративными запросами. Как работают федеративные запросы в {{ ydb-short-name }} объясняется более подробно в отдельной статье [{#T}](federated_query/index.md).

#### Внешний источник данных {#external-data-source}

**Внешний источник данных**, **внешнее подключение**, **external data source** или **external connection** — это метаданные, описывающие, как подключиться к поддерживаемой внешней системе для выполнения [федеративных запросов](#federated-queries).

#### Внешняя таблица {#external-table}

**Внешняя таблица** или **external table** — это метаданные, описывающие конкретный набор данных, который можно извлечь из [внешнего источника данных](#external-data-source).

#### Секрет {#secret}

**Секрет** или **secret** — это конфиденциальные метаданные, требующие особого обращения. Например, секреты могут использоваться в определениях [внешних источников данных](#external-data-source) и представляют собой такие сущности, как пароли и токены.

### Папка {#folder}

Как и в файловых системах, **папка**, **каталог**, **folder** или **directory** является контейнером для других сущностей. В случае {{ ydb-short-name }}, эти сущности могут быть [таблицами](#table) (включая [внешние таблицы](#external-table)), [топиками](#topic), другими папками и т.д.

## Продвинутая терминология {#advanced-terminology}

Этот раздел объясняет термины, которые полезны для [{{ ydb-short-name }} контрибьюторов](../contributor/index.md) и пользователей, желающих глубже понять, что происходит внутри системы.

### Реализация акторов {#actor-implementation}

#### Система акторов {#actor-system}

**Система акторов** или **actor system** — это C++ библиотека с [реализацией](https://github.com/ydb-platform/ydb/tree/main/ydb/library/actors) [акторной модели](https://en.wikipedia.org/wiki/Actor_model) для нужд {{ ydb-short-name }}.

#### Акторный сервис {#actor-service}

**Акторный сервис** или **actor service** — это [актор](#actor), который имеет известное имя и обычно выполняется в единственном экземпляре на [узле](#node).

#### ActorId {#actorid}

**ActorId** — это уникальный идентификатор актора или [таблетки](#tablet) в [кластере](#cluster).

#### Интерконнект акторной системы {#actor-system-interconnect}

**Интерконнект акторной системы**, **интерконнект**, **actor system interconnect** или **interconnect** — это внутренний сетевой слой [кластера](#cluster). Все [акторы](#actor) взаимодействуют друг с другом в системе через интерконнект.

#### Локал {#local}

**Локал** или **local** — это [акторный сервис](#actor-service), работающий на каждом [узле](#node). Он напрямую управляет [таблетками](#tablet) на своём узле и взаимодействует с [Hive](#hive). Он регистрируется в Hive и получает команды на запуск таблеток.

### Реализация таблеток {#tablet-implementation}

[**Таблетка**](#tablet) или **tablet** — это [актор](#actor) с постоянным состоянием. Она включает набор данных, за которые эта таблетка отвечает, и конечный автомат, через который изменяются данные (или состояние) таблетки. Таблетка является отказоустойчивой сущностью, поскольку данные таблетки хранятся в [распределённом хранилище](#distributed-storage), которое переживает сбои дисков и узлов. Таблетка автоматически перезапускается на другом [узле](#node) в случае отказа или перегрузки предыдущего. Данные в таблетке изменяются последовательно, так как инфраструктура системы гарантирует, что нет более одного [лидера таблетки](#tablet-leader), через которого выполняются изменения данных таблетки.

Таблетка решает ту же задачу, что и алгоритмы [Paxos](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%9F%D0%B0%D0%BA%D1%81%D0%BE%D1%81) и [Raft](https://ru.wikipedia.org/wiki/Raft_(%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC)) в других системах, а именно задачу [распределённого консенсуса](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BD%D1%81%D0%B5%D0%BD%D1%81%D1%83%D1%81_%D0%B2_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%91%D0%BD%D0%BD%D1%8B%D1%85_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F%D1%85). С технической точки зрения реализация таблетки может быть описана как реплицированная машина состояний (Replicated State Machine, RSM) поверх общего журнала, поскольку состояние таблетки полностью описывается упорядоченным журналом команд, хранящимся в распределённом и отказоустойчивом хранилище.

Во время выполнения машина состояний таблетки управляется тремя компонентами:

1. Общая таблеточная часть обеспечивает согласованность журнала и восстановление в случае сбоев.
2. **Исполнитель** или **executor** — это абстракция локальной базы данных, а именно структуры данных и код, которые организуют работу с данными, хранимыми таблеткой.
3. Актор с пользовательским кодом, реализующим специфическую логику конкретного типа таблетки.

В {{ ydb-short-name }} существует несколько видов специализированных таблеток, хранящих всевозможные данные для различных задач. Многие функции {{ ydb-short-name }}, такие как [таблицы](#table) и [топики](#topic), реализованы как разные виды таблеток. Таким образом, повторное использование инфраструктуры таблеток является одним из ключевых средств расширяемости {{ ydb-short-name }} как платформы.

Обычно в кластере {{ ydb-short-name }} работает на порядки больше таблеток по сравнению с процессами или потоками, которые другие системы использовали бы для кластера аналогичного размера. В кластере {{ ydb-short-name }} легко могут одновременно работать сотни тысяч и миллионы таблеток.

Поскольку таблетка хранит своё состояние в [распределённом хранилище](#distributed-storage), она может быть (пере)запущена на любом узле кластера. Таблетки идентифицируются с помощью [TabletID](#tabletid), 64-битного числа, назначаемого при создании таблетки.

### Лидер таблетки {#tablet-leader}

**Лидер таблетки** или **tablet leader** — это текущий активный лидер данной таблетки. Лидер таблетки принимает команды, назначает им порядок и подтверждает их внешнему миру. Гарантируется, что в любой момент времени существует не более одного лидера для каждой таблетки.

### Кандидат таблетки {#tablet-candidate}

**Кандидат таблетки** или **tablet candidate** — это один из участников выборов, который хочет стать [лидером](#tablet-leader) данной таблетки. Если кандидат выигрывает выборы, он становится лидером таблетки.

### Подписчик таблетки {#tablet-follower}

**Подписчик таблетки**, **горячий резерв**, **tablet follower** или **hot standby** — это копия [лидера таблетки](#tablet-leader), которая применяет журнал команд, принятых лидером (с некоторой задержкой). У таблетки может быть ноль или больше подписчиков. Подписчики выполняют две основные функции:

* В случае завершения или отказа лидера подписчики являются предпочтительными [кандидатами](#tablet-candidate) на роль нового лидера, так как они могут стать лидером намного быстрее, чем другие кандидаты, поскольку они применили большую часть журнала.
* Подписчики могут отвечать на запросы только для чтения, если клиент явно выбирает опциональный расслабленных режим  транзакций, допускающий устаревшие чтения (stale reads).

### Поколение таблетки {#tablet-generation}

**Поколение таблетки** или **tablet generation** — это номер, идентифицирующий реинкарнацию лидера таблетки. Он меняется только при выборе нового лидера и всегда увеличивается.

### Локальная база данных таблетки {#local-database}

**Локальная база данных таблетки**, **локальная база данных**, **tablet local database** или **local database** — это набор структур данных и связанного кода, которые управляют состоянием таблетки и хранимыми ей данными. Логически состояние локальной базы данных представлено набором таблиц, очень похожих на реляционные таблицы. Модификация состояния локальной базы данных осуществляется локальными транзакциями таблетки, создаваемыми пользовательским актором таблетки.

Each local database table is stored using the [LSM-дерево](#lsm-tree) data structure.

#### Log-structured merge-tree {#lsm-tree}

A **[log-structured merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree)** or **LSM tree**, is a data structure designed to optimize write and read performance in storage systems. It is used in {{ ydb-short-name }} for storing [local database](#local-database) tables and [VDisks](#vdisk) data.

#### MemTable {#memtable}

All data written to a [local database](#local-database) tables is initially stored in an in-memory data structure called a **MemTable**. When the MemTable reaches a predefined size, it is flushed to disk as an immutable [SST](#sst).

#### Sorted string table {#sst}

A **sorted string table** or **SST** is an immutable data structure that stores rows sorted by key, facilitating efficient key lookups and range queries. Each SST is composed of a contiguous series of small data pages, typically around 7 KiB in size, which further optimizes the process of reading data from disk. An SST typically represents a part of [LSM tree](#lsm-tree).

#### Пайп таблетки {#tablet-pipe}

**Пайп таблетки**, **tablet pipe** или **TabletPipe** — это виртуальное соединение, которое может быть установлено с таблеткой. Оно включает поиск [лидера таблетки](#tablet-leader) по [TabletID](#tabletid). Это рекомендуемый способ работы с таблеткой. Термин **открыть пайп к таблетке** описывает процесс разрешения (поиска) таблетки в кластере и установления с ней виртуального канала связи.

#### TabletID {#tabletid}

**TabletID** — это уникальный идентификатор [таблетки](#tablet) в рамках кластера.

#### Bootstrapper {#bootstrapper}

**Bootstrapper** — это основной механизм запуска таблеток, используемый для системных таблеток (например, для [Hive](#hive), [DS controller](#ds-controller), корневого [SchemeShard](#scheme-shard)). [Hive](#hive) инициализирует остальные таблетки.

### Shared cache {#shared-cache}

A **shared cache** is an [actor](#actor) that stores data pages recently accessed and read from [distributed storage](#distributed-storage). Caching these pages reduces disk I/O operations and accelerates data retrieval, enhancing overall system performance.

### Memory controller {#memory-controller}

A **memory controller** is an [actor](#actor) that manages {{ ydb-short-name }} [memory limits](../deploy/configuration/config.md#memory-controller).

### Типы таблеток {#tablet-types}

[Таблетки](#tablet) можно рассматривать как фреймворк для создания надёжных компонентов, работающих в распределённой системе. Многие компоненты {{ ydb-short-name }} реализованы с использованием этого фреймворка, они перечислены ниже.

#### Scheme shard {#scheme-shard}

**Scheme shard** или **SchemeShard** — это таблетка, которая хранит схему базы данных, включая метаданные пользовательских [таблиц](#table), [топиков](#topic) и т.д.

Кроме того, существует **корневой scheme shard**, который хранит информацию о базах данных, созданных в кластере.

#### Data shard {#data-shard}

**Data shard** или **DataShard** — это таблетка, которая управляет сегментом [построчной пользовательской таблицы](datamodel/table.md#row-oriented-tables). Логическая пользовательская таблица делится на сегменты по непрерывным диапазонам первичного ключа таблицы. За каждый такой диапазон отвечает отдельная таблетка DataShard. Сам диапазон также называется [партицией](#partition). Таблетка DataShard хранит данные построчно, что эффективно для OLTP нагрузок.

#### Column shard {#column-shard}

**Column shard** или **ColumnShard** — это таблетка, которая хранит сегмент данных [колоночной пользовательской таблицы](datamodel/table.md#column-oriented-tables).

#### KV Tablet {#kv-tablet}

**KV Tablet**, **key-value tablet** или **таблетка ключ-значение** — это таблетка, которая реализует простое отображение ключ->значение, где ключи и значения — это строки. У неё также есть несколько специфических функций, таких как блокировки.

#### PQ Tablet {#pq-tablet}

**PQ Tablet** или **persistent queue tablet** — это таблетка, которая реализует концепцию [топика](#topic). Каждый топик состоит из одного или нескольких разделов, и каждый раздел управляется отдельным экземпляром таблетки PQ.

#### TxAllocator {#txallocator}

**TxAllocator**, **аллокатор транзакций** или **transaction allocator** — это системная таблетка, которая выделяет уникальные идентификаторы транзакций ([TxID](#txid)) в кластере. Обычно в кластере имеется несколько таких таблеток, из которых [transaction proxy](#txproxy) предварительно выделяет и кэширует диапазоны для локального выпуска в рамках одного процесса.

#### Координатор {#coordinator}

**Координатор** или **coordinator** — это системная таблетка, обеспечивающая глобальный порядок транзакций. Задача координатора заключается в присвоении логического времени [PlanStep](#planstep) каждой транзакции, спланированной через этого координатора. Каждой транзакции назначается ровно один координатор, выбранный путём хеширования её [TxId](#txid).

#### Медиатор {#mediator}

**Медиатор** — это системная таблетка, которая распределяет транзакции, спланированные [координаторами](#coordinator), между участниками транзакций (обычно это [DataShards](#data-shard)). Медиаторы обеспечивают продвижение глобального времени. Каждый участник транзакции ассоциируется ровно с одним медиатором. Медиаторы позволяют избежать необходимости в полном наборе соединений между всеми координаторами и всеми участниками всех транзакций.

#### Hive {#hive}

**Hive** — это системная таблетка, отвечающая за запуск и управление другими таблетками. Её обязанности включают перемещение таблеток между узлами в случае отказа или перегрузки [узла](#node).

#### Система управления кластером {#cms}

**Система управления кластером**, **cluster management system** или **CMS** — это системная таблетка, отвечающая за управление информацией о текущем состоянии [кластера {{ ydb-short-name }}](#cluster). Эта информация используется для выполнения постепенных перезапусков кластера без воздействия на пользовательские нагрузки, обслуживания, переконфигурации кластера и т.д.

### Слот {#slot}

**Слот** в {{ ydb-short-name }} может использоваться в двух контекстах:

* **Слот** — это часть ресурсов сервера, выделенная для запуска одного [узла](#node) {{ ydb-short-name }}. Обычный размер слота — 10 процессорных ядер и 50 ГБ оперативной памяти. Слоты используются, если кластер {{ ydb-short-name }} развернут на серверах или виртуальных машинах с достаточными ресурсами для размещения нескольких слотов.
* **Слот VDisk** или **VSlot** — это доля PDisk, которая может быть выделена одному из [VDisk](#vdisk).

### Хранилище состояния {#state-storage}

**Хранилище состояния**, **state storage** или **StateStorage** — это распределённый сервис, который хранит информацию о таблетках, а именно:

* Текущий лидер таблетки или его отсутствие.
* Подписчики таблетки.
* Поколение и шаг таблетки `(generation:step)`.

Хранилище состояния используется как служба для разрешения имён таблеток, то есть для получения [ActorId](#actorid) по [TabletID](#tabletid). StateStorage также используется в процессе выбора [лидера таблетки](#tablet-leader).

Информация в хранилище состояния является волатильной. Таким образом, она теряется при отключении питания или перезапуске процесса. Несмотря на название, этот сервис не является постоянным долгосрочным хранилищем. Он содержит только информацию, которую легко восстановить и которая не должна быть долговечной. Однако хранилище состояния хранит информацию на нескольких узлах, чтобы минимизировать влияние отказов узлов. Через этот сервис также можно собрать кворум, который используется для выбора лидеров таблеток.

Из-за своей природы сервис хранилища состояния работает по принципу максимально возможных усилий. Например, отсутствие нескольких лидеров таблеток гарантируется через протокол выбора лидеров на [распределённом хранилище](#distributed-storage), а не на хранилище состояния.

#### Компакшн {#compaction}

**Компакшн**, **комактизация** или **compaction** — это внутренний фоновый процесс перестройки данных [LSM-дерева](#lsm-tree). Данные в [VDisk](#vdisk) и [локальных базах данных](#local-database) организованы в виде LSM-деревьев. Поэтому различают **компакшн VDisk** и **таблеточный компакшн**. Процесс компакшна обычно довольно ресурсоёмкий, поэтому принимаются меры по минимизации накладных расходов, связанных с ним, например, путём ограничения числа одновременно исполняемых компакшнов.

#### gRPC-прокси {#grpc-proxy}

**gRPC-прокси** или **gRPC proxy** — это прокси-система для внешних пользовательских запросов. Клиентские запросы поступают в систему по протоколу [gRPC](https://grpc.io), затем компонент прокси переводит их во внутренние вызовы для выполнения этих запросов, передаваемых через [интерконнект](#interconnect). Этот прокси предоставляет интерфейс как для запросов-ответов, так и для двунаправленной потоковой передачи данных.

### Реализация распределённого хранилища {#distributed-storage-implementation}

**Распределённое хранилище** — это распределённый отказоустойчивый слой хранения данных, который сохраняет бинарные записи, называемые [LogoBlob](#logoblob), адресуемые с помощью определённого типа идентификатора, называемого [LogoBlobID](#logoblobid). Таким образом, распределённое хранилище является хранилищем ключ-значение, которое сопоставляет LogoBlobID строке размером до 10 МБ. Распределённое хранилище состоит из множества [групп хранения](#storage-group), каждая из которых является независимым репозиторием данных.

Распределённое хранилище сохраняет неизменяемые данные, при этом каждый неизменяемый блок данных идентифицируется определённым ключом LogoBlobID. API распределённого хранилища очень специфичен, предназначен только для использования [таблетками](#tablet) для хранения их данных и журналов изменений. Таким образом, оно не предназначено для хранения данных общего назначения. Данные в распределённом хранилище удаляются с помощью специальных команд барьера. Из-за отсутствия мутаций в его интерфейсе распределённое хранилище может быть реализовано без реализации [распределённого консенсуса](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BD%D1%81%D0%B5%D0%BD%D1%81%D1%83%D1%81_%D0%B2_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%91%D0%BD%D0%BD%D1%8B%D1%85_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F%D1%85). Распределённое хранилище является лишь одним из компонентов, который таблетки используют для реализации распределённого консенсуса.

#### LogoBlob {#logoblob}

**LogoBlob** — это набор двоичных неизменяемых данных, идентифицируемый [LogoBlobID](#logoblobid) и хранящийся в [распределённом хранилище](#distributed-storage). Размер блока данных ограничен на уровне [VDisk](#vdisk) и выше по стэку. В настоящее время максимальный размер блока данных, который могут обработать VDisk, составляет 10 МБ.

#### LogoBlobID {#logoblobid}

**LogoBlobID** — это идентификатор [LogoBlob](#logoblob) в [распределённом хранилище](#distributed-storage). Он имеет структуру вида `[TabletID, Generation, Step, Channel, Cookie, BlobSize, PartID]`. Основные элементы LogoBlobID:
  
* `TabletID` — это [ID](#tabletid) таблетки, которой принадлежит LogoBlob.
* `Generation` — это поколение таблетки, в котором был записан блок данных.
* `Channel` — это [канал](#channel) таблетки, на котором записан LogoBlob.
* `Step` — это инкрементный счётчик, обычно в пределах поколения таблетки.
* `Cookie` — это уникальный идентификатор блока данных в пределах одного `Step`. Cookie обычно используется при записи нескольких блоков данных в один `Step`.
* `BlobSize` — это размер LogoBlob.
* `PartID` — это идентификатор части блока данных. Он важен, когда оригинальный LogoBlob разбивается на части с использованием [кодирования с исправлением ошибок](#erasure-coding), и части записываются на соответствующие [VDisk](#vdisk) и [группы хранения](#storage-group).

#### Репликация {#replication}

**Репликация** — это процесс, обеспечивающий наличие достаточного количества копий (реплик) данных для поддержания желаемых характеристик доступности кластера {{ ydb-short-name }}. Обычно используется в геораспределённых кластерах {{ ydb-short-name }}.

#### Кодирование с исправлением ошибок {#erasure-coding}

[**Кодирование с исправлением ошибок**](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B8%D1%80%D0%B0%D1%8E%D1%89%D0%B8%D0%B9_%D0%BA%D0%BE%D0%B4) или **erasure coding** — это метод кодирования данных, при котором исходные данные дополняются избыточностью и разделяются на несколько фрагментов, обеспечивая возможность восстановления исходных данных в случае потери одного или нескольких фрагментов. Он широко используется в кластерах {{ ydb-short-name }} с одной [зоной доступности](#regions-az) в отличии от [репликации](#replication) с 3 репликами. Например, наиболее популярная схема erasure coding 4+2 обеспечивает ту же надёжность, что и три реплики, с избыточностью пространства 1.5 против 3.

#### PDisk {#pdisk}

**PDisk**, **physical disk** или **физический диск** — это компонент, который контролирует физический дисковый накопитель (блочное устройство). Другими словами, PDisk — это подсистема, которая реализует абстракцию, аналогичную специализированной файловой системе поверх блочных устройств (или файлов, эмулирующих блочное устройство для целей тестирования). PDisk обеспечивает контроль целостности данных (включая [кодирование с исправлением ошибок](#erasure-coding) групп секторов для восстановления данных на отдельных повреждённых секторах, контроль целостности с помощью контрольных сумм), прозрачное шифрование всех данных на диске и транзакционные гарантии дисковых операций (подтверждение записи строго после `fsync`).

PDisk содержит планировщик, который обеспечивает совместное использование пропускной способности устройства между несколькими клиентами ([VDisk](#vdisk)). PDisk делит блочное устройство на блоки, называемые [слотами](#slot) (размером около 128 мегабайт; разрешены и меньшие блоки). В каждый момент времени не более одного VDisk может владеть каждым слотом. PDisk также поддерживает журнал восстановления, общий для записей службы PDisk и всех VDisk.
  
#### VDisk {#vdisk}

**VDisk**, **virtual disk** или **виртуальный диск** — это компонент, который реализует хранение данных [распределённого хранилища](#distributed-storage) [LogoBlob](#logoblob) на [PDisk](#pdisk). VDisk хранит все свои данные на PDisk. Один VDisk соответствует одному PDisk, но обычно несколько VDisk связаны с одним PDisk. В отличие от PDisk, который скрывает блоки и журналы за собой, VDisk предоставляет интерфейс на уровне LogoBlob и [LogoBlobID](#logoblobid), например запись LogoBlob, чтение данных LogoBlobID и удаление набора LogoBlob с помощью специальной команды. VDisk является членом [группы хранения](#storage-group). Сам VDisk является локальным, но многие VDisk в данной группе обеспечивают надёжное хранение данных. VDisk в группе синхронизируют данные друг с другом и реплицируют данные в случае потерь. Набор VDisk в группе хранения образует распределённый RAID.

#### Yard {#yard}

**Yard** — это название API [PDisk](#pdisk). Он позволяет [VDisk](#vdisk) читать и записывать данные в блоки и журналы, резервировать блоки, удалять блоки и транзакционно получать и возвращать владение блоками. В некоторых контекстах Yard можно считать синонимом PDisk.

#### Skeleton {#skeleton}

**Skeleton** — это [актор](#actor), который предоставляет интерфейс к [VDisk](#vdisk).

#### SkeletonFront {#skeletonfront}

**SkeletonFront** — это прокси-актор для Skeleton, который контролирует поток сообщений, поступающих в Skeleton.

#### Контроллер распределённого хранилища {#ds-controller}

**Контроллер распределённого хранилища**, **distributed storage controller**, **DS-controller** или **DS-контроллер** управляет динамической конфигурацией распределённого хранилища, включая информацию о [PDisk](#pdisk), [VDisk](#vdisk) и [группах хранения](#storage-group). Он взаимодействует с [node warden](#node-warden) для запуска различных компонентов распределённого хранилища. Он взаимодействует с [Hive](#hive) для выделения [каналов](#channel) таблеткам.

#### Прокси {#ds-proxy}

**Прокси распределённого хранилища**, **DS-прокси**, **BS-прокси**, **distributed storage proxy**, **DS-proxy** или **BS-proxy** выполняет роль клиентской библиотеки для выполнения операций с [распределённым хранилищем](#distributed-storage). Пользователями DS-прокси являются [таблетки](#tablet), которые записывают и читают из распределённого хранилища. DS-прокси скрывает распределённую природу распределённого хранилища от пользователя. Задача DS-прокси — запись в кворум [VDisk](#vdisk), выполнение повторных попыток при необходимости и контроль потока записи/чтения для предотвращения перегрузки VDisk.

Технически DS-прокси реализован как [акторный сервис](#actor-service), запускаемый [node warden](#node-warden) на каждом узле для каждой группы хранения, обрабатывающий все запросы к группе (запись, чтение и удаление [LogoBlob](#logoblob), блокировка группы). При записи данных DS-прокси выполняет [кодирование с исправлением ошибок](#erasure-coding) данных, разделяя LogoBlob на части, которые затем отправляются соответствующим VDisk. DS-прокси выполняет обратный процесс при чтении, получая части от VDisk и восстанавливая из них LogoBlob.

#### Node warden {#node-warden}

**Node warden** или `BS_NODE` — это [сервис акторов](#actor-service) на каждом узле кластера, запускающий [PDisks](#pdisk), [VDisks](#vdisk) и [DS прокси](#proxy) [статических групп хранения](#static-group) при запуске узла. Также он взаимодействует с [DS контроллером](#ds-controller) для запуска PDisk, VDisk и DS прокси [динамических групп](#dynamic-group). DS прокси динамических групп запускается по запросу: node warden обрабатывает «недоставленные» сообщения к DS прокси, запускает соответствующие DS прокси и получает конфигурацию групп от DS контроллера.

#### Область отказа {#fail-realm}

[Группа хранения](#storage-group) — это набор [VDisk](#vdisk), объединённых в одну или несколько **областей отказа** или **fail realm**. Область отказа содержит один или несколько [доменов отказа](#fail-domain). Коррелированный отказ двух VDisk в одной и той же области отказа более вероятен, чем отказ двух VDisk из разных областей отказа. Область отказа обычно соответствует физическому понятию датацентра или зоны доступности (AZ).

#### Домен отказа {#fail-domain}

[Fail realm](#fail-realm) содержит одну или несколько **доменов отказа** или **fail domains**. Коррелированная ошибка двух [VDisks](#vdisk) в пределах одного Fail domain более вероятна, чем ошибка двух VDisks из разных Fail domains. Fail domain обычно соответствует понятию [серверной стойки](#rack).

#### Канал распределённого хранилища {#channel}

**Канал распределённого хранилища**, **канал**, **distributed storage channel**, **DS channel** или **channel** — это логическое соединение между [таблеткой](#tablet) и группой [распределённого хранилища](#distributed-storage). Таблетка может записывать данные в различные каналы, и каждый канал отображается на определённую [группу хранения](#storage-group). Наличие нескольких каналов позволяет таблетке:

* Записывать больше данных, чем может содержать одна группа хранения.
* Хранить различные [LogoBlob](#logoblobs) в различных группах хранения, с различными свойствами, такими как кодирование с исправлением ошибок или на различных носителях (HDD, SSD, NVMe).

### Реализация распределённых транзакций {#distributed-transaction-implementation}

Ниже объяснены термины, связанные с реализацией [распределённых транзакций](#distributed-transactions). {% if oss == "true" %}Сама реализация описана в отдельной статье [{#T}](../contributor/datashard-distributed-txs.md).{% endif %}

#### Детерминированные транзакции {#deterministic-transactions}

Распределённые транзакции {{ ydb-short-name }} вдохновлены исследовательской работой [Building Deterministic Transaction Processing Systems without Deterministic Thread Scheduling](http://cs-www.cs.yale.edu/homes/dna/papers/transactions-wodet11.pdf) Александра Томсона и Дэниела Дж. Абади из Йельского университета. В статье введено понятие **детерминированной транзакционной обработки** или **deterministic transaction processing**, которое позволяет эффективно обрабатывать распределённые транзакции. Оригинальная статья накладывала ограничения на виды операций, которые могут выполняться таким образом. Поскольку эти ограничения мешали реальным пользовательским сценариям, {{ ydb-short-name }} развила свои алгоритмы для их выполнения, используя детерминированные транзакции в качестве этапов выполнения пользовательских транзакций с дополнительной оркестрацией и блокировками.

#### Оптимистичные блокировки {#optimistic-locking}

Как и во многих других системах управления базами данных, запросы {{ ydb-short-name }} могут накладывать блокировки на определённые фрагменты данных, такие как строки таблиц, чтобы гарантировать, что параллельные изменения не приведут их в несогласованное состояние. Однако {{ ydb-short-name }} проверяет эти блокировки не в начале транзакций, а при попытках их коммита (фиксации). Первый подход называется **пессимистичными блокировками** или **perssimistic locking** (например, используется в PostgreSQL), а второй — **оптимистичными блокировками** или **optimistic locking** (используется в {{ ydb-short-name }}).

#### Этап подготовки {#prepare-stage}

**Этап подготовки** — это фаза транзакции, во время которой тело транзакции регистрируется на всех участвующих шардах.

#### Этап выполнения {#execute-stage}

**Этап выполнения** — это фаза транзакции, в ходе которой запланированная транзакция исполняется и генерируется ответ.

В некоторых случаях, вместо [подготовки](#prepare-stage) и выполнения, транзакция немедленно выполняется и генерируется ответ. Например, это происходит для транзакций, затрагивающих только один шард или для согласованных чтений из снимка данных (snapshot).

#### Грязные операции {#dirty-operations}

В случае только для чтения транзакций, аналогично "read uncommitted" в других системах управления базами данных, может потребоваться чтение данных, которые ещё не были зафиксированы на диске. Это называется **грязными операциями** или **dirty operations**.

#### Набор чтения-записи {#rw-set}

**Набор чтения-записи**, **RW-набор**, **read-write set** или **RW-set** — это набор данных, который будет участвовать в выполнении [распределённой транзакции](#distributed-transactions). Он объединяет даггые набора чтения, которые будут считываться, и набор записи, для которого будут выполняться модификации.

#### Набор чтения {#read-set}

**Набор чтения**, **ReadSet данные**, **read set** или **ReadSet data** — это то, что пересылают участвующие шарды во время выполнения транзакции. В случае транзакций с данными он может содержать информацию о состоянии [оптимистичных блокировок](#optimistic-locking), готовности шарда к коммиту или решение отменить транзакцию.

#### Транзакционные прокси {#transaction-proxy}

**Транзакционные прокси**, **transaction proxy** или `TX_PROXY` — это сервис, который оркестрирует выполнение многих [распределённых транзакций](#distributed-transactions): последовательные фазы, выполнение фаз, планирование и агрегацию результатов. В случае прямой оркестрации другими акторами (например, KQP data transactions), он используется для кэширования и выделения уникальных [TxID](#txid).

#### Флаги транзакции {#txflags}

**Флаги транзакции**, **transaction flags** или **TxFlags** — это битовая маска флагов, которые каким-то образом изменяют выполнение транзакции.

#### Идентификатор транзакции {#txid}

**Идентификатор транзакции** или **TxID** — это уникальный идентификатор, назначаемый каждой транзакции при её принятии {{ ydb-short-name }}.

#### Идентификатор порядка транзакции {#transaction-order-id}

**Идентификатор порядка транзакции** или **transaction order id** — это уникальный идентификатор, назначаемый каждой транзакции во время планирования. Он состоит из [PlanStep](#planstep) и [Transaction ID](#txid).

#### Планируемый шаг {#planstep}

**Планируемый шаг**, **шаг**, **PlanStep** или **Step** — это логическое время, на которое запланировано выполнение набора транзакций.

#### Время медиатора {#mediator-time}

Во время выполнения распределённых транзакций, **время медиатора**, **медиаторное время** или **mediator time** — это логическое время, до которого (включительно) шард-участник должен знать весь план выполнения. Оно используется для продвижения времени при отсутствии транзакций на конкретном шарде, чтобы определить, может ли он читать из снимка (snapshot).

#### MiniKQL {#minikql}

**MiniKQL** — это язык, который позволяет выразить одну [детерминированную транзакцию](#deterministic-transactions) в системе. Это функциональный, строго типизированный язык. Концептуально язык описывает граф чтения из базы данных, выполнения вычислений над прочитанными данными и записи результатов в базу данных и/или в специальный документ, представляющий результат запроса (для показа пользователю). Транзакция MiniKQL должна явно задавать свой набор чтения (читаемые данные) и предполагать детерминированный выбор ветвей выполнения (например, отсутствует случайность).
  
MiniKQL — это язык низкого уровня. Конечные пользователи системы видят только запросы на языке [YQL](#yql), который опирается на MiniKQL в своей реализации.

#### KQP {#kqp}

**KQP** — это компонент {{ ydb-short-name }}, отвечающий за оркестрацию выполнения пользовательских запросов и генерацию окончательного ответа.

### Глобальная схема {#global-schema}

**Глобальная схема**, **схема базы данных**, **global scheme**, **global schema** или **database schema** — это схема всех данных, хранящихся в [базе данных](#database). Она состоит из [таблиц](#table) и других сущностей, таких как [топики](#topic). Метаданные об этих сущностях называются глобальной схемой. Термин используется в противоположность **локальной схеме** или **local schema**, которая относится к схеме данных внутри [таблетки](#tablet). Пользователи {{ ydb-short-name }} никогда не видят локальную схему и работают только с глобальной схемой.

### KiKiMR {#kikimr}

**KiKiMR** — это устаревшее название {{ ydb-short-name }}, использовавшееся до того, как он стал [продуктом с открытым исходным кодом](https://github.com/ydb-platform/ydb) (open source). Оно всё ещё может встречаться в исходном коде, старых статьях и видео и т.д.