# Трансформация и подготовка данных (ETL/ELT)

Подготовка данных для анализа — ключевой этап в построении хранилища данных. {{ydb-short-name}} поддерживает все стандартные подходы к трансформации данных, позволяя выбрать инструмент, наиболее подходящий для конкретной задачи: от чистого SQL до сложных пайплайнов на Apache Spark.

## SQL-трансформации (ELT)

Трансформации данных с помощью SQL зачастую являются наиболее производительными, поскольку вся обработка происходит непосредственно в движке {{ydb-short-name}}, без перемещения данных во внешние системы и обратно. Логика описывается на SQL и выполняется распределённым MPP-движком, оптимизированным для аналитических операций.

### Производительность в бенчмарке TPC-H

Производительность операций ELT напрямую зависит от скорости выполнения сложных аналитических запросов. Стандартным отраслевым тестом для оценки таких запросов является [TPC-H](https://www.tpc.org/tpch/).

Сравнение с другой распределённой аналитической СУБД на наборе запросов TPC-H показывает, что {{ydb-short-name}} демонстрирует более стабильную производительность, особенно при выполнении запросов, содержащих:

- Соединения (`JOIN`) большого числа таблиц, пяти и более;
- Вложенные подзапросы, используемые для фильтрации;
- Агрегации (`GROUP BY`) с последующей сложной фильтрацией по результатам.

![](_includes/ydb_vs_another.png){width=600}

Такая стабильность свидетельствует о высокой эффективности стоимостного оптимизатора запросов {{ ydb-short-name }} при построении планов выполнения для комплексных SQL-шаблонов, типичных для реальных ELT-процессов. Для платформы хранилища данных (DWH) это означает предсказуемое время обновления данных и снижение рисков неконтролируемой деградации производительности в продакшн-среде.

### Основные сценарии

- Построение витрин данных: используйте привычный синтаксис [`INSERT INTO ... SELECT FROM ...`](../../../../yql/reference/syntax/insert_into.md) для создания агрегированных таблиц (витрин) из «сырых» данных;
- Объединение OLTP и OLAP данных: {{ydb-short-name}} позволяет в одном запросе объединять данные как из транзакционных (строковых), так и из аналитических (колоночных) таблиц. Это даёт возможность обогащать «холодные» аналитические данные актуальной информацией из OLTP-системы без необходимости их дублирования;
- Массовое обновление: для «слепой» записи больших объёмов данных без проверки существования можно использовать оператор [`UPSERT INTO`](../../../../yql/reference/syntax/upsert_into.md).

## Управление SQL-пайплайнами с помощью dbt

Для управления сложными SQL-пайплайнами используйте [плагин к dbt](../../../../integrations/migration/dbt.md). Плагин позволяет дата-инженерам описывать модели данных в виде `SELECT`-запросов, а dbt автоматически строит граф зависимостей между моделями и выполняет их в правильном порядке. Этот подход помогает реализовать принципы software engineering (тестирование, документирование, версионирование) при работе с SQL-кодом.

## Сложные трансформации с помощью Apache Spark (ETL) {#spark}

Для задач, требующих сложной логики на языках программирования (Python, Scala, Java) или интеграции с ML-пайплайнами, рекомендуется использовать Apache Spark.

Специализированный [коннектор для Apache Spark](../../../../integrations/ingestion/spark.md) обеспечивает высокопроизводительную параллельную работу с данными. В отличие от многих других баз данных, в {{ydb-short-name}} нет выделенного мастер-узла для экспорта данных. Spark читает данные напрямую со всех узлов хранения {{ydb-short-name}}, что обеспечивает высокую скорость и линейную масштабируемость чтения.

## Оркестрация пайплайнов

Для запуска пайплайнов по расписанию и управления зависимостями используются оркестраторы.

- Apache Airflow: Для оркестрации данных в {{ydb-short-name}} поддерживается [провайдер к Apache Airflow](../../../../integrations/orchestration/airflow.md). С его помощью можно создавать DAG'и, которые запускают `dbt run`, выполняют YQL-скрипты или инициируют Spark-джобы.
- Встроенные механизмы: Для некоторых задач не требуется внешний оркестратор. {{ydb-short-name}} может выполнять часть операций автоматически:

    - Удаление устаревших данных по TTL: автоматическая очистка партиций по истечении заданного времени.
    - Автоматический сompaction: процессы слияния и оптимизации данных для LSM-дерева происходят в фоновом режиме, что исключает необходимость регулярного запуска команд вроде `VACUUM`.

## Интеграция с другими ETL-инструментами через JDBC

{{ydb-short-name}} предоставляет [JDBC-драйвер](../../../../reference/languages-and-apis/jdbc-driver/index.md), что позволяет использовать широкий спектр существующих ETL-инструментов, таких как [Apache NiFi](https://nifi.apache.org/) и другие JDBC-совместимые системы.
