# Топология кластера {{ ydb-short-name }}

Кластер {{ ydb-short-name }} состоит из [узлов хранения](glossary.md#storage-node) и [узлов баз данных](glossary.md#database-node). Работоспособность обоих типов узлов важна для обеспечения доступности баз данных {{ ydb-short-name }}: узлы баз данных реализуют логику управления данными, а узлы хранения обеспечивают выполнение операций ввода-вывода. Вместе с тем, наибольшее влияние на отказоустойчивость кластера и его способность обеспечить надежное хранение данных оказывает подсистема [распределенного хранилища](glossary.md#distributed-storage), состоящая из набора узлов хранения. При развертывании кластера необходимо выбрать [режим работы](#cluster-config) в соответствии с ожидаемой нагрузкой и требованиями к [доступности баз данных](#database-availability).

## Режимы работы кластера {#cluster-config}

Топология кластера строится в соответствии с его режимом работы, который должен быть выбран исходя из требований к уровню отказоустойчивости. Модель отказа, используемая в {{ ydb-short-name }}, основана на концепциях [домена отказа](glossary.md#fail-domain) и [области отказа](glossary.md#fail-realm). {{ ydb-short-name }} предоставляет следующие режимы работы распределенного хранилища:

- `none`. Избыточность отсутствует. Любой отказ приводит к временной недоступности или потере всех или части хранимых данных. Этот режим рекомендован к использованию только для разработки приложений или проведения функциональных тестов.
- `block-4-2`. Избыточность по схеме [стирающего кода](https://ru.wikipedia.org/wiki/Стирающий_код). На каждые 4 блока исходных данных формируется 2 дополнительных блока с кодами избыточности. Узлы хранения размещаются как минимум в 8 доменах отказа (обычно серверных стойках). [Пул хранения](glossary.md#storage-pool) остается полностью доступен при недоступности двух любых доменов отказа, продолжая записывать все 6 частей данных в сохранивщихся доменах. Этот режим рекомендован для кластеров, размещенных в одном дата-центре или зоне доступности.
- `mirror-3-dc`. Данные реплицируются в 3 зоны доступности (обычно разные дата-центры), с использованием как минимум 3 доменов отказа (обычно серверных стоек) в каждой зоне доступности. Кластер {{ ydb-short-name }} остается доступным при выходе из строя любой зоны доступности; кроме того, дополнительно из строя может выйти еще один домен отказа в любой из 2 работоспособных зон доступности без прекращения работы кластера. Этот режим рекомендован для кластеров с высокими требованиями к отказоустойчивости, размещенных в нескольких дата-центрах.
- `mirror-3-dc-3-nodes`. Упрощенный вариант режима `mirror-3-dc`. В этом режиме требуется не менее 3 серверов, каждый из которых должен содержать как минимум 3 диска для хранения данных. Для обеспечения гарантий отказоустойчивости каждый сервер должен размещаться в своем независимом дата-центре. Доступность кластера обеспечивается при условии отказа не более, чем 1 сервера из 3. Этот режим рекомендован для функционального тестирования или прототипирования.

{% note info %}

Под выходом из строя узла подразумевается как полная, так и частичная его недоступность, например выход из строя одного диска на узле.

{% endnote %}

Приведенная ниже таблица описывает требования и обеспечиваемый уровень устойчивости к отказам для различных режимов работы кластера:

| Режим | Множитель<br/>объема<br/>хранения | Минимальное<br/>количество<br/>узлов | Домен<br/>отказа | Область<br/>отказа | Кол-во<br/>дата-центров | Кол-во<br/>серверных<br/>стоек |
| --- | --- | --- | --- | --- | --- | --- |
| `none`, избыточность отсутствует | 1 | 1 | Узел | Узел | 1 | 1 |
| `block-4-2`, переживает отказ 2 стоек | 1.5 | 8 (10 рекомендовано) | Стойка | Дата-центр | 1 | 8 |
| `mirror-3-dc`, переживает отказ дата-центра и еще 1 стойки в оставшихся дата-центрах | 3 | 9 (12 рекомендовано) | Стойка | Дата-центр | 3 | 3 в каждом дата-центре |
| `block-4-2` *(упрощенный)*, переживает отказ 1 стойки | 1.5 | 10 | ½ стойки | Дата-центр | 1 | 5 |
| `mirror-3-dc` *(упрощенный)*, переживает отказ дата-центра и еще 1 сервера в оставшихся дата-центрах | 3 | 12 | ½ стойки | Дата-центр | 3 | 6 |
| `mirror-3-dc` *(3 узла)*, переживает отказ одного узла или одного дата-центра | 3 | 3 | Сервер | Дата-центр | 3 | Не важно |

{% note info %}

Приведенный выше множитель объема хранения относится только к фактору обеспечения отказоустойчивости. Для планирования размера хранилища необходимо учитывать другие влияющие факторы (например, фрагментацию и гранулярность [слотов](glossary.md#slot)).

{% endnote %}

Базовой единицей выделения ресурсов хранения данных в кластере {{ ydb-short-name }} является [группа хранения](glossary.md#storage-group). При создании группы хранения {{ ydb-short-name }} ее части — [VDisk-и](glossary.md#vdisk) — на [физических дисках](glossary.md#pdisk), принадлежащих разным доменам отказа. Для режима `block-4-2` каждая группа хранения распределена между 8 доменами отказа, а в режиме `mirror-3-dc` группа хранения распределяется между 3 областями отказа, в каждой из которых используется 3 домена отказа.

О том, как задать топологию кластера {{ ydb-short-name }}, читайте в разделе [{#T}](../deploy/configuration/config.md#domains-blob).

### Упрощенные конфигурации {#reduced}

В случаях, когда невозможно использовать [рекомендованное количество](#cluster-config) оборудования, можно разделить серверы одной стойки на 2 фиктивных домена отказа. В такой конфигурации отказ одной стойки будет означать отказ не одного, а сразу двух доменов. При использовании таких упрощенных конфигураций {{ ydb-short-name }} сохраняет работоспособность при отказе сразу двух доменов. Минимальное количество стоек в кластере для режима `block-4-2` составляет 5, для `mirror-3-dc` — по 2 в каждом ЦОД (т.е. всего 6 стоек).

В минимальной отказоустойчивой конфигурации {{ ydb-short-name }} используется режим `mirror-3-dc-3-nodes`, и кластер состоит из 3 серверов. В такой конфигурации каждый сервер одновременно является одновременно доменом отказа и областью отказа, и кластер может выдержать сбой только одного сервера.

## Восстановление избыточности {#rebuild}

В случае отказа физического диска {{ ydb-short-name }} может автоматически реконфигурировать связанные с ним группы хранения. При этом не важно, связана ли недоступность диска с выходом из строя сервера целиком. Автоматическая реконфигурация групп хранения снижает вероятность потери данных при возникновении множественных отказов, возникающих с достаточными для завершения восстановления избыточности интервалами. По умолчанию реконфигурация происходит через 1 час после того, как {{ ydb-short-name }} выявляет отказ.

Реконфигурация дисковой группы заменяет VDisk, размещенный на отказавшем оборудовании, новым VDisk, который система старается разместить на работоспособном оборудовании. Действуют те же правила, что и при создании новой группы хранения:

* Новый VDisk создается в домене отказа, отличающемся от доменов отказа всех остальных VDisk в группе.
* В режиме `mirror-3-dc` новый VDisk размещается в той же области отказа, что и отказавший VDisk.

Для того, чтобы реконфигурация в принципе была возможной, в кластере должны быть свободные слоты для создания VDisk в разных доменах отказа. При расчете количества оставляемых свободными слотов следует учитывать вероятность отказа оборудования, длительность репликации, а также время, необходимое на замену отказавшего оборудования.

Процесс реконфигурации дисковой группы создает повышенную нагрузку на остальные VDisk группы и на сеть. Для снижения влияния восстановления избыточности на производительность системы суммарная скорость репликации данных ограничивается как на стороне VDisk-источника так и на стороне VDisk-получателя.

Время восстановления избыточности при этом зависит от объема данных и от производительности оборудования. Например, репликация на быстрых NVMe SSD может завершиться за час, а на больших HDD репликация может длиться более суток.

Реконфигурация дисковых групп может быть ограничена или оказаться полностью невозможной, если оборудование кластера состоит из минимально необходимого количества доменов отказа:

* при отказе всего домена реконфигурация теряет смысл т.к. новый VDisk может быть расположен только в уже отказавшем домене отказа;
* при отказе части домена реконфигурация возможна, но при этом нагрузка, которая раньше обрабатывалась отказавшим оборудованием, будет перераспределена только по оборудованию в том же домене отказа.

Если же в кластере доступно хотя бы на 1 домен отказа больше, чем минимально необходимо для создания групп хранения (9 доменов для `block-4-2` и по 4 домена в каждой области отказа для `mirror-3-dc`), то при отказе части оборудования нагрузка может быть перераспределена по всему оставшемуся в строю оборудованию.

## Пропускная способность и производительность {#capacity}

Система может работать с доменами отказа любого размера, однако при малом количестве доменов и неодинаковом количестве дисков в разных доменах количество групп хранения, которые возможно будет создать, окажется ограничено. При этом часть оборудования в слишком больших доменах отказа может оказаться недоутилизированной. В случае полной утилизации оборудования существенный перекос в размерах доменов может сделать невозможной реконфигурацию.

Например, в кластере с режимом отказоустойчивости `block-4-2` имеется 15 стоек. В первой из 15 стоек расположено 20 серверов, а в остальных 14 стойках — по 10. Для полноценной утилизации всех 20 серверов из первой стойки {{ ydb-short-name }} будет создавать группы так, что в каждой из них будет участвовать 1 диск из этого самого большого домена отказа. В результате при выходе из строя оборудования в любом другом домене отказа нагрузка не сможет быть распределена на оборудование в первой стойке.

{{ ydb-short-name }} может объединять в группу диски разных производителей, различной емкости и скорости. Результирующие характеристики группы определяются набором наихудших характеристик оборудования, которое ее обслуживает. Обычно лучших результатов удается добиться при использовании однотипного оборудования.

{% note info %}

При создании больших кластеров следует учитывать, что оборудование из одной партии с большей вероятностью может иметь одинаковый дефект и отказать одновременно.

{% endnote %}

Таким образом, в качестве оптимальных аппаратных конфигураций кластеров {{ ydb-short-name }} для промышленного применения рекомендуются следующие:

* **Кластер в 1 зоне доступности**: использует режим отказоустойчивости `block4-2` и состоит из 9 или более стоек с равным количеством одинаковых серверов в каждой.
* **Кластер в 3 зонах доступности**: использует режим отказоустойчивости `mirror3-dc` и расположен в 3 дата-центрах с 4 или более стойками в каждом, стойки укомплектованы равным количеством одинаковых серверов.

## Обеспечение доступности баз данных {#database-availability}

[База данных](glossary.md#database) в кластере {{ ydb-short-name }} доступна, если ее хранилище и вычислительные ресурсы находятся в рабочем состоянии:

- Все выделенные базе данных [группы хранения](glossary.md#storage-group) должны быть доступны, что означает соблюдение допустимого уровня отказов для каждой группы.
- Вычислительные ресурсы доступных в данный момент [узлов базы данных](glossary.md#database-node) (в первую очередь объем оперативной памяти) должны быть достаточны для запуска всех [таблеток](glossary.md#tablet), управляющих пользовательскими объектами, такими как [таблицы](glossary.md#table) или [топики](glossary.md#topic), и для обработки пользовательских сессий.

Чтобы база данных могла пережить отказ одного из дата-центров в кластере, использующем режим работы `mirror-3-dc`, должны быть выполнены следующие условия:

- [Узлы хранения](glossary.md#storage-node) должны обеспечивать как минимум двойной запас пропускной способности ввода-вывода и дисковой емкости, по сравнению с необходимым для работы в обычных условиях. В худшем случае нагрузка на сохранившиеся узлы при длительном отключении одного из дата-центров может утроиться, но лишь на ограниченный период времени — до завершения восстановления ставших недоступными дисков в сохранившихся дата-центрах.
- [Узлы базы данных](glossary.md#database-node) должны быть равномерно распределены между всеми 3 дата-центрами, и содержать достаточно ресурсов для обработки полной нагрузки при сохранении 2 из 3 дата-центров. Это означает наличие как минимум 35% запаса по ресурсам CPU и оперативной памяти в обычных условиях, т.е. при отсутствии каких-либо отказов. Если узлы базы данных обычно нагружены более, чем на 65%, следует рассмотреть возможность добавления дополнительных узлов, либо увеличения вычислительно мощности каждого узла.

## Дополнительная информация

* [Документация для DevOps инженеров](../devops/index.md)
* [Примеры конфигурационных файлов кластера](https://github.com/ydb-platform/ydb/tree/main/ydb/deploy/yaml_config_examples/)
