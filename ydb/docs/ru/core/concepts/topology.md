# Топология кластера {{ ydb-short-name }}

Кластер {{ ydb-short-name }} состоит из статических и динамических узлов:

* статические узлы обеспечивают хранение данных, реализуя одну из поддерживаемых схем избыточности в зависимости от установленного режима работы;
* динамические узлы обеспечивают выполнение запросов, координацию транзакций и другие функции управления данными.

Топология кластера определяется требованиями к отказоустойчивости. Доступны следующие режимы работы:

Режим | Множитель<br/>объема хранения | Минимальное<br/>количество<br/>узлов | Описание
--- | --- | --- | ---
`none` | 1 | 1 | Избыточность отсутствует.<br/>Любой сбой оборудования приводит к недоступности пула хранения.<br/>Режим рекомендуется использовать только для функционального тестирования.
`block-4-2` | 1,5 | 8 | Применяется [Erasure coding](https://ru.wikipedia.org/wiki/Стирающий_код) с двумя блоками избыточности, добавляемыми к четырем блокам исходных данных. Узлы хранилища размещаются в не менее чем 8 доменах отказа (обычно стойках).<br/>Пул хранения доступен при потере любых двух доменов, продолжая запись всех 6 частей данных в оставшихся доменах.<br/>Режим рекомендуется для пулов хранения в пределах одной зоны доступности (обычно центра обработки данных).
`mirror-3-dc` | 3 | 9 | Данные реплицируются в 3 зоны доступности, использующие 3 домена отказа (обычно стойки) внутри каждой зоны.<br/>Пул хранения доступен при сбое одной зоны доступности и одного домена отказа в оставшихся зонах.<br/>Режим рекомендуется для мультидатацентровых инсталляций.
`mirror-3dc-3-nodes` | 3 | 3 | Является упрощенным вариантом `mirror-3dc`. Для данного режима необходимо минимум 3 сервера по 3 диска в каждом. Для обеспечения наибольшей отказоустойчивости каждый сервер должен находиться в независимом датацентре.<br/>Работоспособность в данном режиме сохраняется при выходе из строя не более 1 узла.<br/>Режим рекомендуется использовать только для функционального тестирования.

{% note info %}

Под выходом из строя узла подразумевается как полная, так и частичная его недоступность, например выход из строя одного диска на узле.

Приведенный выше множитель объема хранения относится только к фактору обеспечения отказоустойчивости. Для планирования размера хранилища необходимо учитывать другие влияющие факторы (например, фрагментацию и гранулярность слотов).

{% endnote %}

О том, как задать топологию кластера {{ ydb-short-name }} читайте в разделе [{#T}](../deploy/configuration/config.md#domains-blob).

## Промышленные конфигурации хранения данных

Для обеспечения необходимой отказоустойчивости {{ ydb-short-name }} нужно правильно сконфигурировать [дисковую подсистему](cluster/distributed_storage.md): выбрать подходящий [режим отказоустойчивости](#fault-tolerance) и [конфигурацию кластера](#cluster-config).

### Режимы отказоустойчивости {#fault-tolerance}

Для промышленных инсталляций {{ ydb-short-name }} рекомендуется использовать следующие [режимы отказоустойчивости](topology.md):

* `block-4-2` — если кластер расположен в одной зоне доступности.
* `mirror-3-dc` — если кластер расположен в трех зонах доступности.

В модели отказа {{ ydb-short-name }} различаются понятия домена отказа и области отказа.

Домен отказа (fail domain)

: Набор оборудования, для которого вероятен одновременный отказ.

  Например, доменом отказа можно считать диски одного сервера (в связи с вероятной недоступностью всех дисков сервера при отказе блока питания или сетевого контроллера сервера). Также доменом отказа можно считать серверы, расположенные в одной серверной стойке (в связи с вероятной недоступностью всего оборудования в стойке при проблемах с питанием или сетевым оборудованием, расположенным в той же стойке).

  Обработка любых отказов домена осуществляется автоматически, без остановки работы системы.

Область отказа (fail realm)

: Набор доменов отказа, для которого вероятен одновременный отказ.

  Например, областью отказа можно считать оборудование, расположенное в одном центре обработки данных (ЦОД), который может выйти из строя в результате стихийного бедствия.

Обычно под доменом отказа подразумевается серверная стойка, а под областью отказа – ЦОД.

При создании [групп хранения](glossary.md#storage-groups) {{ ydb-short-name }} объединяет в группы VDisk, расположенные на PDisk из разных доменов отказа. Таким образом, для режима `block-4-2` необходимо распределение PDisk по не менее чем 8 доменам отказа, а для режима `mirror-3-dc` — по 3 областям отказа с не менее чем 3 доменами отказа в каждой.

### Конфигурации кластеров {#cluster-config}

При отказе диска {{ ydb-short-name }} может автоматически переконфигурировать группу хранения так, чтобы вместо расположенного на отказавшем оборудовании VDisk использовался новый VDisk, который система пытается расположить на работающем в момент реконфигурации оборудовании. При этом соблюдается то же правило, что и при создании группы — VDisk создается в домене отказа, отличном от доменов отказа всех остальных VDisk этой группы (и в той же области отказа, что и отказавший VDisk для `mirror-3-dc`).

Это вызывает ряд проблем в случе, когда оборудование кластера распределено по минимально необходимому количеству доменов отказа:

* при отказе всего домена реконфигурация теряет смысл т.к. новый VDisk может быть расположен только в отказавшем домене отказа;
* при отказе части домена реконфигурация возможна, но при этом нагрузка, которая раньше обрабатывалась отказавшим оборудованием будет перераспределена только по оборудованию в том же домене отказа.

Если же в кластере доступно хотя бы на 1 домен отказа больше, чем минимально необходимо для создания групп хранения (9 доменов для `block-4-2` и по 4 домена в каждой области отказа для `mirror-3-dc)`, то при отказе части оборудования нагрузка может быть перераспределена по всему оставшемуся в строю оборудованию.

Система может работать с доменами отказа любого размера, однако при малом количестве доменов и неодинаковом количестве дисков в разных доменах количество групп хранения, которые возможно будет создать, окажется ограничено. При этом часть оборудования в слишком больших доменах отказа может оказаться недоутилизированной. В случае полной утилизации оборудования существенный перекос в размерах доменов может сделать невозможной реконфигурацию.

Например, в кластере с режимом отказоустойчивости `block-4-2` имеется 15 стоек. В первой из 15 стоек расположено 20 серверов, а в остальных 14 стойках — по 10. Для полноценной утилизации всех 20 серверов из первой стойки {{ ydb-short-name }} будет создавать группы так, что в каждой из них будет участвовать 1 диск из этого самого большого домена отказа. В результате при выходе из строя оборудования в любом другом домене отказа нагрузка не сможет быть распределена на оборудование в первой стойке.

{{ ydb-short-name }} может объединять в группу диски разных производителей, различной емкости и скорости. Результирующие характеристики группы определяются набором наихудших характеристик оборудования, которое ее обслуживает. Обычно лучших результатов удается добиться при использовании однотипного оборудования. При создании больших кластеров следует учитывать и то, что оборудование из одной партии с большей вероятностью может иметь одинаковый дефект и отказать одновременно.

Таким образом, в качестве оптимальных аппаратных конфигураций для промышленного применения рекомендуются следующие:

* **Кластер в 1 зоне доступности** — использует режим отказоустойчивости `block4-2` и состоит из 9 или более стоек с равным количеством одинаковых серверов в каждой.
* **кластер в 3 зонах доступности** — использует режим отказоустойчивости `mirror3-dc` и  расположен в 3 ЦОД с 4 или более стойками в каждом, стойки укомплектованы равным количеством одинаковых серверов.

Смотрите также [{#T}](#reduced).

### Восстановление избыточности {#rebuild}

Автоматическая реконфигурация групп хранения снижает вероятность потери данных при возникновении множественных отказов, возникающих с достаточными для завершения восстановления избыточности интервалами. По умолчанию реконфигурация происходит через 1 час после того, как {{ ydb-short-name }} выявляет отказ.

После реконфигурации новый VDisk автоматически наполняется данными для восстановления требуемой избыточности хранения в группе. При этом возникает повышенная нагрузка на остальные VDisk группы и на сеть. Для снижения влияния восстановления избыточности на производительность системы суммарная скорость репликации данных ограничивается как на стороне VDisk-источника так и на стороне VDisk-получателя.

Время восстановления избыточности при этом зависит от объема данных и от производительности оборудования. Например, репликация на быстрых NVMe SSD может завершиться за час, а на больших HDD репликация может длиться более суток.  Для того, чтобы реконфигурация в принципе была возможной, в кластере должны быть свободные слоты для создания VDisk в разных доменах отказа. При расчете количества оставляемых свободными слотов следует учитывать вероятность отказа оборудования, длительность репликации, время, необходимое на замену отказавшего оборудования.

### Сокращенные конфигурации {#reduced}

В случаях, когда невозможно использовать [рекомендованное количество](#cluster-config) оборудования, можно разделить серверы одной стойки на 2 фиктивных домена отказа. В такой конфигурации отказ 1 стойки будет означать отказ не 1, а сразу 2 доменов. В [обоих режимах](#fault-tolerance) отказоустойчивости {{ ydb-short-name }} сохранит работоспособность при отказе 2 доменов. При использовании конфигурации с фиктивными доменами отказа минимальное количество стоек в кластере для режима `block-4-2` составляет 5, для `mirror-3-dc` — по 2 в каждом ЦОД.

### Уровень отказоустойчивости {#reliability}

В таблице приведены уровни отказоустойчивости для различных режимов отказоустойчивости и аппаратных конфигураций кластера {{ ydb-short-name }}:

Режим<br>отказоустойчивости | Домен<br>отказа | Область<br>отказа | Количество<br>ЦОД | Количество<br>серверных стоек | Уровень<br>отказоустойчивости
:--- | :---: | :---: | :---: | :---: | :---
`block-4-2` | Стойка | ЦОД | 1 | 9 или более | Переживает отказ 2 стоек
`block-4-2` | ½ стойки | ЦОД | 1 | 5 или более | Переживает отказ 1 стойки
`block-4-2` | Сервер | ЦОД | 1 | Не важно | Переживает отказ 2 серверов
`mirror-3-dc` | Стойка | ЦОД | 3 | По 4 в каждом ЦОД | Переживает отказ ЦОД и 1 стойки в одном из двух других ЦОД
`mirror-3-dc` | Сервер | ЦОД | 3 | Не важно | Переживает отказ ЦОД и 1 сервера в одном из двух других ЦОД

## Дополнительная информация

* [Документация для DevOps инженеров](../devops/index.md)
* [Примеры конфигурационных файлов кластера](https://github.com/ydb-platform/ydb/tree/main/ydb/deploy/yaml_config_examples/)
