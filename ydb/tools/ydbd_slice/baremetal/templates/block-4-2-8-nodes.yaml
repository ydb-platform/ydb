# YDB configuration options and their values
# are described in documentaion https://ydb.tech/en/docs/deploy/configuration/config

# Option will be removed further
use_new_style_kikimr_cfg: true

# static erasure is the parameter that
# describes the fault tolerance mode of the
# cluster. See docs for more details https://ydb.tech/en/docs/deploy/configuration/config#domains-blob

host_configs: # the list of available host configurations in the cluster.
- drive:
  - path: /dev/disk/by-partlabel/kikimr_nvme_01   # path of the first disk in the host configration.
    type: SSD                                     # kind of the disk: available kinds are SSD, NVME or HDD
  - path: /dev/disk/by-partlabel/kikimr_nvme_02
    type: SSD
    # Add more disks if required.
  host_config_id: 1 # the unique id of the host config
hosts:
- host: &host_1 CHANGE_ME_HOST_1 # storage node DNS name, change if required.
  host_config_id: 1             # numeric host configuration template identifier.
  location:               # this parameter describes where host is located.
    body: 1                     # string representing a host serial number.
    data_center: 'DCA'       # string representing the datacenter / availability zone where the host is located.
                                # if cluster is deployed using mirror-3-dc fault tolerance mode, all hosts must be distributed
                                # across 3 datacenters.
    rack: '1'                   # string representing a rack identifier where the host is located.
                                # if cluster is deployed using block-4-2 erasure, all hosts should be distrubited
                                # across at least 8 racks.
    # For testing purpose it does not really matter, where all hosts are located.
    # All hosts can be located in one datacenter and even in one rack.
    # Just do not change `data_center` and `rack` options.
    # NOTE(shmel1k@): If you host has label like '_4ssd', that means, that
    # they will be located at `kikimr_ssd_01-04`
    # If your host has label like '_2hdd', that means, that
    # they will be located at `/dev/disk/by-partlabel/kikimr_hdd_03-04` (enumeration starts with 03)
- host: &host_2 CHANGE_ME_HOST_2
  host_config_id: 1
  location:
    body: 2
    data_center: 'DCA'
    rack: '2'
- host: &host_3 CHANGE_ME_HOST_3
  host_config_id: 1
  location:
    body: 3
    data_center: 'DCA'
    rack: '3'

- host: &host_4 CHANGE_ME_HOST_4
  host_config_id: 1
  location:
    body: 4
    data_center: 'DCA'
    rack: '4'
- host: &host_5 CHANGE_ME_HOST_5
  host_config_id: 1
  location:
    body: 5
    data_center: 'DCA'
    rack: '5'
- host: &host_6 CHANGE_ME_HOST_6
  host_config_id: 1
  location:
    body: 6
    data_center: 'DCA'
    rack: '6'

- host: &host_7 CHANGE_ME_HOST_7
  host_config_id: 1
  location:
    body: 7
    data_center: 'DCA'
    rack: '7'
- host: &host_8 CHANGE_ME_HOST_8
  host_config_id: 1
  location:
    body: 8
    data_center: 'DCA'
    rack: '8'

# static erasure is the parameter that
# describes the fault tolerance mode of the
# cluster. See docs for more details https://ydb.tech/en/docs/deploy/configuration/config#domains-blob
static_erasure: block-4-2
state_storage:
  allow_incorrect: true
  node_ids: [1, 2, 3, 4, 5, 6, 7, 8]
fail_domain_type: rack

# NOTE(shmel1k@): this template domains_config differs from production configuration.
# It will be fixed soon, stay tuned.
domains:
  # There can be only one root domain in a cluster. Domain name prefixes all scheme objects names, e.g. full name of a table table1 in database db1
  # in a cluster with domains_config.domain.name parameter set to Root would be equal to /Root/db1/table1
  - domain_name: Root
    dynamic_slots: 8
    databases:
      - name: "testdb"
        storage_units:
          - count: 1 # How many groups will be allocated for database
            kind: ssd # What storage will group use
        compute_units:
          - count: 1 # How many dynamic nodes will database have
            kind: slot
            zone: any
    storage_pools:
      - kind: ssd
        num_groups: 1
    storage_pool_kinds:
    - kind: ssd
      # fault tolerance mode name - none, block-4-2, or mirror-3-dc.
      # See docs for more details https://ydb.tech/en/docs/deploy/configuration/config#domains-blob
      erasure: block-4-2
      fail_domain_type: rack
      filter_properties:
        type: SSD # device type to match host_configs.drive.type

table_service_config:
  sql_version: 1
actor_system_config:    # the configuration of the actor system which descibes how cores of the instance are distributed
  executor:             # accross different types of workloads in the instance.
  - name: System        # system executor of the actor system. in this executor YDB launches system type of workloads, like system tablets
                        # and reads from storage.
    threads: 2          # the number of threads allocated to system executor.
    type: BASIC
  - name: User          # user executor of the actor system. In this executor YDB launches user workloads, like datashard activities,
                        # queries and rpc calls.
    threads: 3          # the number of threads allocated to user executor.
    type: BASIC
  - name: Batch         # user executor of the actor system. In this executor YDB launches batch operations, like scan queries, table
                        # compactions, background compactions.
    threads: 2          # the number of threads allocated to the batch executor.
    type: BASIC
  - name: IO            # the io executor. In this executor launches sync operations and writes logs.
    threads: 1
    time_per_mailbox_micro_secs: 100
    type: IO
  - name: IC            # the interconnect executor which YDB uses for network communications accross different nodes of the cluster.
    spin_threshold: 10
    threads: 1          # the number of threads allocated to the interconnect executor.
    time_per_mailbox_micro_secs: 100
    type: BASIC
  scheduler:
    progress_threshold: 10000
    resolution: 256
    spin_threshold: 0
blob_storage_config:    # configuration of static blobstorage group.
                        # YDB uses this group to store system tablets' data, like SchemeShard
  service_set:
    groups:
    - erasure_species: block-4-2  # fault tolerance mode name for the static group
      rings:           # in block-4-2 must have exactly 1 ring or availability zone.
      - fail_domains:
        - vdisk_locations:    # fail domains of the static group describe where each vdisk of the static group should be located.
          - node_id: *host_1
            pdisk_category: &pdisk_category CHANGE_PDISK_CATEGORY # Set this option to 1, if cluster has SSD/NVME disks
            # or to 0, if cluster has HDD.
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 1
        - vdisk_locations:
          - node_id: *host_2
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 2
        - vdisk_locations:
          - node_id: *host_3
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 3
        - vdisk_locations:
          - node_id: *host_4
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 4
        - vdisk_locations:
          - node_id: *host_5
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 5
        - vdisk_locations:
          - node_id: *host_6
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 6
        - vdisk_locations:
          - node_id: *host_7
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 7
        - vdisk_locations:
          - node_id: *host_8
            pdisk_category: *pdisk_category
            path: /dev/disk/by-partlabel/kikimr_nvme_01
            pdisk_guid: 8
# TODO: migrate to channel_profile_config
profiles:
  - channels:
    - storage_pool_kind: ssd
    - storage_pool_kind: ssd
    - storage_pool_kind: ssd

interconnect_config:
    start_tcp: true

grpc_config: