# YDB configuration options and their values
# are described in documentaion https://ydb.tech/en/docs/deploy/configuration/config

# Option will be removed further
use_new_style_kikimr_cfg: true
yaml_config_enabled: true

log:
  cluster_name: "default_devslice" # Change cluster name in order to have better experience with logs searching in Monium.
  sys_log: true
  uaclient_config:
    uri: "[fd53::1]:16400"
    grpc_max_message_size: 4194304

host_configs: # the list of available host configurations in the cluster.
- drive:
  - path: &disk_path_1 /dev/disk/by-partlabel/kikimr_nvme_01   # path of the first disk in the host configration.
    type: SSD                                     # kind of the disk: available kinds are SSD, NVME or HDD
    # Add more disks if required.
  host_config_id: 1 # the unique id of the host config
hosts:
- host: &host_1 CHANGE_ME_HOST_1 # storage node DNS name, change if required.
  host_config_id: 1             # numeric host configuration template identifier.
  location:               # this parameter describes where host is located.
    body: 1                     # string representing a host serial number.
    data_center: 'DCA'       # string representing the datacenter / availability zone where the host is located.
                                # if cluster is deployed using mirror-3-dc fault tolerance mode, all hosts must be distributed
                                # across 3 datacenters.
    rack: '1'                   # string representing a rack identifier where the host is located.
                                # if cluster is deployed using block-4-2 erasure, all hosts should be distrubited
                                # across at least 8 racks.
    # For testing purpose it does not really matter, where all hosts are located.
    # All hosts can be located in one datacenter and even in one rack.
    # Just do not change `data_center` and `rack` options.
    # NOTE(shmel1k@): If you host has label like '_4ssd', that means, that
    # they will be located at `kikimr_ssd_01-04`
    # If your host has label like '_2hdd', that means, that
    # they will be located at `/dev/disk/by-partlabel/kikimr_hdd_03-04` (enumeration starts with 03)

# static erasure is the parameter that
# describes the fault tolerance mode of the
# cluster. See docs for more details https://ydb.tech/en/docs/deploy/configuration/config#domains-blob
static_erasure: none
state_storage:
  allow_incorrect: true
  node_ids: [1]
fail_domain_type: none

# NOTE(shmel1k@): this template domains_config differs from production configuration.
# It will be fixed soon, stay tuned.
domains:
  # There can be only one root domain in a cluster. Domain name prefixes all scheme objects names, e.g. full name of a table table1 in database db1
  # in a cluster with domains_config.domain.name parameter set to Root would be equal to /Root/db1/table1
  - domain_name: Root
    dynamic_slots: 8
    databases:
      - name: "testdb"
        storage_units:
          - count: 1 # How many groups will be allocated for database
            kind: ssd # What storage will group use
        compute_units:
          - count: 1 # How many dynamic nodes will database have
            kind: slot
            zone: any
    storage_pools:
      - kind: ssd
        num_groups: 1
    storage_pool_kinds:
    - kind: ssd
      # fault tolerance mode name - none, block-4-2, or mirror-3-dc.
      # See docs for more details https://ydb.tech/en/docs/deploy/configuration/config#domains-blob
      erasure: none
      fail_domain_type: disk
      filter_properties:
        type: SSD # device type to match host_configs.drive.type

table_service_config:
  sql_version: 1
actor_system_config:    # the configuration of the actor system which descibes how cores of the instance are distributed
  executor:             # accross different types of workloads in the instance.
  - name: System        # system executor of the actor system. in this executor YDB launches system type of workloads, like system tablets
                        # and reads from storage.
    threads: 2          # the number of threads allocated to system executor.
    type: BASIC
  - name: User          # user executor of the actor system. In this executor YDB launches user workloads, like datashard activities,
                        # queries and rpc calls.
    threads: 3          # the number of threads allocated to user executor.
    type: BASIC
  - name: Batch         # user executor of the actor system. In this executor YDB launches batch operations, like scan queries, table
                        # compactions, background compactions.
    threads: 2          # the number of threads allocated to the batch executor.
    type: BASIC
  - name: IO            # the io executor. In this executor launches sync operations and writes logs.
    threads: 1
    time_per_mailbox_micro_secs: 100
    type: IO
  - name: IC            # the interconnect executor which YDB uses for network communications accross different nodes of the cluster.
    spin_threshold: 10
    threads: 1          # the number of threads allocated to the interconnect executor.
    time_per_mailbox_micro_secs: 100
    type: BASIC
  scheduler:
    progress_threshold: 10000
    resolution: 256
    spin_threshold: 0
blob_storage_config:    # configuration of static blobstorage group.
                        # YDB uses this group to store system tablets' data, like SchemeShard
  service_set:
    groups:
    - erasure_species: none  # fault tolerance mode name for the static group
      rings:           # in block-4-2 must have exactly 1 ring or availability zone.
      - fail_domains:
        - vdisk_locations:    # fail domains of the static group describe where each vdisk of the static group should be located.
          - node_id: *host_1
            pdisk_category: &pdisk_category 1 # Set this option to 1, if cluster has SSD/NVME disks
            # or to 0, if cluster has HDD.
            path: *disk_path_1
            pdisk_guid: 1

# TODO: migrate to channel_profile_config
profiles:
  - channels:
    - storage_pool_kind: ssd
    - storage_pool_kind: ssd
    - storage_pool_kind: ssd

interconnect_config:
    start_tcp: true

grpc_config: